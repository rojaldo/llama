<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.23">
<title>LangChain</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<style>
/*! Stylesheet for CodeRay to loosely match GitHub themes | MIT License */
pre.CodeRay{background:#f7f7f8}
.CodeRay .line-numbers{border-right:1px solid;opacity:.35;padding:0 .5em 0 0;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
.CodeRay span.line-numbers{display:inline-block;margin-right:.75em}
.CodeRay .line-numbers strong{color:#000}
table.CodeRay{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.CodeRay td{vertical-align:top;line-height:inherit}
table.CodeRay td.line-numbers{text-align:right}
table.CodeRay td.code{padding:0 0 0 .75em}
.CodeRay .debug{color:#fff!important;background:navy!important}
.CodeRay .annotation{color:#007}
.CodeRay .attribute-name{color:navy}
.CodeRay .attribute-value{color:#700}
.CodeRay .binary{color:#509}
.CodeRay .comment{color:#998;font-style:italic}
.CodeRay .char{color:#04d}
.CodeRay .char .content{color:#04d}
.CodeRay .char .delimiter{color:#039}
.CodeRay .class{color:#458;font-weight:bold}
.CodeRay .complex{color:#a08}
.CodeRay .constant,.CodeRay .predefined-constant{color:teal}
.CodeRay .color{color:#099}
.CodeRay .class-variable{color:#369}
.CodeRay .decorator{color:#b0b}
.CodeRay .definition{color:#099}
.CodeRay .delimiter{color:#000}
.CodeRay .doc{color:#970}
.CodeRay .doctype{color:#34b}
.CodeRay .doc-string{color:#d42}
.CodeRay .escape{color:#666}
.CodeRay .entity{color:#800}
.CodeRay .error{color:#808}
.CodeRay .exception{color:inherit}
.CodeRay .filename{color:#099}
.CodeRay .function{color:#900;font-weight:bold}
.CodeRay .global-variable{color:teal}
.CodeRay .hex{color:#058}
.CodeRay .integer,.CodeRay .float{color:#099}
.CodeRay .include{color:#555}
.CodeRay .inline{color:#000}
.CodeRay .inline .inline{background:#ccc}
.CodeRay .inline .inline .inline{background:#bbb}
.CodeRay .inline .inline-delimiter{color:#d14}
.CodeRay .inline-delimiter{color:#d14}
.CodeRay .important{color:#555;font-weight:bold}
.CodeRay .interpreted{color:#b2b}
.CodeRay .instance-variable{color:teal}
.CodeRay .label{color:#970}
.CodeRay .local-variable{color:#963}
.CodeRay .octal{color:#40e}
.CodeRay .predefined{color:#369}
.CodeRay .preprocessor{color:#579}
.CodeRay .pseudo-class{color:#555}
.CodeRay .directive{font-weight:bold}
.CodeRay .type{font-weight:bold}
.CodeRay .predefined-type{color:inherit}
.CodeRay .reserved,.CodeRay .keyword{color:#000;font-weight:bold}
.CodeRay .key{color:#808}
.CodeRay .key .delimiter{color:#606}
.CodeRay .key .char{color:#80f}
.CodeRay .value{color:#088}
.CodeRay .regexp .delimiter{color:#808}
.CodeRay .regexp .content{color:#808}
.CodeRay .regexp .modifier{color:#808}
.CodeRay .regexp .char{color:#d14}
.CodeRay .regexp .function{color:#404;font-weight:bold}
.CodeRay .string{color:#d20}
.CodeRay .string .string .string{background:#ffd0d0}
.CodeRay .string .content{color:#d14}
.CodeRay .string .char{color:#d14}
.CodeRay .string .delimiter{color:#d14}
.CodeRay .shell{color:#d14}
.CodeRay .shell .delimiter{color:#d14}
.CodeRay .symbol{color:#990073}
.CodeRay .symbol .content{color:#a60}
.CodeRay .symbol .delimiter{color:#630}
.CodeRay .tag{color:teal}
.CodeRay .tag-special{color:#d70}
.CodeRay .variable{color:#036}
.CodeRay .insert{background:#afa}
.CodeRay .delete{background:#faa}
.CodeRay .change{color:#aaf;background:#007}
.CodeRay .head{color:#f8f;background:#505}
.CodeRay .insert .insert{color:#080}
.CodeRay .delete .delete{color:#800}
.CodeRay .change .change{color:#66f}
.CodeRay .head .head{color:#f4f}
</style>
</head>
<body class="article">
<div id="header">
<h1>LangChain</h1>
<div id="toc" class="toc">
<div id="toctitle">Índice de contenidos</div>
<ul class="sectlevel1">
<li><a href="#_1_introducción_a_langchain">1. 1. Introducción a LangChain</a>
<ul class="sectlevel2">
<li><a href="#_1_1_qué_es_langchain_y_para_qué_sirve">1.1. 1.1 ¿Qué es LangChain y para qué sirve?</a></li>
<li><a href="#_1_2_historia_y_evolución_de_langchain">1.2. 1.2 Historia y evolución de LangChain</a></li>
<li><a href="#_1_3_casos_de_uso_y_aplicaciones_en_la_industria">1.3. 1.3 Casos de uso y aplicaciones en la industria</a></li>
<li><a href="#_1_4_arquitectura_general_y_componentes_clave">1.4. 1.4 Arquitectura general y componentes clave</a></li>
</ul>
</li>
<li><a href="#_2_instalación_y_configuración_del_entorno_para_trabajar_con_modelos_ollama_en_local">2. 2. Instalación y Configuración del Entorno para trabajar con modelos Ollama en local</a>
<ul class="sectlevel2">
<li><a href="#_2_1_requisitos_previos">2.1. 2.1 Requisitos previos</a></li>
<li><a href="#_2_2_instalación_de_langchain_y_dependencias">2.2. 2.2 Instalación de LangChain y dependencias</a></li>
<li><a href="#_2_3_configuración_de_ollama_y_modelos_locales">2.3. 2.3 Configuración de Ollama y modelos locales</a></li>
<li><a href="#_2_4_verificación_de_la_instalación_y_primeros_tests">2.4. 2.4 Verificación de la instalación y primeros tests</a></li>
</ul>
</li>
<li><a href="#_3_fundamentos_de_llms_y_su_integración_con_langchain">3. 3. Fundamentos de LLMs y su integración con LangChain</a>
<ul class="sectlevel2">
<li><a href="#_3_1_qué_es_un_large_language_model_llm">3.1. 3.1 ¿Qué es un Large Language Model (LLM)?</a></li>
<li><a href="#_3_2_integración_de_modelos_open_source_y_comerciales">3.2. 3.2 Integración de modelos open source y comerciales</a></li>
<li><a href="#_3_3_conexión_con_modelos_de_hugging_face">3.3. 3.3 Conexión con modelos de Hugging Face</a></li>
</ul>
</li>
<li><a href="#_4_modelos_y_prompts_en_langchain">4. 4. Modelos y Prompts en LangChain</a>
<ul class="sectlevel2">
<li><a href="#_4_1_uso_básico_de_llms_y_chatmodels">4.1. 4.1 Uso básico de LLMs y ChatModels</a></li>
<li><a href="#_4_2_creación_y_gestión_de_prompt_templates">4.2. 4.2 Creación y gestión de Prompt Templates</a></li>
<li><a href="#_4_3_prompts_dinámicos_y_personalizados">4.3. 4.3 Prompts dinámicos y personalizados</a></li>
<li><a href="#_4_4_buenas_prácticas_en_la_redacción_de_prompts">4.4. 4.4 Buenas prácticas en la redacción de prompts</a></li>
</ul>
</li>
<li><a href="#_5_chains_composición_y_orquestación">5. 5. Chains: Composición y Orquestación</a>
<ul class="sectlevel2">
<li><a href="#_5_1_qué_es_una_chain_en_langchain">5.1. 5.1 ¿Qué es una Chain en LangChain?</a></li>
<li><a href="#_5_2_chains_simples_y_secuenciales">5.2. 5.2 Chains simples y secuenciales</a></li>
<li><a href="#_5_3_chains_personalizadas_y_anidadas">5.3. 5.3 Chains personalizadas y anidadas</a></li>
<li><a href="#_5_4_chains_para_preguntas_y_respuestas">5.4. 5.4 Chains para preguntas y respuestas</a></li>
<li><a href="#_5_5_manejo_de_múltiples_entradas_y_salidas">5.5. 5.5 Manejo de múltiples entradas y salidas</a></li>
</ul>
</li>
<li><a href="#_6_manejo_y_procesamiento_de_documentos_con_ollama_y_langchain">6. 6. Manejo y Procesamiento de Documentos con Ollama y LangChain</a>
<ul class="sectlevel2">
<li><a href="#_6_1_document_loaders_carga_de_pdfs_txt_web_etc">6.1. 6.1 Document Loaders: carga de PDFs, TXT, web, etc.</a></li>
<li><a href="#_6_2_procesamiento_y_limpieza_de_documentos">6.2. 6.2 Procesamiento y limpieza de documentos</a></li>
<li><a href="#_6_3_splitters_fragmentación_de_texto_y_chunking">6.3. 6.3 Splitters: fragmentación de texto y chunking</a>
<ul class="sectlevel3">
<li><a href="#_tipos_de_splitters_y_estrategias">6.3.1. Tipos de splitters y estrategias</a></li>
<li><a href="#_ejemplo_uso_de_recursivecharactertextsplitter">6.3.2. Ejemplo: Uso de RecursiveCharacterTextSplitter</a></li>
<li><a href="#_ejemplo_splitter_personalizado_para_otros_idiomas_o_estructuras">6.3.3. Ejemplo: Splitter personalizado para otros idiomas o estructuras</a></li>
</ul>
</li>
<li><a href="#_6_4_extracción_de_información_relevante">6.4. 6.4 Extracción de información relevante</a>
<ul class="sectlevel3">
<li><a href="#_cómo_funciona_la_extracción_de_información">6.4.1. ¿Cómo funciona la extracción de información?</a></li>
<li><a href="#_técnicas_y_métodos_de_extracción">6.4.2. Técnicas y métodos de extracción</a></li>
<li><a href="#_ejemplo_1_resumir_fragmentos_de_texto_con_ollama">6.4.3. Ejemplo 1: Resumir fragmentos de texto con Ollama</a></li>
<li><a href="#_ejemplo_2_extracción_de_entidades_nombradas">6.4.4. Ejemplo 2: Extracción de entidades nombradas</a></li>
<li><a href="#_ejemplo_3_clasificación_temática_de_fragmentos">6.4.5. Ejemplo 3: Clasificación temática de fragmentos</a></li>
<li><a href="#_ejemplo_4_extracción_de_metadatos">6.4.6. Ejemplo 4: Extracción de metadatos</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_7_embeddings_y_bases_de_datos_vectoriales">7. 7. Embeddings y Bases de Datos Vectoriales</a>
<ul class="sectlevel2">
<li><a href="#_7_1_qué_son_los_embeddings_y_para_qué_se_usan">7.1. 7.1 ¿Qué son los embeddings y para qué se usan?</a></li>
<li><a href="#_7_2_creación_de_embeddings_con_llms">7.2. 7.2 Creación de embeddings con LLMs</a></li>
<li><a href="#_7_3_almacenamiento_en_bases_de_datos_vectoriales_pinecone_faiss_chroma">7.3. 7.3 Almacenamiento en bases de datos vectoriales (Pinecone, FAISS, Chroma)</a></li>
<li><a href="#_7_4_búsqueda_semántica_y_recuperación_de_información">7.4. 7.4 Búsqueda semántica y recuperación de información</a></li>
</ul>
</li>
<li><a href="#_8_retrieval_augmented_generation_rag">8. 8. Retrieval Augmented Generation (RAG)</a>
<ul class="sectlevel2">
<li><a href="#_8_1_concepto_y_arquitectura_de_rag">8.1. 8.1 Concepto y arquitectura de RAG</a></li>
<li><a href="#_8_2_implementación_de_rag_con_langchain">8.2. 8.2 Implementación de RAG con LangChain</a></li>
<li><a href="#_8_3_integración_de_bases_de_datos_vectoriales_en_rag">8.3. 8.3 Integración de bases de datos vectoriales en RAG</a></li>
<li><a href="#_8_4_ejemplos_prácticos_sistemas_de_preguntas_y_respuestas">8.4. 8.4 Ejemplos prácticos: sistemas de preguntas y respuestas</a></li>
</ul>
</li>
<li><a href="#_9_memoria_y_estado_en_aplicaciones_langchain">9. 9. Memoria y Estado en Aplicaciones LangChain</a>
<ul class="sectlevel2">
<li><a href="#_9_1_qué_es_la_memoria_en_langchain">9.1. 9.1 ¿Qué es la memoria en LangChain?</a></li>
<li><a href="#_9_2_tipos_de_memoria_conversationbuffer_summary_entity_etc">9.2. 9.2 Tipos de memoria: ConversationBuffer, Summary, Entity, etc.</a></li>
<li><a href="#_9_3_implementación_de_memoria_en_chatbots">9.3. 9.3 Implementación de memoria en chatbots</a></li>
<li><a href="#_9_4_casos_de_uso_avanzados_de_memoria">9.4. 9.4 Casos de uso avanzados de memoria</a></li>
</ul>
</li>
<li><a href="#_10_agentes_en_langchain">10. 10. Agentes en LangChain</a>
<ul class="sectlevel2">
<li><a href="#_10_1_qué_es_un_agente_y_cómo_funciona">10.1. 10.1 ¿Qué es un agente y cómo funciona?</a></li>
<li><a href="#_10_2_agentes_reactivos_y_planificadores">10.2. 10.2 Agentes reactivos y planificadores</a></li>
<li><a href="#_10_3_herramientas_y_plugins_para_agentes">10.3. 10.3 Herramientas y plugins para agentes</a></li>
<li><a href="#_10_4_agentes_para_búsqueda_web_análisis_sql_y_más">10.4. 10.4 Agentes para búsqueda web, análisis SQL, y más</a></li>
<li><a href="#_10_5_creación_de_agentes_conversacionales_personalizados">10.5. 10.5 Creación de agentes conversacionales personalizados</a></li>
</ul>
</li>
<li><a href="#_11_integración_con_apis_y_herramientas_externas">11. 11. Integración con APIs y Herramientas Externas</a>
<ul class="sectlevel2">
<li><a href="#_11_1_conexión_con_apis_rest_y_servicios_externos">11.1. 11.1 Conexión con APIs REST y servicios externos</a></li>
<li><a href="#_11_2_integración_con_google_aws_y_otras_plataformas">11.2. 11.2 Integración con Google, AWS, y otras plataformas</a></li>
<li><a href="#_11_3_automatización_de_flujos_de_trabajo_con_agentes">11.3. 11.3 Automatización de flujos de trabajo con agentes</a></li>
</ul>
</li>
<li><a href="#_12_desarrollo_de_aplicaciones_conversacionales">12. 12. Desarrollo de Aplicaciones Conversacionales</a>
<ul class="sectlevel2">
<li><a href="#_12_1_construcción_de_chatbots_avanzados">12.1. 12.1 Construcción de chatbots avanzados</a></li>
<li><a href="#_12_2_manejo_de_contexto_y_multihilo">12.2. 12.2 Manejo de contexto y multihilo</a></li>
<li><a href="#_12_3_integración_de_memoria_y_rag_en_chatbots">12.3. 12.3 Integración de memoria y RAG en chatbots</a></li>
<li><a href="#_12_4_ejemplos_de_asistentes_virtuales_y_casos_reales">12.4. 12.4 Ejemplos de asistentes virtuales y casos reales</a></li>
</ul>
</li>
<li><a href="#_13_despliegue_y_producción">13. 13. Despliegue y Producción</a>
<ul class="sectlevel2">
<li><a href="#_13_1_opciones_de_despliegue_local_cloud_serverless">13.1. 13.1 Opciones de despliegue (local, cloud, serverless)</a></li>
<li><a href="#_13_2_integración_con_frameworks_web_fastapi_gradio_streamlit">13.2. 13.2 Integración con frameworks web (FastAPI, Gradio, Streamlit)</a></li>
<li><a href="#_13_3_seguridad_autenticación_y_control_de_acceso">13.3. 13.3 Seguridad, autenticación y control de acceso</a></li>
<li><a href="#_13_4_monitorización_y_logging_de_aplicaciones">13.4. 13.4 Monitorización y logging de aplicaciones</a></li>
</ul>
</li>
<li><a href="#_15_recursos_y_siguientes_pasos">14. 15. Recursos y Siguientes Pasos</a>
<ul class="sectlevel2">
<li><a href="#_15_1_documentación_oficial_y_comunidad">14.1. 15.1 Documentación oficial y comunidad</a></li>
<li><a href="#_15_2_repositorios_y_ejemplos_recomendados">14.2. 15.2 Repositorios y ejemplos recomendados</a></li>
<li><a href="#_15_3_roadmap_avanzado_integración_con_agentes_autónomos_y_nuevas_tendencias">14.3. 15.3 Roadmap avanzado: integración con agentes autónomos y nuevas tendencias</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="_1_introducción_a_langchain">1. 1. Introducción a LangChain</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_1_1_qué_es_langchain_y_para_qué_sirve">1.1. 1.1 ¿Qué es LangChain y para qué sirve?</h3>
<div class="paragraph">
<p>LangChain es un framework de código abierto diseñado para facilitar la creación de aplicaciones que integran modelos de lenguaje de gran tamaño (LLM) como GPT-3, GPT-4, LLaMA, Claude y otros[1][2][14][17]. Permite a desarrolladores y científicos de datos construir sistemas avanzados de IA conversacional, chatbots, asistentes virtuales, motores de búsqueda semántica, sistemas de generación de contenido y aplicaciones de análisis de documentos, entre otros[3][5][8][17]. LangChain actúa como una capa de orquestación que conecta los LLM con fuentes de datos externas (APIs, bases de datos, documentos), gestiona el estado de la conversación y permite la automatización de flujos de trabajo complejos mediante cadenas de procesamiento (chains) y agentes inteligentes[1][2][9][10][14]. Su modularidad y flexibilidad hacen posible personalizar, escalar y desplegar aplicaciones de IA de forma rápida y eficiente, superando las limitaciones de los LLM puros al dotarlos de acceso a información actualizada y capacidades de razonamiento multi-paso[13][16][17].</p>
</div>
</div>
<div class="sect2">
<h3 id="_1_2_historia_y_evolución_de_langchain">1.2. 1.2 Historia y evolución de LangChain</h3>
<div class="paragraph">
<p>LangChain fue lanzado como proyecto de código abierto por Harrison Chase en octubre de 2022, inicialmente en Python y luego en JavaScript[2][5][12][20]. Su aparición coincidió con el auge de ChatGPT, lo que impulsó su adopción masiva y lo convirtió en el proyecto open source de más rápido crecimiento en GitHub en 2023[2][5][12][20]. LangChain ha evolucionado rápidamente, incorporando nuevas funcionalidades como el LangChain Expression Language (LCEL) para definir cadenas de acciones de forma declarativa, y herramientas como LangServe para desplegar aplicaciones como APIs de producción[5]. El framework ha recibido inversiones significativas y ha establecido alianzas con líderes tecnológicos, expandiendo su ecosistema y capacidades[4][5]. Su desarrollo continuo ha permitido integrar docenas de conectores a servicios cloud, bases de datos vectoriales, sistemas de almacenamiento, herramientas de análisis y más de 50 tipos de fuentes de datos[5][13][14][18].</p>
</div>
</div>
<div class="sect2">
<h3 id="_1_3_casos_de_uso_y_aplicaciones_en_la_industria">1.3. 1.3 Casos de uso y aplicaciones en la industria</h3>
<div class="paragraph">
<p>LangChain se utiliza en una amplia variedad de sectores y casos de uso, entre los que destacan[6][8][15][16][17]:
- <strong>Chatbots y asistentes virtuales</strong>: Empresas de atención al cliente implementan chatbots avanzados que mantienen contexto, resuelven dudas frecuentes y automatizan tareas repetitivas.
- <strong>Análisis y generación de documentos</strong>: Plataformas legales y de recursos humanos usan LangChain para resumir contratos, extraer información clave y generar informes automáticos.
- <strong>Sistemas de búsqueda y RAG (Retrieval-Augmented Generation)</strong>: Bibliotecas digitales, plataformas educativas y empresas tecnológicas emplean LangChain para búsquedas semánticas y respuestas basadas en fuentes actualizadas.
- <strong>Automatización de procesos empresariales</strong>: Compañías de finanzas, salud y marketing automatizan flujos de trabajo complejos, como análisis de sentimiento, generación de reportes y traducción automática.
- <strong>Integración con herramientas y APIs</strong>: LangChain permite crear agentes que interactúan con APIs externas, bases de datos, sistemas de almacenamiento y servicios cloud, facilitando la creación de asistentes personalizados y herramientas de productividad[7][15][16].
- <strong>Casos reales</strong>: Morningstar desarrolló un motor de inteligencia de inversiones; Elastic AI Assistant aceleró su desarrollo de productos; Retool mejoró la precisión de modelos personalizados gracias a LangChain[8][15].</p>
</div>
</div>
<div class="sect2">
<h3 id="_1_4_arquitectura_general_y_componentes_clave">1.4. 1.4 Arquitectura general y componentes clave</h3>
<div class="paragraph">
<p>La arquitectura de LangChain es modular y está compuesta por varios componentes esenciales que permiten construir aplicaciones complejas de IA[9][10][11][14][18][19]:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Models (Modelos)</strong>: Interfaces para conectar y gestionar diferentes LLMs (OpenAI, Hugging Face, modelos propios), permitiendo intercambiarlos fácilmente[10][19].</p>
</li>
<li>
<p><strong>Prompts (Plantillas de indicaciones)</strong>: Herramientas para crear, gestionar y reutilizar prompts, incluyendo plantillas dinámicas y few-shot learning para mejorar la precisión de las respuestas[10][19].</p>
</li>
<li>
<p><strong>Chains (Cadenas)</strong>: Secuencias de pasos (enlaces) que procesan datos, interactúan con modelos y realizan tareas compuestas. Permiten orquestar flujos de trabajo complejos y multi-etapa[9][13][14].</p>
</li>
<li>
<p><strong>Agents (Agentes)</strong>: Programas inteligentes capaces de tomar decisiones sobre qué herramientas o cadenas ejecutar en función de la consulta del usuario. Los agentes pueden interactuar con APIs, buscar información, ejecutar código y más[9][10][19].</p>
</li>
<li>
<p><strong>Memory (Memoria)</strong>: Módulos para gestionar el contexto y el historial de las conversaciones, permitiendo respuestas personalizadas y contextuales en chatbots y asistentes[10][19].</p>
</li>
<li>
<p><strong>Retrievers y RAG</strong>: Herramientas para buscar y recuperar información relevante de bases de datos vectoriales, documentos o APIs externas, integrando RAG (Retrieval-Augmented Generation) para respuestas basadas en datos actualizados[13][16].</p>
</li>
<li>
<p><strong>Document Loaders y Embeddings</strong>: Componentes para cargar, fragmentar y vectorizar documentos, facilitando la búsqueda semántica y el análisis de grandes volúmenes de texto[10][11][19].</p>
</li>
<li>
<p><strong>Callbacks y Monitorización</strong>: Permiten registrar, monitorear y depurar el funcionamiento de las cadenas y agentes, facilitando el mantenimiento y la mejora continua[13][19].</p>
</li>
<li>
<p><strong>Integraciones y conectores</strong>: Más de 50 integraciones con servicios cloud, bases de datos, APIs, almacenamiento y herramientas externas, ampliando el alcance y las capacidades de las aplicaciones construidas con LangChain[5][13][18].</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Esta arquitectura flexible y componible permite a los desarrolladores crear desde simples chatbots hasta complejos sistemas de IA empresarial, integrando múltiples fuentes de datos, herramientas y modelos de lenguaje de última generación.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_2_instalación_y_configuración_del_entorno_para_trabajar_con_modelos_ollama_en_local">2. 2. Instalación y Configuración del Entorno para trabajar con modelos Ollama en local</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_2_1_requisitos_previos">2.1. 2.1 Requisitos previos</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Sistema operativo compatible</strong>: macOS 12+, Windows 10/11 (preferiblemente con WSL2), o Linux (Ubuntu 20.04+, Debian 11+, Fedora 37+)[1][2][5].</p>
</li>
<li>
<p><strong>Hardware mínimo</strong>:</p>
</li>
<li>
<p>Procesador de 64 bits.</p>
</li>
<li>
<p>8GB de RAM (16GB recomendado para modelos grandes)[1][5].</p>
</li>
<li>
<p>10GB de espacio libre en disco (más para modelos de mayor tamaño)[1][5].</p>
</li>
<li>
<p>GPU NVIDIA/AMD opcional para acelerar la inferencia, pero Ollama funciona también en CPU[1][5][7].</p>
</li>
<li>
<p><strong>Python 3.8+</strong> instalado si se va a usar integración con LangChain u otros frameworks de IA.</p>
</li>
<li>
<p><strong>Docker</strong> instalado si se desea usar la interfaz web Open WebUI o desplegar Ollama en contenedores[3][5][6].</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_2_2_instalación_de_langchain_y_dependencias">2.2. 2.2 Instalación de LangChain y dependencias</h3>
<div class="ulist">
<ul>
<li>
<p>Crear y activar un entorno virtual Python:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">python -m venv ollama-env
source ollama-env/bin/activate      # Linux/Mac
ollama-env\Scripts\activate.bat     # Windows</code></pre>
</div>
</div>
</li>
<li>
<p>Instalar dependencias esenciales:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">pip install langchain-ollama python-dotenv</code></pre>
</div>
</div>
</li>
<li>
<p>(Opcional) Instalar librerías para búsqueda semántica y RAG:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">pip install chromadb faiss-cpu</code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_2_3_configuración_de_ollama_y_modelos_locales">2.3. 2.3 Configuración de Ollama y modelos locales</h3>
<div class="ulist">
<ul>
<li>
<p>Descargar e instalar Ollama:</p>
</li>
<li>
<p><strong>Linux</strong>:
+
[source,bash]
----
curl -fsSL <a href="https://ollama.com/install.sh" class="bare">https://ollama.com/install.sh</a> | sh
----</p>
</li>
<li>
<p><strong>macOS</strong>:
+
[source,bash]
----
brew install ollama
----
o descargar el instalador desde la web oficial[1][2][5].</p>
</li>
<li>
<p><strong>Windows</strong>:
+
Descargar el instalador desde <a href="https://ollama.com" class="bare">https://ollama.com</a> y ejecutarlo, o instalar Ollama dentro de WSL2 siguiendo los pasos de Linux[5].</p>
</li>
<li>
<p>Verificar instalación:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">ollama --version</code></pre>
</div>
</div>
</li>
<li>
<p>Iniciar el servicio Ollama:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">ollama serve</code></pre>
</div>
</div>
</li>
<li>
<p>Descargar modelos LLM locales (ejemplo con Llama 3):</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">ollama pull llama3</code></pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>Puedes listar todos los modelos disponibles con:</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">ollama list</code></pre>
</div>
</div>
</li>
<li>
<p>(Opcional) Instalar y lanzar la interfaz web Open WebUI:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data --name open-webui --restart always \
  ghcr.io/open-webui/open-webui:main</code></pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>Accede a la UI en &lt;http://localhost:3000&gt;[3][6].</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_2_4_verificación_de_la_instalación_y_primeros_tests">2.4. 2.4 Verificación de la instalación y primeros tests</h3>
<div class="ulist">
<ul>
<li>
<p>Probar Ollama desde terminal:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">ollama run llama3 &quot;¿Cuál es la capital de Francia?&quot;</code></pre>
</div>
</div>
</li>
<li>
<p>Probar integración con LangChain desde Python:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaLLM</span>

llm = OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3</span><span class="delimiter">&quot;</span></span>)
respuesta = llm.invoke(<span class="string"><span class="delimiter">&quot;</span><span class="content">Resume en una frase la teoría de la relatividad.</span><span class="delimiter">&quot;</span></span>)
print(respuesta)</code></pre>
</div>
</div>
</li>
<li>
<p>Verificar que el modelo responde correctamente y que no aparecen errores de conexión.</p>
</li>
<li>
<p>Si usas la interfaz web, prueba cargar un modelo y realizar una consulta desde el navegador.</p>
</li>
<li>
<p>Para comprobar uso de GPU, puedes monitorizar con <code>nvidia-smi</code> (en sistemas compatibles).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Con estos pasos, tendrás un entorno local listo para experimentar y desarrollar aplicaciones de IA generativa con modelos Ollama y LangChain, sin depender de la nube ni exponer tus datos fuera de tu equipo[1][5][6].</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_3_fundamentos_de_llms_y_su_integración_con_langchain">3. 3. Fundamentos de LLMs y su integración con LangChain</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_3_1_qué_es_un_large_language_model_llm">3.1. 3.1 ¿Qué es un Large Language Model (LLM)?</h3>
<div class="paragraph">
<p>Un <strong>Large Language Model (LLM)</strong> es un modelo de inteligencia artificial de aprendizaje profundo entrenado con enormes cantidades de texto (libros, artículos, código, webs) para comprender, generar y manipular lenguaje humano de forma avanzada. Utilizan arquitecturas basadas en transformers y mecanismos de autoatención, permitiendo captar relaciones complejas entre palabras y frases. Los LLMs predicen el siguiente token en una secuencia, lo que les permite generar texto coherente, resumir, traducir, responder preguntas y mucho más. Ejemplos de LLMs incluyen GPT-3/4 (OpenAI), LLaMA 3 (Meta), Mistral 7B, entre otros.</p>
</div>
</div>
<div class="sect2">
<h3 id="_3_2_integración_de_modelos_open_source_y_comerciales">3.2. 3.2 Integración de modelos open source y comerciales</h3>
<div class="paragraph">
<p>LangChain facilita la integración tanto de modelos open source (código abierto) como comerciales, proporcionando interfaces estandarizadas para trabajar con ambos tipos de modelos. Los modelos open source (como LLaMA, Mistral, BERT) pueden ejecutarse localmente o a través de plataformas como Hugging Face, brindando control total y privacidad. Los modelos comerciales (como GPT-4, Claude, Cohere) se consumen mediante APIs en la nube, ofreciendo acceso a modelos de última generación y actualizaciones continuas, aunque con dependencia de proveedores externos y costes asociados.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 33.3333%;">
<col style="width: 33.3335%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Tipo</th>
<th class="tableblock halign-left valign-top">Ejemplos</th>
<th class="tableblock halign-left valign-top">Ventajas</th>
<th class="tableblock halign-left valign-top">Clase LangChain</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Open Source</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">LLaMA, Mistral, BERT</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Privacidad, control total</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">HuggingFacePipeline</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Comerciales</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GPT-4, Claude, Cohere</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Alto rendimiento, actualizaciones</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ChatOpenAI</p></td>
</tr>
</tbody>
</table>
<div class="listingblock">
<div class="title">Ejemplo de cambio rápido entre modelos:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">HuggingFacePipeline</span>
<span class="keyword">from</span> <span class="include">langchain_openai</span> <span class="keyword">import</span> <span class="include">ChatOpenAI</span>

modelo_oss = HuggingFacePipeline.from_model_id(<span class="string"><span class="delimiter">&quot;</span><span class="content">mistralai/Mistral-7B</span><span class="delimiter">&quot;</span></span>)
modelo_comercial = ChatOpenAI(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">gpt-4-turbo</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_3_3_conexión_con_modelos_de_hugging_face">3.3. 3.3 Conexión con modelos de Hugging Face</h3>
<div class="paragraph">
<p>Para conectar modelos de Hugging Face en LangChain:</p>
</div>
<div class="paragraph">
<p>Instala las dependencias necesarias:</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">pip install langchain-huggingface transformers</code></pre>
</div>
</div>
<div class="paragraph">
<p>Configura tu token de Hugging Face si usas modelos en la nube:</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">os</span>
os.environ[<span class="string"><span class="delimiter">&quot;</span><span class="content">HUGGINGFACEHUB_API_TOKEN</span><span class="delimiter">&quot;</span></span>] = <span class="string"><span class="delimiter">&quot;</span><span class="content">hf_...</span><span class="delimiter">&quot;</span></span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Carga el modelo deseado:</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">HuggingFaceEndpoint</span>

<span class="comment"># Modelo remoto</span>
llm_remote = HuggingFaceEndpoint(repo_id=<span class="string"><span class="delimiter">&quot;</span><span class="content">google/flan-t5-xxl</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># Modelo local</span>
<span class="keyword">from</span> <span class="include">transformers</span> <span class="keyword">import</span> <span class="include">AutoModelForCausalLM</span>, <span class="include">AutoTokenizer</span>
model = AutoModelForCausalLM.from_pretrained(<span class="string"><span class="delimiter">&quot;</span><span class="content">mistralai/Mistral-7B</span><span class="delimiter">&quot;</span></span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="string"><span class="delimiter">&quot;</span><span class="content">mistralai/Mistral-7B</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_4_modelos_y_prompts_en_langchain">4. 4. Modelos y Prompts en LangChain</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_4_1_uso_básico_de_llms_y_chatmodels">4.1. 4.1 Uso básico de LLMs y ChatModels</h3>
<div class="paragraph">
<p>LangChain ofrece interfaces unificadas para trabajar con modelos de lenguaje de diferentes proveedores. Los <code>ChatModels</code> manejan mensajes estructurados con roles (system, human, assistant), mientras que los <code>LLMs</code> trabajan con texto plano.</p>
</div>
<div class="listingblock">
<div class="title">Ejemplo de ChatModel con OpenAI:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_openai</span> <span class="keyword">import</span> <span class="include">ChatOpenAI</span>
<span class="keyword">from</span> <span class="include">langchain_core.messages</span> <span class="keyword">import</span> <span class="include">SystemMessage</span>, <span class="include">HumanMessage</span>

chat = ChatOpenAI(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">gpt-4-turbo</span><span class="delimiter">&quot;</span></span>)
messages = [
    SystemMessage(<span class="string"><span class="delimiter">&quot;</span><span class="content">Eres un experto en historia del arte</span><span class="delimiter">&quot;</span></span>),
    HumanMessage(<span class="string"><span class="delimiter">&quot;</span><span class="content">Explica el cubismo en 50 palabras</span><span class="delimiter">&quot;</span></span>)
]
respuesta = chat.invoke(messages)
print(respuesta.content)</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Ejemplo de LLM local con Ollama:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">OllamaLLM</span>

llm = OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1:8b</span><span class="delimiter">&quot;</span></span>)
respuesta = llm.invoke(<span class="string"><span class="delimiter">&quot;</span><span class="content">Diferencia entre HTTP y HTTPS</span><span class="delimiter">&quot;</span></span>)
print(respuesta)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_4_2_creación_y_gestión_de_prompt_templates">4.2. 4.2 Creación y gestión de Prompt Templates</h3>
<div class="paragraph">
<p>Los <code>PromptTemplate</code> permiten crear plantillas reutilizables con variables dinámicas.</p>
</div>
<div class="listingblock">
<div class="title">Estructura básica:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_core.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>

plantilla = PromptTemplate.from_template(
    <span class="string"><span class="delimiter">&quot;</span><span class="content">Traduce al {idioma} el siguiente texto: {texto}</span><span class="delimiter">&quot;</span></span>
)
prompt_formateado = plantilla.format(idioma=<span class="string"><span class="delimiter">&quot;</span><span class="content">francés</span><span class="delimiter">&quot;</span></span>, texto=<span class="string"><span class="delimiter">&quot;</span><span class="content">Hola mundo</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Plantilla con múltiples variables:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">plantilla_avanzada = PromptTemplate(
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">producto</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">tono</span><span class="delimiter">&quot;</span></span>],
    template=<span class="string"><span class="delimiter">&quot;</span><span class="content">Escribe un tweet promocionando {producto} con un tono {tono}</span><span class="delimiter">&quot;</span></span>
)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_4_3_prompts_dinámicos_y_personalizados">4.3. 4.3 Prompts dinámicos y personalizados</h3>
<div class="paragraph">
<p>Se pueden crear prompts adaptativos usando lógica condicional y selección de ejemplos.</p>
</div>
<div class="listingblock">
<div class="title">Ejemplo con lógica condicional:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_core.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>

plantilla = <span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">{%- if formal -%}</span><span class="content">
</span><span class="content">Estimado {nombre}: {mensaje_formal}</span><span class="content">
</span><span class="content">{%- else -%}</span><span class="content">
</span><span class="content">¡Hola {nombre}! {mensaje_informal}</span><span class="content">
</span><span class="content">{%- endif -%}</span><span class="content">
</span><span class="delimiter">&quot;&quot;&quot;</span></span>

prompt = PromptTemplate.from_template(plantilla, template_format=<span class="string"><span class="delimiter">&quot;</span><span class="content">jinja2</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Selección de ejemplos con FewShotPromptTemplate:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_core.prompts</span> <span class="keyword">import</span> <span class="include">FewShotPromptTemplate</span>, <span class="include">PromptTemplate</span>

ejemplos = [
    {<span class="string"><span class="delimiter">&quot;</span><span class="content">palabra</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">feliz</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">antonimo</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">triste</span><span class="delimiter">&quot;</span></span>},
    {<span class="string"><span class="delimiter">&quot;</span><span class="content">palabra</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">rápido</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">antonimo</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">lento</span><span class="delimiter">&quot;</span></span>}
]

formato_ejemplo = PromptTemplate(
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">palabra</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">antonimo</span><span class="delimiter">&quot;</span></span>],
    template=<span class="string"><span class="delimiter">&quot;</span><span class="content">Palabra: {palabra}</span><span class="char">\n</span><span class="content">Antónimo: {antonimo}</span><span class="delimiter">&quot;</span></span>
)

prompt = FewShotPromptTemplate(
    examples=ejemplos,
    example_prompt=formato_ejemplo,
    prefix=<span class="string"><span class="delimiter">&quot;</span><span class="content">Lista de antónimos:</span><span class="delimiter">&quot;</span></span>,
    suffix=<span class="string"><span class="delimiter">&quot;</span><span class="content">Palabra: {input}</span><span class="char">\n</span><span class="content">Antónimo:</span><span class="delimiter">&quot;</span></span>
)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_4_4_buenas_prácticas_en_la_redacción_de_prompts">4.4. 4.4 Buenas prácticas en la redacción de prompts</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Reglas clave para prompts efectivos</strong>:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Especificidad</strong>: Definir claramente el formato y alcance de la respuesta</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="text">&quot;Genera 3 opciones de nombres para una startup de IoT. Formato: lista numerada&quot;</code></pre>
</div>
</div>
</li>
<li>
<p><strong>Contextualización</strong>: Proporcionar información relevante</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="text">&quot;Como experto en marketing digital con 10 años de experiencia, redacta...&quot;</code></pre>
</div>
</div>
</li>
<li>
<p><strong>Ejemplificación</strong>: Incluir casos de uso</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="text">&quot;Ejemplo de entrada: 'mesa de madera' → Ejemplo de salida: 'Tablero macizo de roble natural'&quot;</code></pre>
</div>
</div>
</li>
<li>
<p><strong>Validación</strong>: Implementar chequeos de formato</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">respuesta = llm.invoke(prompt)
<span class="keyword">if</span> <span class="keyword">not</span> respuesta.startswith(<span class="string"><span class="delimiter">&quot;</span><span class="content">1.</span><span class="delimiter">&quot;</span></span>):
    <span class="keyword">raise</span> <span class="exception">ValueError</span>(<span class="string"><span class="delimiter">&quot;</span><span class="content">Formato de respuesta inválido</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</li>
<li>
<p><strong>Iteración</strong>: Refinar mediante pruebas A/B</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="text">Versión A: &quot;Resume el texto en 100 palabras&quot;
Versión B: &quot;Extrae los 3 puntos clave principales&quot;</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. Tabla comparativa de enfoques:</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 40%;">
<col style="width: 40%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Técnica</th>
<th class="tableblock halign-left valign-top">Ventajas</th>
<th class="tableblock halign-left valign-top">Caso de uso típico</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Plantillas simples</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Rápidas de implementar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Traducciones, resúmenes</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lógica condicional</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Adaptabilidad contextual</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Chatbots, respuestas personalizadas</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Few-shot learning</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Mayor precisión</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Clasificación de texto</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_5_chains_composición_y_orquestación">5. 5. Chains: Composición y Orquestación</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_5_1_qué_es_una_chain_en_langchain">5.1. 5.1 ¿Qué es una Chain en LangChain?</h3>
<div class="paragraph">
<p>Una <strong>Chain</strong> en LangChain es una secuencia de pasos donde la salida de un componente (como un modelo Ollama local, función o herramienta) se convierte en la entrada del siguiente. Esto permite construir flujos de trabajo complejos y modulares, integrando procesamiento de datos, lógica condicional y uso de modelos LLM locales de Ollama en una sola aplicación de IA. Las chains pueden ser simples (lineales) o complejas (anidadas o con lógica condicional), y son la base para organizar tareas y procesos de IA de manera mantenible y escalable.</p>
</div>
</div>
<div class="sect2">
<h3 id="_5_2_chains_simples_y_secuenciales">5.2. 5.2 Chains simples y secuenciales</h3>
<div class="paragraph">
<p>Las <strong>chains simples</strong> conectan varios pasos de manera lineal, donde cada paso recibe una entrada y genera una salida, que pasa al siguiente paso. Esto es útil para pipelines claros y directos, como generación de texto seguida de traducción o resumen.</p>
</div>
<div class="listingblock">
<div class="title">Ejemplo de SimpleSequentialChain con modelos Ollama:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain_core.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">LLMChain</span>, <span class="include">SimpleSequentialChain</span>

<span class="comment"># Paso 1: Generar una idea de producto</span>
prompt1 = PromptTemplate.from_template(<span class="string"><span class="delimiter">&quot;</span><span class="content">Sugiere una idea de producto sobre {tema}.</span><span class="delimiter">&quot;</span></span>)
chain1 = LLMChain(llm=OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>), prompt=prompt1)

<span class="comment"># Paso 2: Generar un eslogan para la idea</span>
prompt2 = PromptTemplate.from_template(<span class="string"><span class="delimiter">&quot;</span><span class="content">Crea un eslogan para este producto: {text}</span><span class="delimiter">&quot;</span></span>)
chain2 = LLMChain(llm=OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>), prompt=prompt2)

<span class="comment"># Encadenar ambos pasos</span>
chain = SimpleSequentialChain(chains=[chain1, chain2])
resultado = chain.run(<span class="string"><span class="delimiter">&quot;</span><span class="content">hogares inteligentes</span><span class="delimiter">&quot;</span></span>)
print(resultado)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_5_3_chains_personalizadas_y_anidadas">5.3. 5.3 Chains personalizadas y anidadas</h3>
<div class="paragraph">
<p>Puedes crear <strong>chains personalizadas</strong> combinando componentes básicos y definiendo la lógica de conexión entre ellos. Se pueden anidar chains, es decir, que la salida de una chain sea la entrada de otra, o construir flujos con lógica condicional y ramificaciones.</p>
</div>
<div class="listingblock">
<div class="title">Ejemplo de chain anidada con Ollama:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain_core.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">LLMChain</span>, <span class="include">SequentialChain</span>

<span class="comment"># Chain 1: Resumir un texto</span>
prompt_resumen = PromptTemplate.from_template(<span class="string"><span class="delimiter">&quot;</span><span class="content">Resume el siguiente texto: {texto}</span><span class="delimiter">&quot;</span></span>)
chain_resumen = LLMChain(llm=OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>), prompt=prompt_resumen)

<span class="comment"># Chain 2: Extraer palabras clave del resumen</span>
prompt_keywords = PromptTemplate.from_template(<span class="string"><span class="delimiter">&quot;</span><span class="content">Extrae palabras clave del siguiente resumen: {resumen}</span><span class="delimiter">&quot;</span></span>)
chain_keywords = LLMChain(llm=OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>), prompt=prompt_keywords)

<span class="comment"># Encadenar ambas usando SequentialChain</span>
chain = SequentialChain(
    chains=[chain_resumen, chain_keywords],
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">texto</span><span class="delimiter">&quot;</span></span>],
    output_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">resumen</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">palabras_clave</span><span class="delimiter">&quot;</span></span>]
)
salida = chain({<span class="string"><span class="delimiter">&quot;</span><span class="content">texto</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">La inteligencia artificial está revolucionando la industria tecnológica...</span><span class="delimiter">&quot;</span></span>})
print(salida)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_5_4_chains_para_preguntas_y_respuestas">5.4. 5.4 Chains para preguntas y respuestas</h3>
<div class="paragraph">
<p>LangChain permite crear chains especializadas para sistemas de <strong>preguntas y respuestas</strong> (QA), combinando recuperación de contexto relevante con generación de respuestas por parte de un modelo Ollama local.</p>
</div>
<div class="listingblock">
<div class="title">Ejemplo de QA chain con Ollama:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">LLMChain</span>
<span class="keyword">from</span> <span class="include">langchain_core.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>

prompt = PromptTemplate.from_template(
    <span class="string"><span class="delimiter">&quot;</span><span class="content">Pregunta: {pregunta}</span><span class="char">\n</span><span class="content">Texto de referencia: {contexto}</span><span class="char">\n</span><span class="content">Respuesta:</span><span class="delimiter">&quot;</span></span>
)
chain = LLMChain(llm=OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>), prompt=prompt)

contexto = <span class="string"><span class="delimiter">&quot;</span><span class="content">LangChain es un framework para orquestar modelos de lenguaje en aplicaciones de IA.</span><span class="delimiter">&quot;</span></span>
pregunta = <span class="string"><span class="delimiter">&quot;</span><span class="content">¿Para qué sirve LangChain?</span><span class="delimiter">&quot;</span></span>
respuesta = chain.invoke({<span class="string"><span class="delimiter">&quot;</span><span class="content">pregunta</span><span class="delimiter">&quot;</span></span>: pregunta, <span class="string"><span class="delimiter">&quot;</span><span class="content">contexto</span><span class="delimiter">&quot;</span></span>: contexto})
print(respuesta)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_5_5_manejo_de_múltiples_entradas_y_salidas">5.5. 5.5 Manejo de múltiples entradas y salidas</h3>
<div class="paragraph">
<p>Para flujos complejos donde cada paso requiere o genera varios datos, puedes usar <code>SequentialChain</code> y mapear explícitamente variables entre pasos, o emplear el LangChain Expression Language (LCEL) para componer pipelines avanzados.</p>
</div>
<div class="listingblock">
<div class="title">Ejemplo de SequentialChain con múltiples variables y Ollama:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain_core.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">LLMChain</span>, <span class="include">SequentialChain</span>

<span class="comment"># Paso 1: Generar idea y resumen</span>
prompt_idea = PromptTemplate.from_template(<span class="string"><span class="delimiter">&quot;</span><span class="content">Sugiere una idea innovadora sobre {tema} en el sector {sector}.</span><span class="delimiter">&quot;</span></span>)
chain_idea = LLMChain(llm=OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>), prompt=prompt_idea)

prompt_resumen = PromptTemplate.from_template(<span class="string"><span class="delimiter">&quot;</span><span class="content">Resume en una frase la siguiente idea: {idea}</span><span class="delimiter">&quot;</span></span>)
chain_resumen = LLMChain(llm=OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>), prompt=prompt_resumen)

chain = SequentialChain(
    chains=[chain_idea, chain_resumen],
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">tema</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">sector</span><span class="delimiter">&quot;</span></span>],
    output_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">idea</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">resumen</span><span class="delimiter">&quot;</span></span>]
)
salida = chain({<span class="string"><span class="delimiter">&quot;</span><span class="content">tema</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">energía renovable</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">sector</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">hogar</span><span class="delimiter">&quot;</span></span>})
print(salida)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Estas técnicas permiten construir pipelines robustos y adaptables, orquestando modelos Ollama locales de forma eficiente y privada.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_6_manejo_y_procesamiento_de_documentos_con_ollama_y_langchain">6. 6. Manejo y Procesamiento de Documentos con Ollama y LangChain</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_6_1_document_loaders_carga_de_pdfs_txt_web_etc">6.1. 6.1 Document Loaders: carga de PDFs, TXT, web, etc.</h3>
<div class="paragraph">
<p>LangChain dispone de loaders especializados para cargar datos de distintos formatos y fuentes, convirtiéndolos en objetos Document estandarizados que pueden ser utilizados por modelos Ollama en local[1][2][3][4][5].</p>
</div>
<div class="listingblock">
<div class="title">Cargar un archivo PDF:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">PyPDFLoader</span>

loader = PyPDFLoader(<span class="string"><span class="delimiter">&quot;</span><span class="content">documento.pdf</span><span class="delimiter">&quot;</span></span>)
documents = loader.load()  <span class="comment"># Devuelve una lista de Document, uno por página</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Cargar un archivo TXT:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">TextLoader</span>

loader = TextLoader(<span class="string"><span class="delimiter">&quot;</span><span class="content">notas.txt</span><span class="delimiter">&quot;</span></span>)
documents = loader.load()</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Cargar varios archivos de un directorio:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">DirectoryLoader</span>, <span class="include">TextLoader</span>

loader = DirectoryLoader(
    <span class="string"><span class="delimiter">&quot;</span><span class="content">./mis_docs</span><span class="delimiter">&quot;</span></span>,
    glob=<span class="string"><span class="delimiter">&quot;</span><span class="content">**/*.txt</span><span class="delimiter">&quot;</span></span>,
    loader_cls=TextLoader
)
documents = loader.load()</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Cargar contenido de una web:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">AsyncHtmlLoader</span>

urls = [<span class="string"><span class="delimiter">&quot;</span><span class="content">https://es.wikipedia.org/wiki/LangChain</span><span class="delimiter">&quot;</span></span>]
loader = AsyncHtmlLoader(urls)
documents = loader.load()</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Cargar un archivo CSV:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.document_loaders.csv_loader</span> <span class="keyword">import</span> <span class="include">CSVLoader</span>

loader = CSVLoader(<span class="string"><span class="delimiter">&quot;</span><span class="content">./datos.csv</span><span class="delimiter">&quot;</span></span>)
documents = loader.load()</code></pre>
</div>
</div>
<div class="paragraph">
<p>Cada loader transforma el contenido y los metadatos en una estructura Document, facilitando la integración, el procesamiento y la indexación para tareas posteriores como chunking, embedding y búsqueda semántica con modelos Ollama.</p>
</div>
</div>
<div class="sect2">
<h3 id="_6_2_procesamiento_y_limpieza_de_documentos">6.2. 6.2 Procesamiento y limpieza de documentos</h3>
<div class="paragraph">
<p>El procesamiento y limpieza de documentos es un paso fundamental antes de utilizar modelos Ollama en local con LangChain, ya que mejora la calidad de los datos y la precisión de las respuestas generadas[1][6][9].</p>
</div>
<div class="ulist">
<div class="title">Pasos habituales de limpieza:</div>
<ul>
<li>
<p>Eliminación de frases o marcas irrelevantes (por ejemplo, anuncios o firmas automáticas)</p>
</li>
<li>
<p>Normalización de espacios y saltos de línea</p>
</li>
<li>
<p>Eliminación de caracteres no imprimibles o especiales</p>
</li>
<li>
<p>Conversión de texto a un formato uniforme y legible por el modelo</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Ejemplo de función de limpieza básica:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">re</span>

<span class="keyword">def</span> <span class="function">clean_text</span>(text):
    <span class="comment"># Eliminar frases específicas no deseadas</span>
    cleaned_text = re.sub(<span class="string"><span class="modifier">r</span><span class="delimiter">'</span><span class="content">\s</span><span class="content">*Free eBooks at Planet eBook</span><span class="content">\.</span><span class="content">com</span><span class="content">\s</span><span class="content">*</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="delimiter">'</span></span>, text, flags=re.DOTALL)
    <span class="comment"># Eliminar espacios adicionales</span>
    cleaned_text = re.sub(<span class="string"><span class="modifier">r</span><span class="delimiter">'</span><span class="content"> +</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content"> </span><span class="delimiter">'</span></span>, cleaned_text)
    <span class="comment"># Eliminar caracteres no imprimibles</span>
    cleaned_text = re.sub(<span class="string"><span class="modifier">r</span><span class="delimiter">'</span><span class="content">[</span><span class="content">\x</span><span class="content">00-</span><span class="content">\x</span><span class="content">1F]</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="delimiter">'</span></span>, cleaned_text)
    <span class="comment"># Reemplazar saltos de línea por espacios</span>
    cleaned_text = cleaned_text.replace(<span class="string"><span class="delimiter">'</span><span class="char">\n</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content"> </span><span class="delimiter">'</span></span>)
    <span class="comment"># Eliminar espacios alrededor de guiones</span>
    cleaned_text = re.sub(<span class="string"><span class="modifier">r</span><span class="delimiter">'</span><span class="content">\s</span><span class="content">*-</span><span class="content">\s</span><span class="content">*</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="delimiter">'</span></span>, cleaned_text)
    <span class="keyword">return</span> cleaned_text

<span class="comment"># Aplicar limpieza a todos los documentos cargados</span>
<span class="keyword">for</span> doc <span class="keyword">in</span> documents:
    doc.page_content = clean_text(doc.page_content)</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Estrategias adicionales de procesamiento:</div>
<ul>
<li>
<p>Preservar la estructura relevante del documento (títulos, secciones) si es importante para el análisis posterior[3][6].</p>
</li>
<li>
<p>Tokenizar el texto si se requiere para procesamiento avanzado o chunking.</p>
</li>
<li>
<p>Identificar y eliminar duplicados o fragmentos irrelevantes.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>LangChain facilita estas tareas mediante su diseño modular, permitiendo integrar funciones de limpieza personalizadas antes de dividir los documentos o generar embeddings para búsqueda y recuperación[6][9]. Una buena limpieza y preprocesamiento asegura que los modelos Ollama trabajen con datos de calidad y produzcan resultados más útiles y precisos.</p>
</div>
</div>
<div class="sect2">
<h3 id="_6_3_splitters_fragmentación_de_texto_y_chunking">6.3. 6.3 Splitters: fragmentación de texto y chunking</h3>
<div class="paragraph">
<p>La fragmentación de texto (chunking) es esencial cuando se trabaja con documentos largos en LangChain y Ollama, ya que los LLMs locales tienen límites de contexto y procesan mejor fragmentos manejables[2][5][7]. Los splitters permiten dividir documentos en partes más pequeñas, manteniendo la coherencia semántica y facilitando la recuperación de información relevante.</p>
</div>
<div class="sect3">
<h4 id="_tipos_de_splitters_y_estrategias">6.3.1. Tipos de splitters y estrategias</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Basados en longitud</strong>: Dividen el texto por número de caracteres o tokens, asegurando chunks de tamaño uniforme[5].</p>
</li>
<li>
<p><strong>Basados en estructura</strong>: Aprovechan la organización natural del texto (párrafos, frases, palabras) para mantener sentido y contexto[2][5].</p>
</li>
<li>
<p><strong>Solapamiento (overlap)</strong>: Añade parte del contenido del chunk anterior al siguiente, evitando pérdida de contexto entre fragmentos[2][7][8].</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_ejemplo_uso_de_recursivecharactertextsplitter">6.3.2. Ejemplo: Uso de RecursiveCharacterTextSplitter</h4>
<div class="paragraph">
<p>El splitter recomendado para la mayoría de aplicaciones es <code>RecursiveCharacterTextSplitter</code>, que intenta dividir primero por párrafos, luego por líneas, frases y finalmente palabras, hasta alcanzar el tamaño de chunk deseado[2][5][7].</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_text_splitters</span> <span class="keyword">import</span> <span class="include">RecursiveCharacterTextSplitter</span>

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=<span class="integer">1000</span>,        <span class="comment"># Máximo de caracteres por chunk</span>
    chunk_overlap=<span class="integer">200</span>,      <span class="comment"># Solapamiento entre fragmentos</span>
    separators=[<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">. </span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">? </span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">! </span><span class="delimiter">&quot;</span></span>],  <span class="comment"># Priorización de cortes</span>
)

<span class="comment"># Supón que 'documents' es una lista de Document cargados previamente</span>
chunks = text_splitter.split_documents(documents)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Cada elemento de <code>chunks</code> es un fragmento del documento original, listo para ser procesado por un modelo Ollama en local.</p>
</div>
</div>
<div class="sect3">
<h4 id="_ejemplo_splitter_personalizado_para_otros_idiomas_o_estructuras">6.3.3. Ejemplo: Splitter personalizado para otros idiomas o estructuras</h4>
<div class="paragraph">
<p>Puedes adaptar los separadores para textos en otros idiomas o con estructuras diferentes[2]:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=<span class="integer">800</span>,
    chunk_overlap=<span class="integer">100</span>,
    separators=[<span class="string"><span class="delimiter">&quot;</span><span class="content">。</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">．</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content"> </span><span class="delimiter">&quot;</span></span>],  <span class="comment"># Separadores para japonés/chino</span>
)
chunks = text_splitter.split_documents(documents)</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Ventajas del chunking con splitters</div>
<ul>
<li>
<p>Permite procesar grandes volúmenes de texto con modelos Ollama locales sin perder contexto relevante.</p>
</li>
<li>
<p>Mejora la eficiencia en tareas de búsqueda semántica y recuperación aumentada (RAG).</p>
</li>
<li>
<p>Facilita la extracción de información relevante y la generación de resúmenes o respuestas precisas.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>El uso adecuado de splitters es un paso fundamental en cualquier pipeline de procesamiento documental avanzado con LangChain y modelos LLM en local.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_6_4_extracción_de_información_relevante">6.4. 6.4 Extracción de información relevante</h3>
<div class="paragraph">
<p>La extracción de información relevante es el proceso de identificar, estructurar y obtener datos clave a partir de documentos extensos o fragmentos de texto, transformando información no estructurada en conocimiento útil y procesable[2][3][8]. Este proceso es fundamental para analizar grandes volúmenes de datos, automatizar flujos de trabajo y facilitar la toma de decisiones en entornos empresariales o de investigación.</p>
</div>
<div class="sect3">
<h4 id="_cómo_funciona_la_extracción_de_información">6.4.1. ¿Cómo funciona la extracción de información?</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Carga y digitalización</strong>: El documento se digitaliza (si es necesario) y se carga en el sistema, pudiendo ser en formatos como PDF, TXT, imágenes o páginas web[1][4][8].</p>
</li>
<li>
<p><strong>Conversión a texto</strong>: Si el documento es una imagen o PDF escaneado, se aplica OCR para obtener el texto editable[1][4].</p>
</li>
<li>
<p><strong>Procesamiento del texto</strong>: El texto se limpia, normaliza y se divide en fragmentos (chunks) para facilitar su análisis por modelos LLM locales como Ollama.</p>
</li>
<li>
<p><strong>Extracción automatizada</strong>: Se utilizan técnicas de Procesamiento del Lenguaje Natural (PLN) y modelos de IA para identificar entidades, relaciones, fechas, cifras, temas, etc.[3][4][8].</p>
</li>
<li>
<p><strong>Estructuración y almacenamiento</strong>: La información extraída se organiza en formatos estructurados (JSON, CSV, tablas) para su consulta, análisis o integración con otros sistemas[2][3][8].</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_técnicas_y_métodos_de_extracción">6.4.2. Técnicas y métodos de extracción</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Extracción basada en reglas</strong>: Usa patrones predefinidos (expresiones regulares, palabras clave) para identificar datos específicos como fechas, nombres o importes[3].</p>
</li>
<li>
<p><strong>Extracción basada en aprendizaje automático</strong>: Utiliza modelos entrenados para reconocer entidades y relaciones en el texto, permitiendo mayor flexibilidad y precisión[3][4].</p>
</li>
<li>
<p><strong>Extracción semántica con LLMs</strong>: Los modelos Ollama pueden resumir, clasificar, extraer entidades o responder preguntas directamente sobre los fragmentos de texto, combinando comprensión contextual y generación de lenguaje natural[8][9].</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_ejemplo_1_resumir_fragmentos_de_texto_con_ollama">6.4.3. Ejemplo 1: Resumir fragmentos de texto con Ollama</h4>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">LLMChain</span>
<span class="keyword">from</span> <span class="include">langchain_core.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>

prompt = PromptTemplate.from_template(<span class="string"><span class="delimiter">&quot;</span><span class="content">Resume el siguiente texto en 2 frases: {texto}</span><span class="delimiter">&quot;</span></span>)

llm = OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>)
chain = LLMChain(llm=llm, prompt=prompt)

<span class="keyword">for</span> chunk <span class="keyword">in</span> chunks:
    resumen = chain.invoke({<span class="string"><span class="delimiter">&quot;</span><span class="content">texto</span><span class="delimiter">&quot;</span></span>: chunk.page_content})
    print(resumen)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_ejemplo_2_extracción_de_entidades_nombradas">6.4.4. Ejemplo 2: Extracción de entidades nombradas</h4>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">prompt_entidades = PromptTemplate.from_template(
    <span class="string"><span class="delimiter">&quot;</span><span class="content">Extrae todas las personas, lugares y fechas del siguiente texto: {texto}</span><span class="delimiter">&quot;</span></span>
)
chain_entidades = LLMChain(llm=llm, prompt=prompt_entidades)

<span class="keyword">for</span> chunk <span class="keyword">in</span> chunks:
    entidades = chain_entidades.invoke({<span class="string"><span class="delimiter">&quot;</span><span class="content">texto</span><span class="delimiter">&quot;</span></span>: chunk.page_content})
    print(entidades)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_ejemplo_3_clasificación_temática_de_fragmentos">6.4.5. Ejemplo 3: Clasificación temática de fragmentos</h4>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">prompt_clasificacion = PromptTemplate.from_template(
    <span class="string"><span class="delimiter">&quot;</span><span class="content">Clasifica el siguiente texto en una de estas categorías: Ciencia, Tecnología, Historia, Otro. Texto: {texto}</span><span class="delimiter">&quot;</span></span>
)
chain_clasificacion = LLMChain(llm=llm, prompt=prompt_clasificacion)

<span class="keyword">for</span> chunk <span class="keyword">in</span> chunks:
    categoria = chain_clasificacion.invoke({<span class="string"><span class="delimiter">&quot;</span><span class="content">texto</span><span class="delimiter">&quot;</span></span>: chunk.page_content})
    print(categoria)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_ejemplo_4_extracción_de_metadatos">6.4.6. Ejemplo 4: Extracción de metadatos</h4>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">prompt_metadatos = PromptTemplate.from_template(
    <span class="string"><span class="delimiter">&quot;</span><span class="content">Identifica el título, autor y fecha de creación del siguiente documento: {texto}</span><span class="delimiter">&quot;</span></span>
)
chain_metadatos = LLMChain(llm=llm, prompt=prompt_metadatos)

<span class="keyword">for</span> chunk <span class="keyword">in</span> chunks:
    metadatos = chain_metadatos.invoke({<span class="string"><span class="delimiter">&quot;</span><span class="content">texto</span><span class="delimiter">&quot;</span></span>: chunk.page_content})
    print(metadatos)</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Ventajas y aplicaciones</div>
<ul>
<li>
<p><strong>Automatización</strong>: Reduce el tiempo y los errores del procesamiento manual de documentos extensos[7][8].</p>
</li>
<li>
<p><strong>Eficiencia</strong>: Permite analizar grandes volúmenes de información y extraer datos clave de forma rápida y precisa[2][7].</p>
</li>
<li>
<p><strong>Privacidad y control</strong>: Al trabajar con Ollama en local, los datos sensibles no salen del entorno seguro[9].</p>
</li>
<li>
<p><strong>Casos de uso</strong>: Gestión de contratos, análisis de informes financieros, revisión de artículos de investigación, extracción de datos legales, generación de resúmenes ejecutivos y más[7][8][9].</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>La extracción de información relevante con modelos Ollama y LangChain es una herramienta poderosa para transformar documentos no estructurados en conocimiento estructurado, facilitando la automatización y la toma de decisiones basada en datos.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_7_embeddings_y_bases_de_datos_vectoriales">7. 7. Embeddings y Bases de Datos Vectoriales</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_7_1_qué_son_los_embeddings_y_para_qué_se_usan">7.1. 7.1 ¿Qué son los embeddings y para qué se usan?</h3>
<div class="paragraph">
<p>Los <strong>embeddings</strong> son representaciones vectoriales numéricas que capturan el significado semántico de textos, imágenes u otros datos. En el procesamiento de lenguaje natural (NLP), los embeddings convierten palabras, frases o documentos en vectores de alta dimensión (por ejemplo, de 300 a 1000 dimensiones), de modo que la proximidad espacial entre estos vectores refleja la similitud semántica: conceptos relacionados, como "gato" y "felino", estarán cerca en el espacio vectorial.
<strong>Principales usos:</strong>
- Búsqueda semántica (encontrar documentos relevantes aunque no coincidan exactamente las palabras)
- Sistemas RAG (Retrieval-Augmented Generation)
- Recomendaciones y clustering de textos
- Clasificación y análisis temático</p>
</div>
</div>
<div class="sect2">
<h3 id="_7_2_creación_de_embeddings_con_llms">7.2. 7.2 Creación de embeddings con LLMs</h3>
<div class="paragraph">
<p>Con Ollama y LangChain puedes generar embeddings de manera local utilizando modelos optimizados para este fin, como <code>nomic-embed-text</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.embeddings</span> <span class="keyword">import</span> <span class="include">OllamaEmbeddings</span>

<span class="comment"># Crear el objeto de embeddings usando un modelo local</span>
embeddings = OllamaEmbeddings(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">nomic-embed-text</span><span class="delimiter">&quot;</span></span>)

texto = <span class="string"><span class="delimiter">&quot;</span><span class="content">LangChain facilita el desarrollo con IA</span><span class="delimiter">&quot;</span></span>
vector = embeddings.embed_query(texto)  <span class="comment"># Devuelve un vector de 3584 dimensiones</span></code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Flujo de procesamiento típico:</strong>
1. Tokenización del texto
2. Paso por las capas del modelo de embeddings
3. Extracción del vector de la última capa</p>
</div>
</div>
<div class="sect2">
<h3 id="_7_3_almacenamiento_en_bases_de_datos_vectoriales_pinecone_faiss_chroma">7.3. 7.3 Almacenamiento en bases de datos vectoriales (Pinecone, FAISS, Chroma)</h3>
<div class="paragraph">
<p>Para búsquedas rápidas y eficientes, los embeddings se almacenan en bases de datos vectoriales. Las opciones más comunes en entornos locales son Chroma y FAISS, aunque Pinecone es una alternativa cloud.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 33.3333%;">
<col style="width: 33.3335%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Base de Datos</th>
<th class="tableblock halign-left valign-top">Tipo</th>
<th class="tableblock halign-left valign-top">Ventajas</th>
<th class="tableblock halign-left valign-top">Ejemplo de uso LangChain</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Chroma</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Open Source</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Integración nativa, persistencia</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Chroma.from_documents()</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">FAISS</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Biblioteca</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Optimizada para CPU/GPU</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">FAISS.from_texts()</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pinecone</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cloud</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Escalabilidad empresarial</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Requiere API key</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><strong>Ejemplo con Chroma:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.vectorstores</span> <span class="keyword">import</span> <span class="include">Chroma</span>

vector_store = Chroma.from_documents(
    documents=documentos,
    embedding=OllamaEmbeddings(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">nomic-embed-text</span><span class="delimiter">&quot;</span></span>),
    persist_directory=<span class="string"><span class="delimiter">&quot;</span><span class="content">./chroma_db</span><span class="delimiter">&quot;</span></span>
)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_7_4_búsqueda_semántica_y_recuperación_de_información">7.4. 7.4 Búsqueda semántica y recuperación de información</h3>
<div class="paragraph">
<p>La búsqueda semántica consiste en comparar el embedding de una consulta con los embeddings almacenados, usando métricas como la similitud coseno o índices jerárquicos como HNSW para búsquedas rápidas.</p>
</div>
<div class="paragraph">
<p><strong>Ejemplo de búsqueda semántica:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Recuperar los 3 documentos más relevantes para la consulta</span>
docs = vector_store.similarity_search(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Qué es LangChain?</span><span class="delimiter">&quot;</span></span>, k=<span class="integer">3</span>)

<span class="comment"># Búsqueda con filtro de metadatos</span>
docs_filtrados = vector_store.max_marginal_relevance_search(
    query=<span class="string"><span class="delimiter">&quot;</span><span class="content">Aprendizaje automático</span><span class="delimiter">&quot;</span></span>,
    filter={<span class="string"><span class="delimiter">&quot;</span><span class="content">tema</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">IA</span><span class="delimiter">&quot;</span></span>},
    k=<span class="integer">5</span>
)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Flujo completo de RAG (Retrieval-Augmented Generation):</strong>
1. Generar embeddings de los documentos y almacenarlos en la base vectorial
2. Convertir la consulta del usuario en un embedding y buscar los chunks más relevantes
3. Alimentar esos chunks al LLM para generar una respuesta contextualizada</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">RetrievalQA</span>

qa_chain = RetrievalQA.from_chain_type(
    llm=OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>),
    retriever=vector_store.as_retriever()
)
respuesta = qa_chain.run(<span class="string"><span class="delimiter">&quot;</span><span class="content">Explica los embeddings en 50 palabras</span><span class="delimiter">&quot;</span></span>)
print(respuesta)</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_8_retrieval_augmented_generation_rag">8. 8. Retrieval Augmented Generation (RAG)</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_8_1_concepto_y_arquitectura_de_rag">8.1. 8.1 Concepto y arquitectura de RAG</h3>

</div>
<div class="sect2">
<h3 id="_8_2_implementación_de_rag_con_langchain">8.2. 8.2 Implementación de RAG con LangChain</h3>

</div>
<div class="sect2">
<h3 id="_8_3_integración_de_bases_de_datos_vectoriales_en_rag">8.3. 8.3 Integración de bases de datos vectoriales en RAG</h3>

</div>
<div class="sect2">
<h3 id="_8_4_ejemplos_prácticos_sistemas_de_preguntas_y_respuestas">8.4. 8.4 Ejemplos prácticos: sistemas de preguntas y respuestas</h3>

</div>
</div>
</div>
<div class="sect1">
<h2 id="_9_memoria_y_estado_en_aplicaciones_langchain">9. 9. Memoria y Estado en Aplicaciones LangChain</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_9_1_qué_es_la_memoria_en_langchain">9.1. 9.1 ¿Qué es la memoria en LangChain?</h3>

</div>
<div class="sect2">
<h3 id="_9_2_tipos_de_memoria_conversationbuffer_summary_entity_etc">9.2. 9.2 Tipos de memoria: ConversationBuffer, Summary, Entity, etc.</h3>

</div>
<div class="sect2">
<h3 id="_9_3_implementación_de_memoria_en_chatbots">9.3. 9.3 Implementación de memoria en chatbots</h3>

</div>
<div class="sect2">
<h3 id="_9_4_casos_de_uso_avanzados_de_memoria">9.4. 9.4 Casos de uso avanzados de memoria</h3>

</div>
</div>
</div>
<div class="sect1">
<h2 id="_10_agentes_en_langchain">10. 10. Agentes en LangChain</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_10_1_qué_es_un_agente_y_cómo_funciona">10.1. 10.1 ¿Qué es un agente y cómo funciona?</h3>

</div>
<div class="sect2">
<h3 id="_10_2_agentes_reactivos_y_planificadores">10.2. 10.2 Agentes reactivos y planificadores</h3>

</div>
<div class="sect2">
<h3 id="_10_3_herramientas_y_plugins_para_agentes">10.3. 10.3 Herramientas y plugins para agentes</h3>

</div>
<div class="sect2">
<h3 id="_10_4_agentes_para_búsqueda_web_análisis_sql_y_más">10.4. 10.4 Agentes para búsqueda web, análisis SQL, y más</h3>

</div>
<div class="sect2">
<h3 id="_10_5_creación_de_agentes_conversacionales_personalizados">10.5. 10.5 Creación de agentes conversacionales personalizados</h3>

</div>
</div>
</div>
<div class="sect1">
<h2 id="_11_integración_con_apis_y_herramientas_externas">11. 11. Integración con APIs y Herramientas Externas</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_11_1_conexión_con_apis_rest_y_servicios_externos">11.1. 11.1 Conexión con APIs REST y servicios externos</h3>

</div>
<div class="sect2">
<h3 id="_11_2_integración_con_google_aws_y_otras_plataformas">11.2. 11.2 Integración con Google, AWS, y otras plataformas</h3>

</div>
<div class="sect2">
<h3 id="_11_3_automatización_de_flujos_de_trabajo_con_agentes">11.3. 11.3 Automatización de flujos de trabajo con agentes</h3>

</div>
</div>
</div>
<div class="sect1">
<h2 id="_12_desarrollo_de_aplicaciones_conversacionales">12. 12. Desarrollo de Aplicaciones Conversacionales</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_12_1_construcción_de_chatbots_avanzados">12.1. 12.1 Construcción de chatbots avanzados</h3>

</div>
<div class="sect2">
<h3 id="_12_2_manejo_de_contexto_y_multihilo">12.2. 12.2 Manejo de contexto y multihilo</h3>

</div>
<div class="sect2">
<h3 id="_12_3_integración_de_memoria_y_rag_en_chatbots">12.3. 12.3 Integración de memoria y RAG en chatbots</h3>

</div>
<div class="sect2">
<h3 id="_12_4_ejemplos_de_asistentes_virtuales_y_casos_reales">12.4. 12.4 Ejemplos de asistentes virtuales y casos reales</h3>

</div>
</div>
</div>
<div class="sect1">
<h2 id="_13_despliegue_y_producción">13. 13. Despliegue y Producción</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_13_1_opciones_de_despliegue_local_cloud_serverless">13.1. 13.1 Opciones de despliegue (local, cloud, serverless)</h3>

</div>
<div class="sect2">
<h3 id="_13_2_integración_con_frameworks_web_fastapi_gradio_streamlit">13.2. 13.2 Integración con frameworks web (FastAPI, Gradio, Streamlit)</h3>

</div>
<div class="sect2">
<h3 id="_13_3_seguridad_autenticación_y_control_de_acceso">13.3. 13.3 Seguridad, autenticación y control de acceso</h3>

</div>
<div class="sect2">
<h3 id="_13_4_monitorización_y_logging_de_aplicaciones">13.4. 13.4 Monitorización y logging de aplicaciones</h3>

</div>
</div>
</div>
<div class="sect1">
<h2 id="_15_recursos_y_siguientes_pasos">14. 15. Recursos y Siguientes Pasos</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_15_1_documentación_oficial_y_comunidad">14.1. 15.1 Documentación oficial y comunidad</h3>

</div>
<div class="sect2">
<h3 id="_15_2_repositorios_y_ejemplos_recomendados">14.2. 15.2 Repositorios y ejemplos recomendados</h3>

</div>
<div class="sect2">
<h3 id="_15_3_roadmap_avanzado_integración_con_agentes_autónomos_y_nuevas_tendencias">14.3. 15.3 Roadmap avanzado: integración con agentes autónomos y nuevas tendencias</h3>

</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2025-06-05 17:46:11 +0200
</div>
</div>
</body>
</html>