<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.23">
<title>LlamaIndex</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<style>
/*! Stylesheet for CodeRay to loosely match GitHub themes | MIT License */
pre.CodeRay{background:#f7f7f8}
.CodeRay .line-numbers{border-right:1px solid;opacity:.35;padding:0 .5em 0 0;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
.CodeRay span.line-numbers{display:inline-block;margin-right:.75em}
.CodeRay .line-numbers strong{color:#000}
table.CodeRay{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.CodeRay td{vertical-align:top;line-height:inherit}
table.CodeRay td.line-numbers{text-align:right}
table.CodeRay td.code{padding:0 0 0 .75em}
.CodeRay .debug{color:#fff!important;background:navy!important}
.CodeRay .annotation{color:#007}
.CodeRay .attribute-name{color:navy}
.CodeRay .attribute-value{color:#700}
.CodeRay .binary{color:#509}
.CodeRay .comment{color:#998;font-style:italic}
.CodeRay .char{color:#04d}
.CodeRay .char .content{color:#04d}
.CodeRay .char .delimiter{color:#039}
.CodeRay .class{color:#458;font-weight:bold}
.CodeRay .complex{color:#a08}
.CodeRay .constant,.CodeRay .predefined-constant{color:teal}
.CodeRay .color{color:#099}
.CodeRay .class-variable{color:#369}
.CodeRay .decorator{color:#b0b}
.CodeRay .definition{color:#099}
.CodeRay .delimiter{color:#000}
.CodeRay .doc{color:#970}
.CodeRay .doctype{color:#34b}
.CodeRay .doc-string{color:#d42}
.CodeRay .escape{color:#666}
.CodeRay .entity{color:#800}
.CodeRay .error{color:#808}
.CodeRay .exception{color:inherit}
.CodeRay .filename{color:#099}
.CodeRay .function{color:#900;font-weight:bold}
.CodeRay .global-variable{color:teal}
.CodeRay .hex{color:#058}
.CodeRay .integer,.CodeRay .float{color:#099}
.CodeRay .include{color:#555}
.CodeRay .inline{color:#000}
.CodeRay .inline .inline{background:#ccc}
.CodeRay .inline .inline .inline{background:#bbb}
.CodeRay .inline .inline-delimiter{color:#d14}
.CodeRay .inline-delimiter{color:#d14}
.CodeRay .important{color:#555;font-weight:bold}
.CodeRay .interpreted{color:#b2b}
.CodeRay .instance-variable{color:teal}
.CodeRay .label{color:#970}
.CodeRay .local-variable{color:#963}
.CodeRay .octal{color:#40e}
.CodeRay .predefined{color:#369}
.CodeRay .preprocessor{color:#579}
.CodeRay .pseudo-class{color:#555}
.CodeRay .directive{font-weight:bold}
.CodeRay .type{font-weight:bold}
.CodeRay .predefined-type{color:inherit}
.CodeRay .reserved,.CodeRay .keyword{color:#000;font-weight:bold}
.CodeRay .key{color:#808}
.CodeRay .key .delimiter{color:#606}
.CodeRay .key .char{color:#80f}
.CodeRay .value{color:#088}
.CodeRay .regexp .delimiter{color:#808}
.CodeRay .regexp .content{color:#808}
.CodeRay .regexp .modifier{color:#808}
.CodeRay .regexp .char{color:#d14}
.CodeRay .regexp .function{color:#404;font-weight:bold}
.CodeRay .string{color:#d20}
.CodeRay .string .string .string{background:#ffd0d0}
.CodeRay .string .content{color:#d14}
.CodeRay .string .char{color:#d14}
.CodeRay .string .delimiter{color:#d14}
.CodeRay .shell{color:#d14}
.CodeRay .shell .delimiter{color:#d14}
.CodeRay .symbol{color:#990073}
.CodeRay .symbol .content{color:#a60}
.CodeRay .symbol .delimiter{color:#630}
.CodeRay .tag{color:teal}
.CodeRay .tag-special{color:#d70}
.CodeRay .variable{color:#036}
.CodeRay .insert{background:#afa}
.CodeRay .delete{background:#faa}
.CodeRay .change{color:#aaf;background:#007}
.CodeRay .head{color:#f8f;background:#505}
.CodeRay .insert .insert{color:#080}
.CodeRay .delete .delete{color:#800}
.CodeRay .change .change{color:#66f}
.CodeRay .head .head{color:#f4f}
</style>
</head>
<body class="article">
<div id="header">
<h1>LlamaIndex</h1>
<div id="toc" class="toc">
<div id="toctitle">Índice de contenidos</div>
<ul class="sectlevel1">
<li><a href="#_introducción_a_llamaindex">1. Introducción a LlamaIndex</a>
<ul class="sectlevel2">
<li><a href="#_qué_es_llamaindex_y_para_qué_sirve">1.1. ¿Qué es LlamaIndex y para qué sirve?</a></li>
<li><a href="#_elementos_clave_de_llamaindex">1.2. Elementos clave de LlamaIndex</a>
<ul class="sectlevel3">
<li><a href="#_agentes">1.2.1. Agentes</a></li>
<li><a href="#_flujos_de_trabajo">1.2.2. Flujos de trabajo</a></li>
<li><a href="#_extracción_de_datos_estructurados">1.2.3. Extracción de datos estructurados</a></li>
<li><a href="#_queryengines">1.2.4. QueryEngines</a></li>
<li><a href="#_chatengines">1.2.5. ChatEngines</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_instalación_y_configuración">2. Instalación y Configuración</a>
<ul class="sectlevel2">
<li><a href="#_requisitos_previos_python_apis_conocimientos_básicos_de_ia">2.1. Requisitos previos (Python, APIs, conocimientos básicos de IA)</a></li>
<li><a href="#_instalación_de_llamaindex_y_dependencias">2.2. Instalación de LlamaIndex y dependencias</a></li>
<li><a href="#_configuración_del_entorno_de_desarrollo">2.3. Configuración del entorno de desarrollo</a></li>
</ul>
</li>
<li><a href="#_fundamentos_de_indexación_de_datos">3. Fundamentos de Indexación de Datos</a>
<ul class="sectlevel2">
<li><a href="#_conceptos_básicos_de_indexación_y_recuperación">3.1. Conceptos básicos de indexación y recuperación</a></li>
<li><a href="#_tipos_de_datos_soportados">3.2. Tipos de datos soportados</a></li>
<li><a href="#_uso_de_cargadores_y_conectores_de_datos_llamahub">3.3. Uso de cargadores y conectores de datos (LlamaHub)</a></li>
</ul>
</li>
<li><a href="#_fases_de_indexación_y_carga_de_datos">4. Fases de Indexación y Carga de Datos</a>
<ul class="sectlevel2">
<li><a href="#_fase_de_ingesta_y_carga_de_datos">4.1. Fase de ingesta y carga de datos</a></li>
<li><a href="#_fase_de_indexación_creación_de_índices_vectoriales_y_otras_estructuras">4.2. Fase de indexación: creación de índices vectoriales y otras estructuras</a>
<ul class="sectlevel3">
<li><a href="#_vectorstoreindex">4.2.1. VectorStoreIndex</a></li>
<li><a href="#_summaryindex">4.2.2. SummaryIndex</a></li>
<li><a href="#_indexnode">4.2.3. IndexNode</a></li>
<li><a href="#_almacenamiento_y_reutilización_storagecontext_y_persistencia">4.2.4. Almacenamiento y reutilización: StorageContext y persistencia</a></li>
</ul>
</li>
<li><a href="#_fase_de_consulta_recuperación_y_generación_aumentada_por_recuperación_rag">4.3. Fase de consulta: recuperación y generación aumentada por recuperación (RAG)</a>
<ul class="sectlevel3">
<li><a href="#_recuperación_de_información">4.3.1. Recuperación de Información</a></li>
<li><a href="#_posprocesamiento">4.3.2. Posprocesamiento</a></li>
<li><a href="#_síntesis_de_respuesta_rag">4.3.3. Síntesis de Respuesta (RAG)</a></li>
<li><a href="#_ejemplo_de_consulta_rag">4.3.4. Ejemplo de consulta RAG</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_componentes_clave_de_llamaindex">5. Componentes Clave de LlamaIndex</a>
<ul class="sectlevel2">
<li><a href="#_componentes_prompts_modelos_bases_de_datos_motores_de_consulta_queryengine">5.1. Componentes: prompts, modelos, bases de datos, motores de consulta (QueryEngine)</a></li>
<li><a href="#_herramientas_y_agentes_definición_usos_y_ejemplos">5.2. Herramientas y agentes: definición, usos y ejemplos</a></li>
<li><a href="#_workflows_en_llamaindex">5.3. Workflows en LlamaIndex</a>
<ul class="sectlevel3">
<li><a href="#_uso_de_context_para_compartir_datos_entre_steps">5.3.1. Uso de Context para compartir datos entre steps</a></li>
<li><a href="#_visualización_del_flujo">5.3.2. Visualización del flujo</a></li>
<li><a href="#_casos_de_uso_comunes">5.3.3. Casos de uso comunes</a></li>
<li><a href="#_ventajas_principales">5.3.4. Ventajas principales</a></li>
</ul>
</li>
<li><a href="#_ejemplo_práctico_desarrollo_de_un_asistente_de_documentación">5.4. Ejemplo práctico: desarrollo de un asistente de documentación</a></li>
</ul>
</li>
<li><a href="#_funcionalidades_avanzadas">6. Funcionalidades Avanzadas</a>
<ul class="sectlevel2">
<li><a href="#_uso_de_almacenes_vectoriales_para_búsquedas_semánticas_eficientes">6.1. Uso de almacenes vectoriales para búsquedas semánticas eficientes</a></li>
<li><a href="#_implementación_de_agentes_inteligentes_react_openai_function_agents">6.2. Implementación de agentes inteligentes (ReAct, OpenAI Function Agents)</a></li>
<li><a href="#_personalización_y_extensión_con_plugins_y_herramientas_externas">6.3. Personalización y extensión con plugins y herramientas externas</a></li>
</ul>
</li>
<li><a href="#_persistencia_y_gestión_de_datos">7. Persistencia y Gestión de Datos</a>
<ul class="sectlevel2">
<li><a href="#_métodos_de_almacenamiento_y_recuperación_de_índices">7.1. Métodos de almacenamiento y recuperación de índices</a></li>
<li><a href="#_persistencia_de_datos_indexados_y_recarga_de_contextos">7.2. Persistencia de datos indexados y recarga de contextos</a></li>
</ul>
</li>
<li><a href="#_ingeniería_de_prompts_y_estrategias_de_consulta">8. Ingeniería de Prompts y Estrategias de Consulta</a>
<ul class="sectlevel2">
<li><a href="#_técnicas_cadena_de_pensamiento_few_shot_prompting_react">8.1. Técnicas: cadena de pensamiento, few-shot prompting, ReAct</a></li>
<li><a href="#_mejores_prácticas_para_optimizar_la_interacción_con_el_llm">8.2. Mejores prácticas para optimizar la interacción con el LLM</a></li>
</ul>
</li>
<li><a href="#_implementación_escalado_y_optimización">9. Implementación, Escalado y Optimización</a>
<ul class="sectlevel2">
<li><a href="#_despliegue_de_aplicaciones_basadas_en_llamaindex">9.1. Despliegue de aplicaciones basadas en LlamaIndex</a></li>
<li><a href="#_escalado_para_alto_rendimiento_y_grandes_volúmenes_de_datos">9.2. Escalado para alto rendimiento y grandes volúmenes de datos</a></li>
<li><a href="#_monitorización_y_optimización_de_aplicaciones">9.3. Monitorización y optimización de aplicaciones</a></li>
</ul>
</li>
<li><a href="#_futuro_de_llamaindex_y_próximos_pasos">10. Futuro de LlamaIndex y Próximos Pasos</a>
<ul class="sectlevel2">
<li><a href="#_tendencias_en_orquestación_de_datos_y_llms">10.1. Tendencias en orquestación de datos y LLMs</a></li>
<li><a href="#_actualizaciones_y_roadmap_de_llamaindex">10.2. Actualizaciones y roadmap de LlamaIndex</a></li>
<li><a href="#_recursos_adicionales_y_comunidad">10.3. Recursos adicionales y comunidad</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="_introducción_a_llamaindex">1. Introducción a LlamaIndex</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_qué_es_llamaindex_y_para_qué_sirve">1.1. ¿Qué es LlamaIndex y para qué sirve?</h3>
<div class="paragraph">
<p>LlamaIndex es un framework flexible y robusto diseñado para facilitar la construcción de aplicaciones basadas en modelos de lenguaje grande (LLMs) conectados a tus propios datos empresariales o privados. Su objetivo principal es servir de puente entre los LLMs y la información relevante, permitiendo que los modelos accedan, comprendan y utilicen datos que no estaban presentes en su entrenamiento original.</p>
</div>
<div class="ulist">
<div class="title">LlamaIndex proporciona:</div>
<ul>
<li>
<p>Conectores para ingestar datos desde múltiples fuentes (APIs, PDFs, SQL, documentos, etc.)</p>
</li>
<li>
<p>Herramientas para indexar y estructurar datos de forma eficiente, facilitando búsquedas contextuales y semánticas</p>
</li>
<li>
<p>Interfaces para construir asistentes de conocimiento, chatbots, agentes autónomos y sistemas de análisis de documentos</p>
</li>
<li>
<p>Capacidades para parsear documentos complejos (tablas, imágenes, estructuras jerárquicas) y convertirlos en formatos comprensibles para los LLMs</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Ejemplo de flujo básico en Python:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.embeddings.ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbedding</span>
<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">VectorStoreIndex</span>, <span class="include">SimpleDirectoryReader</span>, <span class="include">Settings</span>

<span class="comment"># Configurar modelos locales de Ollama</span>
Settings.llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Modelo de lenguaje</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>,
    temperature=<span class="float">0.1</span>  <span class="comment"># Más determinista para búsquedas</span>
)

Settings.embed_model = OllamaEmbedding(
    model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">nomic-embed-text</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Modelo de embeddings</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># Cargar documentos desde directorio</span>
documents = SimpleDirectoryReader(<span class="string"><span class="delimiter">&quot;</span><span class="content">docs/</span><span class="delimiter">&quot;</span></span>).load_data()

<span class="comment"># Crear índice con embeddings locales</span>
index = VectorStoreIndex.from_documents(
    documents,
    embed_model=Settings.embed_model  <span class="comment"># Usar embeddings de Ollama</span>
)

<span class="comment"># Configurar motor de consultas con modelo local</span>
query_engine = index.as_query_engine(
    llm=Settings.llm,
    similarity_top_k=<span class="integer">3</span>  <span class="comment"># Número de resultados a considerar</span>
)

<span class="comment"># Realizar consulta usando el modelo local</span>
respuesta = query_engine.query(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿qué es AlphaStar?</span><span class="delimiter">&quot;</span></span>)
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta basada en documentos:</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, respuesta.response)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_elementos_clave_de_llamaindex">1.2. Elementos clave de LlamaIndex</h3>
<div class="sect3">
<h4 id="_agentes">1.2.1. Agentes</h4>
<div class="paragraph">
<p>Un <strong>agente</strong> es un sistema automatizado de toma de decisiones impulsado por un LLM que interactúa con el mundo a través de un conjunto de herramientas. Los agentes pueden realizar una cantidad arbitraria de pasos para completar una tarea determinada, decidiendo dinámicamente el mejor curso de acción en lugar de seguir pasos predefinidos. Esto les da una flexibilidad adicional para abordar tareas más complejas.</p>
</div>
</div>
<div class="sect3">
<h4 id="_flujos_de_trabajo">1.2.2. Flujos de trabajo</h4>
<div class="paragraph">
<p>Un <strong>workflow</strong> en LlamaIndex es una abstracción específica basada en eventos que permite orquestar una secuencia de pasos y llamadas a LLMs. Los flujos de trabajo pueden utilizarse para implementar cualquier aplicación basada en agentes y son un componente central de LlamaIndex.</p>
</div>
</div>
<div class="sect3">
<h4 id="_extracción_de_datos_estructurados">1.2.3. Extracción de datos estructurados</h4>
<div class="paragraph">
<p>Los <strong>extractores Pydantic</strong> permiten especificar una estructura de datos precisa que se desea extraer y utilizar LLMs para completar las piezas faltantes de manera segura en cuanto a tipos. Esto es útil para extraer datos estructurados de fuentes no estructuradas como archivos PDF, sitios web y más, y es clave para automatizar flujos de trabajo.</p>
</div>
</div>
<div class="sect3">
<h4 id="_queryengines">1.2.4. QueryEngines</h4>
<div class="listingblock">
<div class="content">
<pre>graph TD
  A[Inicio] --&gt; B[Consulta]
  B --&gt; C{¿Respuesta útil?}
  C -- Sí --&gt; D[Mostrar respuesta]
  C -- No --&gt; E[Reformular consulta]
  E --&gt; B
  D --&gt; F[Fin]</pre>
</div>
</div>
<div class="paragraph">
<p>Un <strong>QueryEngine</strong> es un flujo completo que permite realizar preguntas sobre tus datos. Recibe una consulta en lenguaje natural y devuelve una respuesta junto con el contexto de referencia recuperado y pasado al LLM.</p>
</div>
<div class="listingblock">
<div class="title">Ejemplo de QueryEngine:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">VectorStoreIndex</span>, <span class="include">SimpleDirectoryReader</span>, <span class="include">Settings</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.embeddings.huggingface</span> <span class="keyword">import</span> <span class="include">HuggingFaceEmbedding</span>

<span class="comment"># Configura el modelo de embedding y el LLM de Ollama</span>
Settings.embed_model = HuggingFaceEmbedding(model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">BAAI/bge-base-en-v1.5</span><span class="delimiter">&quot;</span></span>)
Settings.llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3</span><span class="delimiter">&quot;</span></span>,
    request_timeout=<span class="float">120.0</span>,  <span class="comment"># Aumenta el timeout si tu modelo es grande</span>
    context_window=<span class="integer">8000</span>     <span class="comment"># Ajusta según la RAM disponible</span>
)

<span class="comment"># Carga documentos desde un directorio local (por ejemplo, ./data)</span>
documents = SimpleDirectoryReader(<span class="string"><span class="delimiter">&quot;</span><span class="content">data</span><span class="delimiter">&quot;</span></span>).load_data()

<span class="comment"># Crea el índice vectorial</span>
index = VectorStoreIndex.from_documents(documents)

<span class="comment"># Crea el query engine</span>
query_engine = index.as_query_engine()

<span class="comment"># Realiza una consulta</span>
response = query_engine.query(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿De qué trata el documento principal?</span><span class="delimiter">&quot;</span></span>)
print(response)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_chatengines">1.2.5. ChatEngines</h4>
<div class="paragraph">
<p>Un <strong>ChatEngine</strong> es un flujo completo para mantener un diálogo con tus datos (interacciones múltiples en lugar de una sola pregunta y respuesta).</p>
</div>
<div class="listingblock">
<div class="title">Ejemplo de ChatEngine con Ollama:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.embeddings.ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbedding</span>
<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">VectorStoreIndex</span>, <span class="include">Document</span>, <span class="include">Settings</span>

<span class="comment"># Configurar modelos locales de Ollama</span>
Settings.llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Modelo de lenguaje local</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>,
    temperature=<span class="float">0.3</span>
)

Settings.embed_model = OllamaEmbedding(
    model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">nomic-embed-text</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Modelo para embeddings</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># Documentos de ejemplo sobre errores 404</span>
documents = [
    Document(
        text=<span class="string"><span class="delimiter">&quot;</span><span class="content">Un error 404 significa que la página que buscas no existe en el servidor. </span><span class="delimiter">&quot;</span></span>
             <span class="string"><span class="delimiter">&quot;</span><span class="content">Esto puede deberse a que la URL está mal escrita o la página fue eliminada. </span><span class="delimiter">&quot;</span></span>
             <span class="string"><span class="delimiter">&quot;</span><span class="content">Para solucionarlo, revisa la dirección web o vuelve a la página principal.</span><span class="delimiter">&quot;</span></span>,
        metadata={<span class="string"><span class="delimiter">&quot;</span><span class="content">tipo_error</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">404</span><span class="delimiter">&quot;</span></span>}
    ),
    Document(
        text=<span class="string"><span class="delimiter">&quot;</span><span class="content">Si recibes un error 404, intenta actualizar la página, limpiar la caché del navegador </span><span class="delimiter">&quot;</span></span>
             <span class="string"><span class="delimiter">&quot;</span><span class="content">o buscar la página desde el menú principal del sitio web.</span><span class="delimiter">&quot;</span></span>,
        metadata={<span class="string"><span class="delimiter">&quot;</span><span class="content">tipo_error</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">404</span><span class="delimiter">&quot;</span></span>}
    ),
    Document(
        text=<span class="string"><span class="delimiter">&quot;</span><span class="content">Los errores 404 son comunes cuando se cambia la estructura de un sitio web. </span><span class="delimiter">&quot;</span></span>
             <span class="string"><span class="delimiter">&quot;</span><span class="content">Si eres el administrador, revisa los enlaces rotos y redirige correctamente.</span><span class="delimiter">&quot;</span></span>,
        metadata={<span class="string"><span class="delimiter">&quot;</span><span class="content">tipo_error</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">404</span><span class="delimiter">&quot;</span></span>}
    )
]

<span class="comment"># Crear índice con configuración local</span>
index = VectorStoreIndex.from_documents(
    documents,
    embed_model=Settings.embed_model
)

<span class="comment"># Configurar motor de chat con el modelo local</span>
chat_engine = index.as_chat_engine(
    llm=Settings.llm,
    verbose=<span class="predefined-constant">True</span>  <span class="comment"># Opcional: ver el proceso de razonamiento</span>
)

<span class="comment"># Consulta usando el modelo local</span>
respuesta = chat_engine.chat(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Qué debo hacer si recibo un error 404?</span><span class="delimiter">&quot;</span></span>)
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta del asistente:</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, respuesta)</code></pre>
</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. ChatEngine vs QueryEngine:</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 40%;">
<col style="width: 40%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Característica</th>
<th class="tableblock halign-left valign-top">QueryEngine</th>
<th class="tableblock halign-left valign-top">ChatEngine</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Propósito principal</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Responder a consultas individuales sobre tus datos.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Mantener una conversación (varias preguntas y respuestas) con memoria de contexto.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Memoria de conversación</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No guarda historial de preguntas y respuestas previas. Cada consulta es independiente.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Guarda y utiliza el historial de la conversación para dar respuestas más contextuales.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Interfaz de uso</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">.query("Pregunta")</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">.chat("Mensaje")</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Casos de uso</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Búsquedas puntuales, preguntas únicas, extracción de información específica.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Chatbots, asistentes conversacionales, flujos de diálogo continuos sobre los datos.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Soporte de contexto</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Solo usa el contexto de la pregunta actual y los datos indexados.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Usa el contexto de la conversación previa y los datos indexados.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ejemplo de uso</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">response = query_engine.query("¿Quién es Paul Graham?")</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">response = chat_engine.chat("Hola, ¿quién es Paul Graham?")</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Composición avanzada</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Puede combinar varios índices o motores para consultas complejas.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Puede combinar memoria, herramientas y lógica de flujo conversacional.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_instalación_y_configuración">2. Instalación y Configuración</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_requisitos_previos_python_apis_conocimientos_básicos_de_ia">2.1. Requisitos previos (Python, APIs, conocimientos básicos de IA)</h3>
<div class="ulist">
<div class="title">Para comenzar a trabajar con LlamaIndex, asegúrate de cumplir con los siguientes requisitos previos:</div>
<ul>
<li>
<p>Python 3.8 o superior instalado en tu sistema.</p>
</li>
<li>
<p>pip actualizado para instalar paquetes de Python.</p>
</li>
<li>
<p>Conocimientos básicos de programación en Python y conceptos generales de IA.</p>
</li>
<li>
<p>Tener instalado y funcionando Ollama en tu máquina local (Ollama sirve modelos LLM en <code>localhost:11434</code>).</p>
</li>
<li>
<p>Suficiente memoria RAM para el modelo que vayas a usar (por ejemplo, Llama 3 8B requiere al menos ~32GB de RAM).</p>
</li>
<li>
<p>Acceso a los datos que quieras indexar (archivos, directorios, bases de datos, etc.).</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_instalación_de_llamaindex_y_dependencias">2.2. Instalación de LlamaIndex y dependencias</h3>
<div class="listingblock">
<div class="title">Instala los paquetes necesarios para trabajar con Ollama y LlamaIndex:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">pip install llama-index-llms-ollama llama-index-embeddings-ollama</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Descarga el modelo Llama 3 para Ollama (ejemplo con Llama 3.2):</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">ollama pull llama3.2
ollama pull nomic-embed-text</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Opcional: instala otros conectores de LlamaIndex según tus fuentes de datos:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">pip install llama-index-readers-file llama-index-readers-pdf llama-index-readers-web</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_configuración_del_entorno_de_desarrollo">2.3. Configuración del entorno de desarrollo</h3>
<div class="listingblock">
<div class="title">Configura LlamaIndex para usar Ollama como LLM y HuggingFace como modelo de embeddings:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.embeddings.ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbedding</span>
<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">Settings</span>, <span class="include">VectorStoreIndex</span>, <span class="include">SimpleDirectoryReader</span>

<span class="comment"># Configuración completa con modelos locales</span>
Settings.embed_model = OllamaEmbedding(
    model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">nomic-embed-text</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Modelo de embeddings alternativo</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>,
    request_timeout=<span class="float">120.0</span>
)

Settings.llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,              <span class="comment"># Nombre exacto del modelo en Ollama</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>,
    request_timeout=<span class="float">300.0</span>,         <span class="comment"># Tiempo ampliado para modelos grandes</span>
    context_window=<span class="integer">8192</span>,           <span class="comment"># Ventana de contexto aumentada</span>
    temperature=<span class="float">0.3</span>                <span class="comment"># Control de creatividad</span>
)

<span class="comment"># Carga de documentos y creación del índice</span>
documents = SimpleDirectoryReader(<span class="string"><span class="delimiter">&quot;</span><span class="content">docs/</span><span class="delimiter">&quot;</span></span>).load_data()
index = VectorStoreIndex.from_documents(
    documents,
    embed_model=Settings.embed_model  <span class="comment"># Usar embeddings locales</span>
)

<span class="comment"># Configuración del motor de consultas</span>
query_engine = index.as_query_engine(
    llm=Settings.llm,
    similarity_top_k=<span class="integer">3</span>,            <span class="comment"># Considerar 3 fragmentos relevantes</span>
    verbose=<span class="predefined-constant">True</span>                    <span class="comment"># Opcional: ver proceso de razonamiento</span>
)

<span class="comment"># Ejecución de la consulta</span>
respuesta = query_engine.query(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Qué temas trata el manual de usuario?</span><span class="delimiter">&quot;</span></span>)
print(respuesta.response)</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_fundamentos_de_indexación_de_datos">3. Fundamentos de Indexación de Datos</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_conceptos_básicos_de_indexación_y_recuperación">3.1. Conceptos básicos de indexación y recuperación</h3>
<div class="ulist">
<div class="title">La indexación en LlamaIndex transforma datos brutos en <strong>representaciones matemáticas optimizadas</strong> mediante estos procesos claved:</div>
<ul>
<li>
<p><strong>Vectorización</strong>: Conversión de texto a vectores numéricos usando modelos como <code>BAAI/bge-small-es-v1.5</code> para búsquedas semánticasd</p>
</li>
<li>
<p><strong>Organización jerárquica</strong>: Estructuración de datos en árboles binarios o grafos para navegación eficiente</p>
</li>
<li>
<p><strong>Metadatos contextuales</strong>: Asociación de información adicional (fuente, fecha) para filtrado avanzado</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">La recuperación combina:</div>
<ul>
<li>
<p>Algoritmos <strong>k-NN</strong> para similitud vectorial</p>
</li>
<li>
<p>Filtros basados en metadatos</p>
</li>
<li>
<p><strong>Recuperación recursiva</strong> para búsquedas en múltiples niveles de contexto</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Ejemplo de pipeline de indexación:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">Document</span>
<span class="keyword">from</span> <span class="include">llama_index.core.node_parser</span> <span class="keyword">import</span> <span class="include">SentenceSplitter</span>
<span class="keyword">from</span> <span class="include">llama_index.core.extractors</span> <span class="keyword">import</span> <span class="include">TitleExtractor</span>
<span class="keyword">from</span> <span class="include">llama_index.core.ingestion</span> <span class="keyword">import</span> <span class="include">IngestionPipeline</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.embeddings.ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbedding</span>

<span class="comment"># Configura el modelo Ollama para LLM y embeddings</span>
ollama_llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Cambia por el modelo que tengas en Ollama</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>,
    temperature=<span class="float">0.3</span>
)
embed_model = OllamaEmbedding(
    model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">nomic-embed-text</span><span class="delimiter">&quot;</span></span>,
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># Prepara tus documentos (puedes cargar desde archivos, aquí un ejemplo simple)</span>
documents = [
    Document(text=<span class="string"><span class="delimiter">&quot;</span><span class="content">Este es el manual de usuario. Explica las políticas de devolución y garantías.</span><span class="delimiter">&quot;</span></span>),
    Document(text=<span class="string"><span class="delimiter">&quot;</span><span class="content">Para devolver un producto, contacte con soporte y siga las instrucciones del sitio web.</span><span class="delimiter">&quot;</span></span>)
]

<span class="comment"># Crea el extractor de títulos usando Ollama como LLM</span>
title_extractor = TitleExtractor(llm=ollama_llm)

<span class="comment"># Define la pipeline de ingesta</span>
pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=<span class="integer">64</span>, chunk_overlap=<span class="integer">0</span>),
        title_extractor,
        embed_model
    ]
)

<span class="comment"># Ejecuta la pipeline sobre los documentos</span>
nodes = pipeline.run(documents=documents)

<span class="comment"># Visualiza los nodos resultantes</span>
<span class="keyword">for</span> node <span class="keyword">in</span> nodes:
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">--- Nodo ---</span><span class="delimiter">&quot;</span></span>)
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Texto:</span><span class="delimiter">&quot;</span></span>, node.text)
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Título:</span><span class="delimiter">&quot;</span></span>, node.metadata.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">document_title</span><span class="delimiter">&quot;</span></span>))
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Embeddings (primeros valores):</span><span class="delimiter">&quot;</span></span>, node.embedding[:<span class="integer">5</span>], <span class="string"><span class="delimiter">&quot;</span><span class="content">...</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_tipos_de_datos_soportados">3.2. Tipos de datos soportados</h3>
<div class="paragraph">
<p>LlamaIndex procesa 160+ formatos mediante:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 14.2857%;">
<col style="width: 28.5714%;">
<col style="width: 28.5714%;">
<col style="width: 28.5715%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Tipo</th>
<th class="tableblock halign-left valign-top">Ejemplos</th>
<th class="tableblock halign-left valign-top">Cargador</th>
<th class="tableblock halign-left valign-top">Caso de uso</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Estructurados</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">SQL, CSV, Excel</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>SQLAlchemyReader</code>, <code>PandasReader</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Análisis tabular</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Semiestructurados</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">JSON, XML, emails</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>JSONReader</code>, <code>BeautifulSoupWebReader</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Integración APIs</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>No estructurados</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">PDF, imágenes, audio</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>LlamaParse</code>, <code>SimpleDirectoryReader</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Documentos complejos</p></td>
</tr>
</tbody>
</table>
<div class="listingblock">
<div class="title">Ejemplo con PDF usando Ollama:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.readers.file</span> <span class="keyword">import</span> <span class="include">PDFReader</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>

<span class="comment"># 1. Cargar el documento PDF</span>
pdf_path = <span class="string"><span class="delimiter">&quot;</span><span class="content">docs/sample.pdf</span><span class="delimiter">&quot;</span></span>
reader = PDFReader()
documents = reader.load_data(file=pdf_path)

<span class="comment"># 2. Configurar el modelo Ollama para resumen</span>
llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Cambia por el modelo que tengas descargado en Ollama</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>,
    temperature=<span class="float">0.2</span>,
    request_timeout=<span class="float">120.0</span>
)

<span class="comment"># 3. Crear el prompt de resumen</span>
texto = documents[<span class="integer">0</span>].text[:<span class="integer">6000</span>]  <span class="comment"># Limita el texto si el PDF es muy largo</span>
prompt = (
    <span class="string"><span class="delimiter">&quot;</span><span class="content">Resume el siguiente texto en español, resaltando los puntos más importantes:</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
    f<span class="string"><span class="delimiter">&quot;</span><span class="content">{texto}</span><span class="char">\n</span><span class="char">\n</span><span class="content">Resumen:</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 4. Generar el resumen</span>
resumen = llm.complete(prompt)
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Resumen del PDF:</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)
print(resumen)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_uso_de_cargadores_y_conectores_de_datos_llamahub">3.3. Uso de cargadores y conectores de datos (LlamaHub)</h3>
<div class="paragraph">
<p>LlamaHub ofrece 160+ conectores para:</p>
</div>
<div class="olist arabic">
<div class="title"><strong>workflow típico:</strong></div>
<ol class="arabic">
<li>
<p>Instalar conector específico</p>
</li>
<li>
<p>Configurar parámetros de conexión</p>
</li>
<li>
<p>Cargar datos como documentos</p>
</li>
</ol>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 2. Los principales conectores incluyen:</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 75%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Reader</th>
<th class="tableblock halign-left valign-top">Descripción</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">PDFReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee y extrae texto de archivos PDF.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">DocxReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee archivos de Microsoft Word (.docx).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">EpubReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee archivos EPUB.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">MarkdownReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee archivos Markdown (.md).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">HTMLTagReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Extrae texto de archivos HTML locales.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ImageReader / ImageCaptionReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Procesa imágenes y extrae texto o descripciones.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CSVReader / PagedCSVReader / PandasCSVReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee archivos CSV.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">RTFReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee archivos RTF.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">MboxReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee archivos de correo electrónico MBOX.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">PptxReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee presentaciones de PowerPoint.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">IPYNBReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee notebooks de Jupyter.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">FlatReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee archivos de texto plano.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">UnstructuredReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Procesa documentos no estructurados.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">PyMuPDFReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Alternativa para leer PDFs usando PyMuPDF.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">XMLReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee y procesa archivos XML.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SitemapReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Extrae y procesa páginas web a partir de un sitemap XML.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">WebPageReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Extrae contenido directamente de URLs individuales.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">NotionPageReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Extrae contenido de páginas de Notion.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ObsidianReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee y procesa notas de Obsidian.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">GoogleDriveReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Carga archivos y carpetas desde Google Drive.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">GoogleDocsReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee documentos de Google Docs.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">GoogleSheetsReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lee hojas de cálculo de Google Sheets.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">GoogleMapsTextSearchReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Busca y carga resultados de Google Maps.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">GoogleChatReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Extrae mensajes de Google Chat.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">DatabaseReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Permite ejecutar queries SQL y extraer datos de bases de datos compatibles con SQLAlchemy.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">StringIterableReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Convierte listas de strings directamente en documentos LlamaIndex.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">VideoAudioReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Extrae texto de archivos de vídeo y audio.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ImageVisionLLMReader</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Procesa imágenes usando modelos de visión.</p></td>
</tr>
</tbody>
</table>
<div class="listingblock">
<div class="title">Ejemplo 1: Carga desde sitemap web</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.readers.web</span> <span class="keyword">import</span> <span class="include">SimpleWebPageReader</span>

<span class="comment"># URL del sitemap.xml del sitio que quieres leer</span>
sitemap_url = <span class="string"><span class="delimiter">&quot;</span><span class="content">https://gpt-index.readthedocs.io/sitemap.xml</span><span class="delimiter">&quot;</span></span>

<span class="comment"># Instancia el lector de sitemaps</span>
<span class="comment"># reader = SitemapReader(sitemap_url, html_to_text=True, limit=5)  # limit opcional para limitar páginas</span>

<span class="comment"># Carga los documentos del sitemap</span>
documents = SimpleWebPageReader(html_to_text=<span class="predefined-constant">True</span>).load_data(
    [<span class="string"><span class="delimiter">&quot;</span><span class="content">http://paulgraham.com/worked.html</span><span class="delimiter">&quot;</span></span>]
)


<span class="comment"># Muestra un resumen de los documentos obtenidos</span>
<span class="keyword">for</span> i, doc <span class="keyword">in</span> <span class="predefined">enumerate</span>(documents):
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">--- Documento {i+1} ---</span><span class="delimiter">&quot;</span></span>)
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">URL:</span><span class="delimiter">&quot;</span></span>, doc.metadata.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">url</span><span class="delimiter">&quot;</span></span>))
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Contenido (primeros 3000 caracteres):</span><span class="delimiter">&quot;</span></span>)
    print(doc.text[:<span class="integer">3000</span>])
    print()</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Ejemplo 2: Integración con Notion</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.readers.notion</span> <span class="keyword">import</span> <span class="include">NotionPageReader</span>

pages = NotionPageReader(
    integration_token=<span class="string"><span class="delimiter">&quot;</span><span class="content">secret_...</span><span class="delimiter">&quot;</span></span>
).load_data(page_ids=[<span class="string"><span class="delimiter">&quot;</span><span class="content">12345</span><span class="delimiter">&quot;</span></span>])  <span class="comment">#</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Ejemplo 3: Carga masiva local</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">SimpleDirectoryReader</span>

documents = SimpleDirectoryReader(
    input_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">datos/</span><span class="delimiter">&quot;</span></span>,
    required_exts=[<span class="string"><span class="delimiter">&quot;</span><span class="content">.pdf</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">.docx</span><span class="delimiter">&quot;</span></span>],
    recursive=<span class="predefined-constant">True</span>
).load_data()  <span class="comment">#</span></code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Caso avanzado - PostgreSQL con Ollama:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.readers.postgres</span> <span class="keyword">import</span> <span class="include">PostgresReader</span>

reader = PostgresReader(
    host=<span class="string"><span class="delimiter">&quot;</span><span class="content">localhost</span><span class="delimiter">&quot;</span></span>,
    user=<span class="string"><span class="delimiter">&quot;</span><span class="content">usuario</span><span class="delimiter">&quot;</span></span>,
    password=<span class="string"><span class="delimiter">&quot;</span><span class="content">contraseña</span><span class="delimiter">&quot;</span></span>,
    dbname=<span class="string"><span class="delimiter">&quot;</span><span class="content">ventas</span><span class="delimiter">&quot;</span></span>
)
query = <span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">    SELECT cliente, ventas</span><span class="content">
</span><span class="content">    FROM transacciones</span><span class="content">
</span><span class="content">    WHERE fecha &gt; '2024-01-01'</span><span class="content">
</span><span class="delimiter">&quot;&quot;&quot;</span></span>
documents = reader.load_data(query=query)  <span class="comment">#</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_fases_de_indexación_y_carga_de_datos">4. Fases de Indexación y Carga de Datos</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_fase_de_ingesta_y_carga_de_datos">4.1. Fase de ingesta y carga de datos</h3>
<div class="paragraph">
<p>Proceso inicial para integrar datos desde múltiples fuentes usando <strong>160+ conectores</strong> de LlamaHub:</p>
</div>
<div class="listingblock">
<div class="title">Carga desde directorio local (no estructurados):</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">SimpleDirectoryReader</span>

<span class="comment"># Verificar que el directorio 'docs/' exista y contenga archivos</span>
<span class="keyword">try</span>:
    documents = SimpleDirectoryReader(
        input_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">docs/</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Asegúrate que esta carpeta existe</span>
        required_exts=[<span class="string"><span class="delimiter">&quot;</span><span class="content">.pdf</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">.md</span><span class="delimiter">&quot;</span></span>],
        recursive=<span class="predefined-constant">True</span>
    ).load_data()

    print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">✅ Documentos cargados: {len(documents)}</span><span class="delimiter">&quot;</span></span>)
    <span class="keyword">for</span> doc <span class="keyword">in</span> documents:
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content"> - {doc.metadata.get('file_name')}</span><span class="delimiter">&quot;</span></span>)

<span class="keyword">except</span> <span class="exception">Exception</span> <span class="keyword">as</span> e:
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">❌ Error: {str(e)}</span><span class="delimiter">&quot;</span></span>)
    documents = []  <span class="comment"># Definir variable como lista vacía para evitar errores</span>

<span class="comment"># Verificar si hay documentos cargados</span>
<span class="keyword">if</span> <span class="keyword">not</span> documents:
    print(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">⚠️  No se encontraron documentos. Verifica:</span><span class="delimiter">&quot;</span></span>)
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">1. Que el directorio 'docs/' existe</span><span class="delimiter">&quot;</span></span>)
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">2. Que contiene archivos PDF o Markdown (.md)</span><span class="delimiter">&quot;</span></span>)
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">3. Que tienes instaladas las dependencias: pip install pymupdf python-docx</span><span class="delimiter">&quot;</span></span>)
<span class="keyword">else</span>:
    <span class="comment"># Aquí puedes continuar con tu procesamiento</span>
    print(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">¡Documentos listos para usar!</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Carga desde API web (semiestructurados):</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Instala las dependencias si es necesario:</span>
<span class="comment"># pip install llama-index-readers-web llama-index-llms-ollama</span>

<span class="keyword">from</span> <span class="include">llama_index.readers.web</span> <span class="keyword">import</span> <span class="include">BeautifulSoupWebReader</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>

<span class="comment"># 1. Define la(s) URL(s) que quieres leer</span>
urls = [<span class="string"><span class="delimiter">&quot;</span><span class="content">https://es.wikipedia.org/wiki/Abraham_Lincoln</span><span class="delimiter">&quot;</span></span>]

<span class="comment"># 2. Instancia el reader y carga los documentos desde la web</span>
reader = BeautifulSoupWebReader()
documents = reader.load_data(urls=urls)

<span class="comment"># 3. Configura el modelo Ollama como LLM local</span>
llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Cambia por el modelo que tengas descargado en Ollama</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>,
    temperature=<span class="float">0.2</span>,
    request_timeout=<span class="float">120.0</span>
)

<span class="comment"># 4. Resume el contenido extraído de cada página</span>
<span class="keyword">for</span> i, doc <span class="keyword">in</span> <span class="predefined">enumerate</span>(documents):
    prompt = (
        <span class="string"><span class="delimiter">&quot;</span><span class="content">Resume en español el siguiente texto web, resaltando los puntos más importantes:</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        f<span class="string"><span class="delimiter">&quot;</span><span class="content">{doc.text[:6000]}</span><span class="char">\n</span><span class="char">\n</span><span class="content">Resumen:</span><span class="delimiter">&quot;</span></span>
    )
    resumen = llm.complete(prompt).text
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">--- Resumen de la página {i+1} ({doc.metadata.get('url', '')}) ---</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)
    print(resumen)</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Carga desde PostgreSQL (estructurados):</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Instalar dependencias necesarias</span>
<span class="comment"># pip install llama-index-readers-postgres llama-index-llms-ollama psycopg2-binary</span>

<span class="keyword">from</span> <span class="include">llama_index_cloud_sql_pg</span> <span class="keyword">import</span> <span class="include">PostgresEngine</span>, <span class="include">PostgresReader</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">Settings</span>

<span class="comment"># 1. Configurar conexión a PostgreSQL</span>
async <span class="keyword">def</span> <span class="function">setup_postgres</span>():
    engine = await PostgresEngine.afrom_instance(
        project_id=<span class="string"><span class="delimiter">&quot;</span><span class="content">tu-proyecto-gcp</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Solo para Cloud SQL</span>
        region=<span class="string"><span class="delimiter">&quot;</span><span class="content">us-central1</span><span class="delimiter">&quot;</span></span>,
        instance=<span class="string"><span class="delimiter">&quot;</span><span class="content">tu-instancia</span><span class="delimiter">&quot;</span></span>,
        database=<span class="string"><span class="delimiter">&quot;</span><span class="content">tu-db</span><span class="delimiter">&quot;</span></span>,
        user=<span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        password=<span class="string"><span class="delimiter">&quot;</span><span class="content">tu-password</span><span class="delimiter">&quot;</span></span>
    )
    <span class="keyword">return</span> engine

<span class="comment"># 2. Configurar Ollama como LLM local</span>
Settings.llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>,
    temperature=<span class="float">0.3</span>
)

<span class="comment"># 3. Cargar documentos desde PostgreSQL</span>
async <span class="keyword">def</span> <span class="function">load_and_process_data</span>():
    engine = await setup_postgres()

    <span class="comment"># Opción 1: Cargar desde tabla completa</span>
    reader = await PostgresReader.create(
        engine=engine,
        table_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">documentos</span><span class="delimiter">&quot;</span></span>,
        content_columns=[<span class="string"><span class="delimiter">&quot;</span><span class="content">contenido</span><span class="delimiter">&quot;</span></span>],
        metadata_columns=[<span class="string"><span class="delimiter">&quot;</span><span class="content">autor</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">fecha</span><span class="delimiter">&quot;</span></span>]
    )

    <span class="comment"># Opción 2: Cargar con query personalizada</span>
    <span class="comment"># reader = await PostgresReader.create(</span>
    <span class="comment">#     engine=engine,</span>
    <span class="comment">#     query=&quot;SELECT * FROM documentos WHERE categoria = 'tecnologia'&quot;,</span>
    <span class="comment">#     content_columns=[&quot;titulo&quot;, &quot;contenido&quot;],</span>
    <span class="comment">#     metadata_columns=[&quot;id&quot;]</span>
    <span class="comment"># )</span>

    documents = await reader.aload_data()

    <span class="comment"># 4. Procesar documentos con Ollama</span>
    <span class="keyword">for</span> doc <span class="keyword">in</span> documents:
        prompt = f<span class="string"><span class="delimiter">&quot;</span><span class="content">Resume este documento técnico: {doc.text[:2000]}</span><span class="delimiter">&quot;</span></span>
        resumen = Settings.llm.complete(prompt)
        doc.metadata[<span class="string"><span class="delimiter">&quot;</span><span class="content">resumen</span><span class="delimiter">&quot;</span></span>] = resumen.text
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Documento {doc.metadata.get('id')} resumido</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># Ejecutar el flujo</span>
<span class="keyword">import</span> <span class="include">asyncio</span>
asyncio.run(load_and_process_data())</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_fase_de_indexación_creación_de_índices_vectoriales_y_otras_estructuras">4.2. Fase de indexación: creación de índices vectoriales y otras estructuras</h3>
<div class="paragraph">
<p>La fase de indexación en LlamaIndex transforma los datos brutos en estructuras consultables, optimizando la recuperación de información en aplicaciones RAG. A continuación se describen los principales tipos de índices y su proceso de creación.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 3. Tabla comparativa de índices</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Escenario</th>
<th class="tableblock halign-left valign-top">Índice Recomendado</th>
<th class="tableblock halign-left valign-top">Ventaja</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Búsqueda semántica</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">VectorStoreIndex</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Contextualización precisa</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Síntesis documental</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">SummaryIndex</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Visión panorámica</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Filtrado por metadatos</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">KeywordTableIndex</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Precisión en términos específicos</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Jerarquías complejas</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TreeIndex</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Razonamiento multinivel</p></td>
</tr>
</tbody>
</table>
<div class="sect3">
<h4 id="_vectorstoreindex">4.2.1. VectorStoreIndex</h4>
<div class="ulist">
<div class="title">Proceso básico:</div>
<ul>
<li>
<p>División de documentos en nodos (fragmentos de 2048 tokens por defecto) para gestionar contextos extensos.</p>
</li>
<li>
<p>Generación de embeddings: cada nodo se convierte en un vector numérico usando modelos como OpenAI o Sentence Transformers.</p>
</li>
<li>
<p>Almacenamiento estructurado:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Un índice vectorial almacena nodos y sus embeddings, permitiendo búsquedas semánticas eficientes.</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">VectorStoreIndex</span>, <span class="include">SimpleDirectoryReader</span>
<span class="keyword">from</span> <span class="include">llama_index.embeddings.ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbedding</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>

<span class="comment"># 1. Configurar modelos</span>
embed_model = OllamaEmbedding(
    model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Modelo de embeddings</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>
)

llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Modelo para generación</span>
    request_timeout=<span class="float">300.0</span>
)

<span class="comment"># 2. Cargar documentos</span>
documents = SimpleDirectoryReader(<span class="string"><span class="delimiter">&quot;</span><span class="content">./docs</span><span class="delimiter">&quot;</span></span>).load_data()

<span class="comment"># 3. Crear índice vectorial</span>
index = VectorStoreIndex.from_documents(
    documents,
    embed_model=embed_model,
)

<span class="comment"># 4. Crear motor de consulta</span>
query_engine = index.as_query_engine(llm=llm)

<span class="comment"># 5. Ejecutar consulta</span>
response = query_engine.query(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Cuál es el tema principal de los documentos?</span><span class="delimiter">&quot;</span></span>)
print(response)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Personalización:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Ajuste del tamaño de fragmentos mediante ServiceContext.from_defaults(chunk_size=512).</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>Inclusión de metadatos (etiquetas, fechas, categorías) para filtrado híbrido.</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_summaryindex">4.2.2. SummaryIndex</h4>
<div class="paragraph">
<p>Es una estructura de datos diseñada para almacenar y consultar información de manera eficiente, centrándose en la síntesis de resúmenes de documentos.</p>
</div>
<div class="ulist">
<div class="title">Funcionamiento del summary index:</div>
<ul>
<li>
<p><strong>Construcción del índice:</strong> Durante la creación del summary index, los textos de los documentos se fragmentan ("chunked"), se convierten en nodos y se almacenan en una secuencia (lista). Cada nodo representa una parte del texto original</p>
</li>
<li>
<p><strong>Consulta:</strong> Cuando se realiza una consulta, el summary index itera sobre los nodos (fragmentos) y sintetiza una respuesta utilizando todos ellos, aplicando filtros opcionales si es necesario</p>
</li>
<li>
<p><strong>Resumen por documento:</strong> En el caso del "document summary index", se extrae un resumen de cada documento y se almacena junto con los nodos correspondientes a ese documento. Al consultar, primero se seleccionan los documentos relevantes basándose en sus resúmenes, y luego se recuperan los fragmentos asociados a esos documentos para generar la respuesta final</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Organiza los nodos en secuencia lineal para síntesis global.</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">SimpleDirectoryReader</span>, <span class="include">SummaryIndex</span>
<span class="keyword">from</span> <span class="include">llama_index.core.node_parser</span> <span class="keyword">import</span> <span class="include">SentenceSplitter</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">Settings</span>

<span class="comment"># 1. Configurar modelos</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>, base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>)
Settings.llm = llm

<span class="comment"># 2. Cargar y dividir documentos</span>
documents = SimpleDirectoryReader(<span class="string"><span class="delimiter">&quot;</span><span class="content">./docs</span><span class="delimiter">&quot;</span></span>).load_data()
splitter = SentenceSplitter(chunk_size=<span class="integer">512</span>)
nodes = splitter(documents)

<span class="comment"># 3. Crear índice de resúmenes</span>
index = SummaryIndex(nodes)

<span class="comment"># 4. Configurar motor de consulta con síntesis jerárquica</span>
query_engine = index.as_query_engine(
    response_mode=<span class="string"><span class="delimiter">&quot;</span><span class="content">tree_summarize</span><span class="delimiter">&quot;</span></span>,
    use_async=<span class="predefined-constant">True</span>
)

<span class="comment"># 5. Generar resumen</span>
response = query_engine.query(<span class="string"><span class="delimiter">&quot;</span><span class="content">Resume los temas principales del documento</span><span class="delimiter">&quot;</span></span>)
print(response)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Ideal para generar resúmenes ejecutivos o respuestas panorámicas.</p>
</div>
<div class="sect4">
<h5 id="_keywordtableindex">KeywordTableIndex</h5>
<div class="paragraph">
<p>KeywordTableIndex es un mecanismo de Indexación por Palabras Clave que permite búsquedas rápidas y precisas basadas en términos clave. Utiliza una tabla hash para almacenar pares de palabras clave y nodos documentales, facilitando la recuperación de información relevante.</p>
</div>
<div class="ulist">
<div class="title">Características clave:</div>
<ul>
<li>
<p><strong>Tabla Hash Conceptual</strong>: Almacena pares <code>(keyword, lista_de_nodos)</code></p>
</li>
<li>
<p><strong>Nodos Documentales</strong>: Fragmentos de texto procesados (oraciones/párrafos)</p>
</li>
<li>
<p><strong>Metadatos Asociados</strong>: Información contextual de cada nodo</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Proceso de Indexación:</div>
<ul>
<li>
<p><strong>Segmentación</strong>: Divide documentos en nodos usando <code>NodeParser</code></p>
</li>
<li>
<p><strong>Extracción Keywords</strong>: Usa modelos LLM para identificar términos relevantes</p>
</li>
<li>
<p><strong>Mapeo Inverso</strong>: Crea relación keywords → nodos</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Consulta:</div>
<ul>
<li>
<p>Análisis léxico de la pregunta</p>
</li>
<li>
<p>Búsqueda en tabla de keywords</p>
</li>
<li>
<p>Recuperación de nodos relevantes</p>
</li>
<li>
<p>Síntesis de respuesta</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Parámetros principales:</strong>
- <code>max_keywords_per_chunk</code>: Controla densidad terminológica
- <code>keyword_extract_template</code>: Define estrategia de extracción
- <code>retriever_mode</code>: Tipo de búsqueda (<code>simple</code>/<code>rake</code>/<code>default</code>)</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 4. Ventajas Comparativas</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Característica</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">KeywordTableIndex</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">VectorIndex</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Velocidad consultas</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Alto</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Medio</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Requisitos recursos</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Bajos</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Altos</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Precisión léxica</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Excelente</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Regular</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Manejo sinónimos</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Limitado</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Bueno</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><strong>Casos ideales de uso:</strong>
- Búsqueda exacta de términos técnicos
Optimiza consultas que requieren razonamiento multinivel.
- Documentación con vocabulario controlado
- Entornos con limitaciones hardware</p>
</div>
<div class="listingblock">
<div class="title">Permite búsquedas rápidas y precisas basadas en términos clave, ideal para documentos con metadatos ricos o etiquetas.</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">SimpleDirectoryReader</span>, <span class="include">KeywordTableIndex</span>
<span class="keyword">from</span> <span class="include">llama_index.core.node_parser</span> <span class="keyword">import</span> <span class="include">SimpleNodeParser</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">Settings</span>

<span class="comment"># 1. Configurar modelo de Ollama</span>
Settings.llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>, base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 2. Cargar y dividir documentos</span>
documents = SimpleDirectoryReader(<span class="string"><span class="delimiter">&quot;</span><span class="content">./docs</span><span class="delimiter">&quot;</span></span>).load_data()
parser = SimpleNodeParser.from_defaults(chunk_size=<span class="integer">512</span>)
nodes = parser.get_nodes_from_documents(documents)

<span class="comment"># 3. Crear índice de tabla de palabras clave</span>
index = KeywordTableIndex(nodes)

<span class="comment"># 4. Configurar motor de consulta</span>
query_engine = index.as_query_engine(
    retriever_mode=<span class="string"><span class="delimiter">&quot;</span><span class="content">simple</span><span class="delimiter">&quot;</span></span>,
    max_keywords_per_query=<span class="integer">5</span>
)

<span class="comment"># 5. Ejecutar consulta basada en keywords</span>
response = query_engine.query(<span class="string"><span class="delimiter">&quot;</span><span class="content">Explica el concepto de aprendizaje automático</span><span class="delimiter">&quot;</span></span>)
print(response)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Permite búsquedas exactas por etiquetas o metadatos.</p>
</div>
</div>
<div class="sect4">
<h5 id="_treeindex">TreeIndex</h5>
<div class="paragraph">
<p>El <code>TreeIndex</code> es una estructura de índice jerárquica en la que cada nodo representa un resumen de sus nodos hijos. Se construye siguiendo un enfoque de abajo hacia arriba: los fragmentos de texto (nodos hoja) se agrupan y se sintetizan resúmenes en niveles superiores, formando así un árbol de resúmenes hasta llegar a uno o varios nodos raíz.</p>
</div>
<div class="ulist">
<div class="title">Estructura y Funcionamiento</div>
<ul>
<li>
<p>Cada nodo hoja contiene un fragmento de texto original.</p>
</li>
<li>
<p>Los nodos internos contienen resúmenes generados automáticamente de sus hijos.</p>
</li>
<li>
<p>El parámetro <code>num_children</code> controla cuántos hijos puede tener cada nodo padre (por defecto, 10).</p>
</li>
<li>
<p>La construcción del árbol puede mostrar progreso y ser asíncrona (<code>use_async</code>).</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Proceso de Indexación</div>
<ol class="arabic">
<li>
<p><strong>División</strong>: Los documentos se dividen en fragmentos (nodos hoja).</p>
</li>
<li>
<p><strong>Agrupación</strong>: Los nodos hoja se agrupan en nodos padres, resumiendo el contenido de los hijos.</p>
</li>
<li>
<p><strong>Iteración</strong>: El proceso se repite hasta formar el/los nodo(s) raíz.</p>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Consulta</div>
<p>Durante la consulta, existen dos modos principales:
* <strong>Recorrido descendente</strong>: Se parte del nodo raíz y se baja por el árbol seleccionando los nodos más relevantes en cada nivel.
* <strong>Síntesis directa</strong>: Se genera una respuesta directamente a partir de los nodos raíz.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 5. Tabla de Parámetros Clave</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Nombre</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Tipo</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Descripción</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Valor por defecto</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">summary_template</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Template</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Prompt para resumir nodos hijos</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">None</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">insert_prompt</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Template</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Prompt para inserción de nodos</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">None</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">num_children</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">int</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Hijos por nodo padre</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">10</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">build_tree</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">bool</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Construir árbol al crear el índice</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">True</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">show_progress</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">bool</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Mostrar barra de progreso</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">False</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<div class="title">Ventajas y Usos</div>
<ul>
<li>
<p>Ideal para documentos largos o jerárquicos.</p>
</li>
<li>
<p>Permite síntesis progresiva y respuestas más estructuradas.</p>
</li>
<li>
<p>Escalable y eficiente para consultas que requieren visión global o resúmenes de alto nivel.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Ejemplo de creación y consulta de un índice de árbol:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">SimpleDirectoryReader</span>, <span class="include">TreeIndex</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">Settings</span>

<span class="comment"># 1. Configurar modelo de Ollama</span>
Settings.llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>, base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 2. Cargar documentos</span>
documents = SimpleDirectoryReader(<span class="string"><span class="delimiter">&quot;</span><span class="content">./docs</span><span class="delimiter">&quot;</span></span>).load_data()

<span class="comment"># 3. Crear índice jerárquico con parámetros personalizados</span>
index = TreeIndex.from_documents(
    documents,
    num_children=<span class="integer">5</span>,  <span class="comment"># 5 nodos hijos por nivel</span>
    build_tree=<span class="predefined-constant">True</span>,  <span class="comment"># Construir estructura durante indexación</span>
    show_progress=<span class="predefined-constant">True</span>  <span class="comment"># Mostrar barra de progreso</span>
)

<span class="comment"># 4. Configurar motor de consulta con traversing</span>
query_engine = index.as_query_engine(
    child_branch_factor=<span class="integer">2</span>,  <span class="comment"># Explorar 2 ramas por nivel</span>
    response_mode=<span class="string"><span class="delimiter">&quot;</span><span class="content">tree_summarize</span><span class="delimiter">&quot;</span></span>  <span class="comment"># Síntesis jerárquica</span>
)

<span class="comment"># 5. Ejecutar consulta compleja</span>
response = query_engine.query(<span class="string"><span class="delimiter">&quot;</span><span class="content">Analiza comparativamente los temas principales del documento</span><span class="delimiter">&quot;</span></span>)
print(response)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_indexnode">4.2.3. IndexNode</h4>
<div class="paragraph">
<p>Un <code>IndexNode</code> en LlamaIndex es una estructura que representa un fragmento ("chunk") de un documento fuente, típicamente texto, aunque puede ser también una imagen u otro tipo de dato. Es una especialización de <code>TextNode</code>, por lo que hereda sus propiedades y funcionalidades, y está diseñado para ser utilizado dentro de los distintos índices de LlamaIndex, como VectorStoreIndex, TreeIndex, SummaryIndex, entre otros.</p>
</div>
<div class="ulist">
<div class="title">Propiedades principales</div>
<ul>
<li>
<p>Contiene el contenido textual o multimodal del fragmento.</p>
</li>
<li>
<p>Almacena metadatos relevantes (por ejemplo, fuente, posición, etiquetas).</p>
</li>
<li>
<p>Gestiona relaciones con otros nodos mediante el atributo <code>relationships</code>, permitiendo definir conexiones como siguiente, anterior, padre, etc.</p>
</li>
<li>
<p>Cada nodo tiene un identificador único (<code>node_id</code>), que puede asignarse manualmente o generarse automáticamente.</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Creación y uso</div>
<p>Puedes crear nodos de manera manual o automática. Lo más común es usar un parser (por ejemplo, <code>SentenceSplitter</code>) para dividir documentos en nodos:</p>
</div>
<div class="listingblock">
<div class="title">Ejemplo de creación automática de nodos:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core.schema</span> <span class="keyword">import</span> <span class="include">TextNode</span>, <span class="include">NodeRelationship</span>, <span class="include">RelatedNodeInfo</span>
<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">VectorStoreIndex</span>, <span class="include">Settings</span>
<span class="keyword">from</span> <span class="include">llama_index.embeddings.ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbedding</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>

<span class="comment"># 1. Configura Ollama como modelo de embeddings y LLM global</span>
Settings.embed_model = OllamaEmbedding(
    model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># O el modelo de embeddings que hayas descargado en Ollama</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>
)
Settings.llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># O el modelo LLM que prefieras</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>,
    request_timeout=<span class="float">60.0</span>
)

<span class="comment"># 2. Crea dos nodos de texto manualmente</span>
node1 = TextNode(text=<span class="string"><span class="delimiter">&quot;</span><span class="content">La inteligencia artificial permite a las máquinas aprender de los datos.</span><span class="delimiter">&quot;</span></span>, id_=<span class="string"><span class="delimiter">&quot;</span><span class="content">nodo_1</span><span class="delimiter">&quot;</span></span>)
node2 = TextNode(text=<span class="string"><span class="delimiter">&quot;</span><span class="content">El aprendizaje automático es una rama de la inteligencia artificial.</span><span class="delimiter">&quot;</span></span>, id_=<span class="string"><span class="delimiter">&quot;</span><span class="content">nodo_2</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 3. Define relaciones entre los nodos (opcional)</span>
node1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=node2.node_id)
node2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=node1.node_id)

<span class="comment"># 4. Construye el índice vectorial usando Ollama para los embeddings</span>
index = VectorStoreIndex(nodes=[node1, node2])

<span class="comment"># 5. Consulta el índice usando Ollama como modelo LLM</span>
query_engine = index.as_query_engine()
response = query_engine.query(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Qué es el aprendizaje automático?</span><span class="delimiter">&quot;</span></span>)
print(response)</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Los <code>IndexNode</code> son la unidad básica sobre la que operan los índices de LlamaIndex. Por ejemplo:</div>
<ul>
<li>
<p>En un VectorStoreIndex, cada nodo se representa como un vector y se almacena para búsquedas semánticas.</p>
</li>
<li>
<p>En un TreeIndex, los nodos hoja son los fragmentos originales y los nodos internos son resúmenes de estos.</p>
</li>
<li>
<p>En un KeywordTableIndex, los nodos se indexan por las palabras clave que contienen.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Ejemplo de workflow</div>
<ol class="arabic">
<li>
<p>Cargar documentos y dividirlos en nodos.</p>
</li>
<li>
<p>Crear relaciones entre nodos si es necesario.</p>
</li>
<li>
<p>Construir el índice deseado (vectorial, jerárquico, etc.) usando la lista de nodos.</p>
</li>
<li>
<p>Realizar consultas, que internamente recuperan y procesan los nodos relevantes.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_almacenamiento_y_reutilización_storagecontext_y_persistencia">4.2.4. Almacenamiento y reutilización: StorageContext y persistencia</h4>
<div class="ulist">
<div class="title">El <code>StorageContext</code> es un contenedor utilitario que centraliza el almacenamiento de:</div>
<ul>
<li>
<p><strong>Nodos</strong>: Fragmentos de documentos procesados (<code>TextNode</code>, <code>IndexNode</code>)</p>
</li>
<li>
<p><strong>Índices</strong>: Metadatos de estructuras de índices (vectoriales, árboles, etc.)</p>
</li>
<li>
<p><strong>Vectores</strong>: Representaciones de embeddings generadas</p>
</li>
<li>
<p><strong>Grafos</strong>: Relaciones entre nodos (opcional)</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 6. Componentes Principales</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Componente</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Descripción</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Implementación por defecto</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>docstore</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Almacena nodos/documentos</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>SimpleDocumentStore</code> (memoria)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>index_store</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Guarda metadatos de índices</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>SimpleIndexStore</code> (memoria)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>vector_store</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Contiene vectores de embeddings</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>SimpleVectorStore</code> (memoria)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>graph_store</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Maneja relaciones complejas</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>SimpleGraphStore</code> (opcional)</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<div class="title">Funcionalidades Clave</div>
<ul>
<li>
<p><strong>Persistencia</strong>: Guarda/recupera todo el estado en disco</p>
</li>
<li>
<p><strong>Personalización</strong>: Permite usar diferentes backends (Chroma, Qdrant, Redis, etc.)</p>
</li>
<li>
<p><strong>Multi-almacén</strong>: Soporta múltiples <code>vector_stores</code> simultáneos</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Creación y uso básico:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">VectorStoreIndex</span>, <span class="include">SimpleDirectoryReader</span>, <span class="include">StorageContext</span>, <span class="include">load_index_from_storage</span>
<span class="keyword">from</span> <span class="include">llama_index.embeddings.ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbedding</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">Settings</span>

<span class="comment"># 1. Configurar modelos de Ollama</span>
Settings.embed_model = OllamaEmbedding(
    model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Modelo de embeddings</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>
)
Settings.llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Modelo para generación</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>,
    request_timeout=<span class="float">300.0</span>
)

<span class="comment"># 2. Cargar documentos</span>
documents = SimpleDirectoryReader(<span class="string"><span class="delimiter">&quot;</span><span class="content">./docs</span><span class="delimiter">&quot;</span></span>).load_data()

<span class="comment"># 3. Crear StorageContext y vector store</span>
storage_context = StorageContext.from_defaults()

<span class="comment"># 4. Construir índice vectorial con Ollama</span>
index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context,
    embed_model=Settings.embed_model
)

<span class="comment"># 5. Persistir el índice</span>
storage_context.persist(persist_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">./mi_almacenamiento</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 6. Cargar desde almacenamiento</span>
nuevo_storage_context = StorageContext.from_defaults(
    persist_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">./mi_almacenamiento</span><span class="delimiter">&quot;</span></span>
)
index_cargado = load_index_from_storage(nuevo_storage_context)

<span class="comment"># 7. Consultar el índice</span>
query_engine = index_cargado.as_query_engine(llm=Settings.llm)
respuesta = query_engine.query(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Cuál es el tema principal?</span><span class="delimiter">&quot;</span></span>)
print(respuesta)</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Consideraciones Importantes</div>
<ul>
<li>
<p><strong>Compatibilidad</strong>: Verificar que los backends usados sean compatibles con LlamaIndex</p>
</li>
<li>
<p><strong>Persistencia completa</strong>: Al usar <code>persist()</code>, asegurarse que todos los componentes estén configurados para persistir</p>
</li>
<li>
<p><strong>Rendimiento</strong>: Almacenes en memoria son más rápidos pero volátiles, discos/remotos ofrecen persistencia</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_fase_de_consulta_recuperación_y_generación_aumentada_por_recuperación_rag">4.3. Fase de consulta: recuperación y generación aumentada por recuperación (RAG)</h3>
<div class="paragraph">
<p>La fase de consulta en LlamaIndex combina la recuperación semántica de fragmentos relevantes y la generación aumentada por recuperación (RAG) para ofrecer respuestas precisas y contextualizadas.</p>
</div>
<div class="ulist">
<div class="title">El proceso consta de tres etapas principales:</div>
<ul>
<li>
<p>Recuperación de Información</p>
</li>
<li>
<p>Posprocesamiento de nodos recuperados</p>
</li>
<li>
<p>Síntesis de Respuesta (RAG)</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_recuperación_de_información">4.3.1. Recuperación de Información</h4>
<div class="paragraph">
<p><strong>Objetivo:</strong> Identificar los fragmentos más relevantes del índice.</p>
</div>
<div class="paragraph">
<p><strong>Mecanismos:</strong>
- Búsqueda vectorial (similitud de embeddings)
- Filtrado por metadatos (autor, fecha, fuente)
- Recuperación híbrida (combinación de keywords y semántica)</p>
</div>
<div class="listingblock">
<div class="title">El <code>Retriever</code> es el componente encargado de esta tarea, configurado con parámetros como:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Configurar el retriever con parámetros personalizados</span>
retriever = index.as_retriever(
    similarity_top_k=<span class="integer">5</span>,  <span class="comment"># Recuperar 5 nodos más similares</span>
    vector_store_query_mode=<span class="string"><span class="delimiter">&quot;</span><span class="content">hybrid</span><span class="delimiter">&quot;</span></span>  <span class="comment"># Búsqueda semántica + keywords</span>
)
nodes = retriever.retrieve(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Qué modelos de Ollama soportan embeddings?</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_posprocesamiento">4.3.2. Posprocesamiento</h4>
<div class="paragraph">
<p><strong>Técnicas aplicadas a los nodos recuperados:</strong>
- <strong>Re-ranking:</strong> Reordenar resultados con modelos como <code>bge-reranker</code>
- <strong>Filtrado:</strong> Eliminar nodos de baja relevancia (<code>similarity_cutoff=0.7</code>)
- <strong>Fusión:</strong> Combinar fragmentos relacionados contextualmente</p>
</div>
<div class="listingblock">
<div class="title">El posprocesamiento mejora la precisión y relevancia de los nodos antes de la síntesis final. Por ejemplo, se puede usar un modelo de re-ranking para ajustar el orden de los nodos recuperados según su relevancia para la consulta.</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core.postprocessor</span> <span class="keyword">import</span> <span class="include">SentenceTransformerRerank</span>

reranker = SentenceTransformerRerank(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">cross-encoder/ms-marco-MiniLM-L-6-v2</span><span class="delimiter">&quot;</span></span>)
nodes_reranked = reranker.postprocess_nodes(nodes, query_str=query)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_síntesis_de_respuesta_rag">4.3.3. Síntesis de Respuesta (RAG)</h4>
<div class="olist arabic">
<div class="title"><strong>Proceso:</strong></div>
<ol class="arabic">
<li>
<p>Los nodos relevantes se inyectan como contexto en el prompt.</p>
</li>
<li>
<p>El LLM genera una respuesta natural basada en el contexto y la pregunta.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="title">El <code>QueryEngine</code> es el componente que integra todo el proceso, permitiendo consultas sobre el índice y generando respuestas contextuales.</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>

<span class="comment"># Configurar modelo generativo</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3:8b</span><span class="delimiter">&quot;</span></span>, temperature=<span class="float">0.3</span>)
query_engine = index.as_query_engine(llm=llm, response_mode=<span class="string"><span class="delimiter">&quot;</span><span class="content">compact</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># Generar respuesta</span>
response = query_engine.query(<span class="string"><span class="delimiter">&quot;</span><span class="content">Explica el mecanismo de atención en transformers</span><span class="delimiter">&quot;</span></span>)
print(response)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Ejemplo de salida:</strong>
"El mecanismo de atención permite a los modelos procesar relaciones contextuales entre palabras, asignando pesos diferenciales a cada token&#8230;&#8203;"</p>
</div>
</div>
<div class="sect3">
<h4 id="_ejemplo_de_consulta_rag">4.3.4. Ejemplo de consulta RAG</h4>
<div class="listingblock">
<div class="title">Un ejemplo completo de consulta RAG con LlamaIndex:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># 1. Instalación de dependencias (requiere Ollama corriendo localmente)</span>
<span class="comment"># pip install llama-index-core llama-index-llms-ollama llama-index-embeddings-ollama</span>

<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">VectorStoreIndex</span>, <span class="include">SimpleDirectoryReader</span>, <span class="include">StorageContext</span>
<span class="keyword">from</span> <span class="include">llama_index.embeddings.ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbedding</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.core.postprocessor</span> <span class="keyword">import</span> <span class="include">SentenceTransformerRerank</span>
<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">Settings</span>

<span class="comment"># 2. Configuración de modelos Ollama</span>
Settings.embed_model = OllamaEmbedding(
    model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Modelo de embeddings</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>
)
Settings.llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Modelo generativo</span>
    base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>,
    temperature=<span class="float">0.3</span>
)

<span class="comment"># 3. Carga y procesamiento de documentos</span>
documents = SimpleDirectoryReader(<span class="string"><span class="delimiter">&quot;</span><span class="content">./docs</span><span class="delimiter">&quot;</span></span>).load_data()

<span class="comment"># 4. Creación de índice vectorial con persistencia</span>
storage_context = StorageContext.from_defaults()
index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context
)
storage_context.persist(persist_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">./mi_almacenamiento</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># ----------------------------</span>
<span class="comment"># Etapa 1: Recuperación de Información</span>
<span class="comment"># ----------------------------</span>
retriever = index.as_retriever(similarity_top_k=<span class="integer">5</span>)
nodes = retriever.retrieve(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Qué es el aprendizaje automático?</span><span class="delimiter">&quot;</span></span>)
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Nodos recuperados crudos:</span><span class="delimiter">&quot;</span></span>, [node.text[:<span class="integer">50</span>] + <span class="string"><span class="delimiter">&quot;</span><span class="content">...</span><span class="delimiter">&quot;</span></span> <span class="keyword">for</span> node <span class="keyword">in</span> nodes])

<span class="comment"># ----------------------------</span>
<span class="comment"># Etapa 2: Posprocesamiento</span>
<span class="comment"># ----------------------------</span>
reranker = SentenceTransformerRerank(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">cross-encoder/ms-marco-MiniLM-L-6-v2</span><span class="delimiter">&quot;</span></span>,
    top_n=<span class="integer">3</span>
)
nodes_reranked = reranker.postprocess_nodes(nodes, query_str=<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Qué es el aprendizaje automático?</span><span class="delimiter">&quot;</span></span>)
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Nodos después de reranking:</span><span class="delimiter">&quot;</span></span>, [node.text[:<span class="integer">50</span>] + <span class="string"><span class="delimiter">&quot;</span><span class="content">...</span><span class="delimiter">&quot;</span></span> <span class="keyword">for</span> node <span class="keyword">in</span> nodes_reranked])

<span class="comment"># ----------------------------</span>
<span class="comment"># Etapa 3: Síntesis de Respuesta (RAG)</span>
<span class="comment"># ----------------------------</span>
query_engine = index.as_query_engine(
    node_postprocessors=[reranker],
    response_mode=<span class="string"><span class="delimiter">&quot;</span><span class="content">compact</span><span class="delimiter">&quot;</span></span>
)
response = query_engine.query(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Qué es el aprendizaje automático?</span><span class="delimiter">&quot;</span></span>)
print(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Respuesta generada:</span><span class="delimiter">&quot;</span></span>, response)</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_componentes_clave_de_llamaindex">5. Componentes Clave de LlamaIndex</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_componentes_prompts_modelos_bases_de_datos_motores_de_consulta_queryengine">5.1. Componentes: prompts, modelos, bases de datos, motores de consulta (QueryEngine)</h3>
<div class="paragraph">
<p>LlamaIndex se estructura alrededor de cuatro pilares fundamentales:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 25%;">
<col style="width: 75%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Componente</th>
<th class="tableblock halign-left valign-top">Función y Ejemplo</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Prompts</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Plantillas para guiar al LLM. Ejemplo con Ollama:
[source,python]
----
from llama_index.core import PromptTemplate</p>
<p class="tableblock">template = """
Contexto:
{context_str}</p>
<p class="tableblock">Responde en español usando markdown:
{query_str}
"""
prompt = PromptTemplate(template)
----</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Modelos</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Configuración de LLMs locales:
[source,python]
----
from llama_index.llms.ollama import Ollama</p>
<p class="tableblock">llm = Ollama(model="llama3.1", temperature=0.3)
----</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Bases de datos</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Almacenes vectoriales locales:
[source,python]
----
from llama_index.vector_stores.lancedb import LanceDBVectorStore</p>
<p class="tableblock">vector_store = LanceDBVectorStore(uri="./data.lancedb")
----</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>QueryEngine</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Motor de consultas RAG personalizado:
[source,python]
----
query_engine = index.as_query_engine(
    similarity_top_k=3,
    llm=llm,
    response_mode="compact"
)
----</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_herramientas_y_agentes_definición_usos_y_ejemplos">5.2. Herramientas y agentes: definición, usos y ejemplos</h3>
<div class="paragraph">
<p><strong>1. FunctionTool (Herramientas básicas):</strong></p>
</div>
<div class="paragraph">
<p>FunctionTool encapsula una función Python, permitiendo que agentes como ReActAgent la invoquen durante el procesamiento de consultas. Esto es útil para tareas específicas que requieren lógica personalizada, como cálculos matemáticos, acceso a APIs externas o manipulación de datos.</p>
</div>
<div class="listingblock">
<div class="title">Ejemplo de uso de FunctionTool para una calculadora simple:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Instala primero las dependencias (ejecutar en terminal):</span>
<span class="comment"># pip install llama-index-llms-ollama llama-index-core</span>

<span class="keyword">from</span> <span class="include">llama_index.core.tools</span> <span class="keyword">import</span> <span class="include">FunctionTool</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.core.agent</span> <span class="keyword">import</span> <span class="include">ReActAgent</span>

<span class="comment"># 1. Definir función matemática</span>
<span class="keyword">def</span> <span class="function">multiplicar</span>(a: <span class="predefined">float</span>, b: <span class="predefined">float</span>) -&gt; <span class="predefined">float</span>:
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Multiplica dos números y devuelve el resultado.</span><span class="delimiter">&quot;&quot;&quot;</span></span>
    <span class="keyword">return</span> a * b

<span class="comment"># 2. Crear herramienta funcional</span>
herramienta_multiplicar = FunctionTool.from_defaults(
    fn=multiplicar,
    name=<span class="string"><span class="delimiter">&quot;</span><span class="content">calculadora_multiplicacion</span><span class="delimiter">&quot;</span></span>,
    description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Útil para realizar multiplicaciones matemáticas</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 3. Configurar modelo local con Ollama</span>
llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Modelo instalado previamente: ollama pull llama3.1</span>
    temperature=<span class="float">0.3</span>,
    context_window=<span class="integer">4096</span>,
    request_timeout=<span class="integer">120</span>
)

<span class="comment"># 4. Crear y ejecutar agente</span>
agente = ReActAgent.from_tools(
    tools=[herramienta_multiplicar],
    llm=llm,
    verbose=<span class="predefined-constant">True</span>
)

<span class="comment"># Prueba de funcionamiento</span>
<span class="keyword">if</span> __name__ == <span class="string"><span class="delimiter">&quot;</span><span class="content">__main__</span><span class="delimiter">&quot;</span></span>:
    respuesta = agente.query(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Cuánto es 8 multiplicado por 7.5?</span><span class="delimiter">&quot;</span></span>)
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta final:</span><span class="delimiter">&quot;</span></span>, respuesta)  <span class="comment"># Debería mostrar: 60.0</span></code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>2. QueryEngineTool (Integración RAG):</strong></p>
</div>
<div class="paragraph">
<p>Un QueryEngineTool es una herramienta que envuelve un motor de consulta (query engine) y lo expone como un "tool" reutilizable dentro de flujos de trabajo de agentes o sistemas multiagente.</p>
</div>
<div class="listingblock">
<div class="title">Ejemplo de uso de QueryEngineTool para consultar documentos locales:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Instalar dependencias:</span>
<span class="comment"># pip install llama-index-core llama-index-llms-ollama llama-index-embeddings-huggingface</span>

<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">SimpleDirectoryReader</span>, <span class="include">VectorStoreIndex</span>, <span class="include">Settings</span>
<span class="keyword">from</span> <span class="include">llama_index.embeddings.huggingface</span> <span class="keyword">import</span> <span class="include">HuggingFaceEmbedding</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.core.tools</span> <span class="keyword">import</span> <span class="include">QueryEngineTool</span>, <span class="include">ToolMetadata</span>
<span class="keyword">from</span> <span class="include">llama_index.core.agent</span> <span class="keyword">import</span> <span class="include">ReActAgent</span>

<span class="comment"># 1. Configuración de modelos locales</span>
Settings.llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Descargar previamente: ollama pull llama3.2</span>
    temperature=<span class="float">0.3</span>,
    context_window=<span class="integer">4096</span>,
    request_timeout=<span class="integer">120</span>
)

Settings.embed_model = HuggingFaceEmbedding(
    model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">BAAI/bge-small-en-v1.5</span><span class="delimiter">&quot;</span></span>,
    device=<span class="string"><span class="delimiter">&quot;</span><span class="content">cpu</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 2. Cargar e indexar documentos</span>
documentos = SimpleDirectoryReader(
    input_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">data</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Carpeta con archivos .txt, .pdf, etc.</span>
    required_exts=[<span class="string"><span class="delimiter">&quot;</span><span class="content">.pdf</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">.txt</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">.md</span><span class="delimiter">&quot;</span></span>]
).load_data()

indice = VectorStoreIndex.from_documents(
    documentos,
    show_progress=<span class="predefined-constant">True</span>
)

<span class="comment"># 3. Crear QueryEngine y QueryEngineTool</span>
query_engine = indice.as_query_engine(llm=Settings.llm)
herramienta_query_engine = QueryEngineTool(
    query_engine=query_engine,
    metadata=ToolMetadata(
        name=<span class="string"><span class="delimiter">&quot;</span><span class="content">consulta_documentos</span><span class="delimiter">&quot;</span></span>,
        description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Herramienta para consultar información en documentos locales</span><span class="delimiter">&quot;</span></span>
    )
)

<span class="comment"># 4. Crear agente y consultar</span>
agente = ReActAgent.from_tools(
    tools=[herramienta_query_engine],
    llm=Settings.llm,
    verbose=<span class="predefined-constant">True</span>
)

<span class="keyword">if</span> __name__ == <span class="string"><span class="delimiter">&quot;</span><span class="content">__main__</span><span class="delimiter">&quot;</span></span>:
    pregunta = <span class="string"><span class="delimiter">&quot;</span><span class="content">Resume los puntos clave del documento sobre IA</span><span class="delimiter">&quot;</span></span>
    respuesta = agente.query(pregunta)
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta:</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, respuesta)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>3. AgentWorkflow (Sistemas multiagente):</strong></p>
</div>
<div class="paragraph">
<p>Un AgentWorkflow es un sistema de orquestación que permite crear flujos de trabajo con múltiples agentes de IA que colaboran entre sí, mantienen estado compartido y transfieren tareas según su especialización. Facilita la construcción de aplicaciones complejas donde diferentes agentes interactúan secuencial o paralelamente para resolver problemas.</p>
</div>
<div class="ulist">
<div class="title">Características clave</div>
<ul>
<li>
<p>Estado compartido: Mantiene contexto entre agentes</p>
</li>
<li>
<p>Transferencia dinámica: Los agentes pueden pasar tareas entre sí</p>
</li>
<li>
<p>Ejecución asíncrona: Soporte nativo para operaciones concurrentes</p>
</li>
<li>
<p>Observabilidad: Registro detallado de cada paso del flujo</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Instalar dependencias primero:</span>
<span class="comment"># pip install llama-index-core llama-index-llms-ollama</span>

<span class="keyword">import</span> <span class="include">asyncio</span>
<span class="keyword">from</span> <span class="include">llama_index.core.agent.workflow</span> <span class="keyword">import</span> <span class="include">AgentWorkflow</span>, <span class="include">ReActAgent</span>
<span class="keyword">from</span> <span class="include">llama_index.core.workflow</span> <span class="keyword">import</span> <span class="include">Context</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>

<span class="comment"># 1. Configurar modelo local</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>, temperature=<span class="float">0.3</span>)

<span class="comment"># 2. Definir herramientas y agentes</span>
async <span class="keyword">def</span> <span class="function">buscar_informacion</span>(ctx: Context, tema: <span class="predefined">str</span>) -&gt; <span class="predefined">str</span>:
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Busca datos sobre un tema en fuentes externas (simulado)</span><span class="delimiter">&quot;&quot;&quot;</span></span>
    estado = await ctx.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">state</span><span class="delimiter">&quot;</span></span>)
    estado[<span class="string"><span class="delimiter">&quot;</span><span class="content">busquedas</span><span class="delimiter">&quot;</span></span>] += <span class="integer">1</span>
    await ctx.set(<span class="string"><span class="delimiter">&quot;</span><span class="content">state</span><span class="delimiter">&quot;</span></span>, estado)

    <span class="comment"># Simulación de búsqueda</span>
    <span class="keyword">return</span> f<span class="string"><span class="delimiter">&quot;</span><span class="content">Datos sobre {tema}: ...</span><span class="delimiter">&quot;</span></span>

async <span class="keyword">def</span> <span class="function">analizar_datos</span>(ctx: Context, datos: <span class="predefined">str</span>) -&gt; <span class="predefined">str</span>:
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Analiza información y genera conclusiones</span><span class="delimiter">&quot;&quot;&quot;</span></span>
    estado = await ctx.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">state</span><span class="delimiter">&quot;</span></span>)
    estado[<span class="string"><span class="delimiter">&quot;</span><span class="content">analisis</span><span class="delimiter">&quot;</span></span>] += <span class="integer">1</span>
    await ctx.set(<span class="string"><span class="delimiter">&quot;</span><span class="content">state</span><span class="delimiter">&quot;</span></span>, estado)

    <span class="keyword">return</span> f<span class="string"><span class="delimiter">&quot;</span><span class="content">Análisis de '{datos[:20]}...': Conclusiones clave...</span><span class="delimiter">&quot;</span></span>

<span class="comment"># 3. Crear agentes especializados</span>
agente_buscador = ReActAgent(
    name=<span class="string"><span class="delimiter">&quot;</span><span class="content">buscador</span><span class="delimiter">&quot;</span></span>,
    description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Especialista en recopilar información</span><span class="delimiter">&quot;</span></span>,
    tools=[buscar_informacion],
    llm=llm
)

agente_analista = ReActAgent(
    name=<span class="string"><span class="delimiter">&quot;</span><span class="content">analista</span><span class="delimiter">&quot;</span></span>,
    description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Especialista en análisis de datos</span><span class="delimiter">&quot;</span></span>,
    tools=[analizar_datos],
    llm=llm
)

<span class="comment"># 4. Configurar workflow</span>
workflow = AgentWorkflow(
    agents=[agente_buscador, agente_analista],
    root_agent=<span class="string"><span class="delimiter">&quot;</span><span class="content">buscador</span><span class="delimiter">&quot;</span></span>,
    initial_state={<span class="string"><span class="delimiter">&quot;</span><span class="content">busquedas</span><span class="delimiter">&quot;</span></span>: <span class="integer">0</span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">analisis</span><span class="delimiter">&quot;</span></span>: <span class="integer">0</span>},
    state_prompt=<span class="string"><span class="delimiter">&quot;</span><span class="content">Estado actual: {state}</span><span class="char">\n</span><span class="content">Consulta: {msg}</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 5. Función de ejecución</span>
async <span class="keyword">def</span> <span class="function">main</span>():
    contexto = Context(workflow)
    respuesta = await workflow.run(
        user_msg=<span class="string"><span class="delimiter">&quot;</span><span class="content">Investigar el impacto de la IA en la medicina</span><span class="delimiter">&quot;</span></span>,
        ctx=contexto
    )

    estado_final = await contexto.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">state</span><span class="delimiter">&quot;</span></span>)
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Respuesta final:</span><span class="char">\n</span><span class="content">{respuesta}</span><span class="delimiter">&quot;</span></span>)
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Estadísticas:</span><span class="char">\n</span><span class="content">- Búsquedas: {estado_final['busquedas']}</span><span class="char">\n</span><span class="content">- Análisis: {estado_final['analisis']}</span><span class="delimiter">&quot;</span></span>)

<span class="keyword">if</span> __name__ == <span class="string"><span class="delimiter">&quot;</span><span class="content">__main__</span><span class="delimiter">&quot;</span></span>:
    asyncio.run(main())</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_workflows_en_llamaindex">5.3. Workflows en LlamaIndex</h3>
<div class="paragraph">
<p>Los <strong>Workflows</strong> en LlamaIndex son una abstracción dirigida por eventos para encadenar varios pasos y eventos en aplicaciones de IA. Permiten construir flujos de trabajo complejos, coordinando agentes, herramientas y pasos de procesamiento de manera estructurada y observable</p>
</div>
<div class="ulist">
<div class="title">Características clave</div>
<ul>
<li>
<p>Arquitectura basada en eventos: cada paso maneja ciertos tipos de eventos y puede emitir nuevos eventos.</p>
</li>
<li>
<p>Decoradores tipados: los pasos se definen con <code>@step</code>, lo que permite la validación automática de entradas y salidas.</p>
</li>
<li>
<p>Estado global compartido: mediante el objeto <code>Context</code>, los pasos pueden compartir y modificar variables globales.</p>
</li>
<li>
<p>Ejecución asíncrona: soporte nativo para operaciones concurrentes.</p>
</li>
<li>
<p>Observabilidad y visualización: generación de diagramas de flujo y registro detallado de la ejecución.</p>
</li>
<li>
<p>Flexibilidad: pueden usarse para agentes, RAG, extracción de datos, flujos interactivos, etc.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Elementos principales de un Workflow</div>
<ul>
<li>
<p><strong>Events</strong>: Clases que representan mensajes o datos que fluyen entre pasos. Ejemplos incluyen <code>StartEvent</code>, <code>StopEvent</code>, y eventos personalizados.</p>
</li>
<li>
<p><strong>Steps</strong>: Funciones decoradas con <code>@step</code> que definen la lógica de cada etapa del workflow. Pueden recibir eventos y devolver nuevos eventos.</p>
</li>
<li>
<p><strong>Workflow</strong>: Clase base que define el flujo general, maneja la ejecución y la orquestación de los pasos.</p>
</li>
<li>
<p><strong>Context</strong>: Objeto global que permite compartir estado y datos entre pasos sin necesidad de pasarlos explícitamente.</p>
</li>
<li>
<p><strong>Event Handlers</strong>: Métodos que responden a eventos específicos, permitiendo la personalización del comportamiento del workflow.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Ejemplo básico: transformación de datos</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core.workflow</span> <span class="keyword">import</span> (
    Event,
    StartEvent,
    StopEvent,
    Workflow,
    step,
)

<span class="comment"># Definimos eventos personalizados para pasar datos entre pasos</span>
<span class="keyword">class</span> <span class="class">TransformEvent</span>(Event):
    transformed_data: <span class="predefined">str</span>

<span class="keyword">class</span> <span class="class">MyWorkflow</span>(Workflow):
    <span class="decorator">@step</span>
    async <span class="keyword">def</span> <span class="function">first_step</span>(<span class="predefined-constant">self</span>, ev: StartEvent) -&gt; TransformEvent:
        <span class="comment"># Recibe el StartEvent, extrae el dato inicial (p.ej., &quot;input&quot;)</span>
        data = ev.input
        <span class="comment"># Transforma el dato</span>
        transformed = f<span class="string"><span class="delimiter">&quot;</span><span class="content">Transformed: {data.upper()}</span><span class="delimiter">&quot;</span></span>
        <span class="keyword">return</span> TransformEvent(transformed_data=transformed)

    <span class="decorator">@step</span>
    async <span class="keyword">def</span> <span class="function">second_step</span>(<span class="predefined-constant">self</span>, ev: TransformEvent) -&gt; StopEvent:
        <span class="comment"># Recibe el evento intermedio y finaliza el workflow</span>
        result = f<span class="string"><span class="delimiter">&quot;</span><span class="content">Final result: {ev.transformed_data}</span><span class="delimiter">&quot;</span></span>
        <span class="keyword">return</span> StopEvent(result=result)

<span class="comment"># Ejecutamos el workflow de forma asíncrona</span>
async <span class="keyword">def</span> <span class="function">main</span>():
    workflow = MyWorkflow(timeout=<span class="integer">60</span>, verbose=<span class="predefined-constant">False</span>)
    result = await workflow.run(input=<span class="string"><span class="delimiter">&quot;</span><span class="content">hello world</span><span class="delimiter">&quot;</span></span>)
    print(result)

<span class="keyword">if</span> __name__ == <span class="string"><span class="delimiter">&quot;</span><span class="content">__main__</span><span class="delimiter">&quot;</span></span>:
    <span class="keyword">import</span> <span class="include">asyncio</span>
    asyncio.run(main())</code></pre>
</div>
</div>
<div class="sect3">
<h4 id="_uso_de_context_para_compartir_datos_entre_steps">5.3.1. Uso de Context para compartir datos entre steps</h4>
<div class="paragraph">
<p>LlamaIndex proporciona la clase Context como un objeto global para gestionar el estado y los datos compartidos durante la ejecución de un workflow. Permite almacenar, recuperar y sincronizar información entre los distintos pasos del workflow, facilitando la cooperación entre ellos y evitando la necesidad de pasar explícitamente datos entre funciones.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 7. Propiedades y Métodos Principales de Context</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 75%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Propiedad/Método</th>
<th class="tableblock halign-left valign-top">Descripción</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">set(key: str, value: Any, make_private: bool = False)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Guarda un valor bajo una clave única en el contexto. Si make_private es True y la clave ya existe, lanza un ValueError.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">get(key: str, default: Optional[Any] = Ellipsis)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Recupera el valor asociado a una clave. Si no existe, devuelve el valor por defecto o lanza un ValueError si no hay valor accesible.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">collect_events(ev: Event, expected: List[Type[Event]], buffer_id: Optional[str] = None)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Permite recolectar y sincronizar eventos para flujos donde varios pasos deben esperar diferentes tipos de eventos.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">mark_in_progress(name: str, ev: Event)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Marca un paso como "en progreso" junto con el evento de entrada asociado.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">add_holding_event(event: Event)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Agrega un evento a la lista de eventos retenidos en la ejecución paso a paso.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">get_holding_events()</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Devuelve una copia de la lista de eventos retenidos.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">send_event(message: Event, step: Optional[str] = None)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Envía un evento a un paso específico del workflow o a todos los receptores si step es None.</p></td>
</tr>
</tbody>
</table>
<div class="listingblock">
<div class="title">Ejemplo de uso de Context en un workflow simple</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core.workflow</span> <span class="keyword">import</span> <span class="include">Workflow</span>, <span class="include">step</span>, <span class="include">Context</span>, <span class="include">Event</span>, <span class="include">StartEvent</span>, <span class="include">StopEvent</span>

<span class="comment"># Evento personalizado opcional, pero StartEvent siempre es necesario</span>
<span class="keyword">class</span> <span class="class">MyStartEvent</span>(StartEvent):
    user_input: <span class="predefined">str</span>

<span class="keyword">class</span> <span class="class">SaveToContextEvent</span>(Event):
    <span class="keyword">pass</span>

<span class="keyword">class</span> <span class="class">MyWorkflow</span>(Workflow):
    <span class="decorator">@step</span>
    async <span class="keyword">def</span> <span class="function">start</span>(<span class="predefined-constant">self</span>, ctx: Context, ev: MyStartEvent) -&gt; SaveToContextEvent:
        <span class="comment"># Guarda el input en el contexto</span>
        await ctx.set(<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>, ev.user_input)
        <span class="keyword">return</span> SaveToContextEvent()

    <span class="decorator">@step</span>
    async <span class="keyword">def</span> <span class="function">show</span>(<span class="predefined-constant">self</span>, ctx: Context, ev: SaveToContextEvent) -&gt; StopEvent:
        <span class="comment"># Recupera el input del contexto</span>
        value = await ctx.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>)
        print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Valor recuperado del contexto:</span><span class="delimiter">&quot;</span></span>, value)
        <span class="keyword">return</span> StopEvent(result=value)

<span class="comment"># Ejecución del workflow</span>
async <span class="keyword">def</span> <span class="function">main</span>():
    w = MyWorkflow()
    result = await w.run(user_input=<span class="string"><span class="delimiter">&quot;</span><span class="content">Hola LlamaIndex</span><span class="delimiter">&quot;</span></span>)  <span class="comment"># Esto crea un MyStartEvent</span>
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Resultado final:</span><span class="delimiter">&quot;</span></span>, result)

<span class="comment"># Para ejecutarlo:</span>
<span class="comment"># import asyncio</span>
<span class="comment"># asyncio.run(main())</span>

<span class="keyword">if</span> __name__ == <span class="string"><span class="delimiter">&quot;</span><span class="content">__main__</span><span class="delimiter">&quot;</span></span>:
    <span class="keyword">import</span> <span class="include">asyncio</span>
    asyncio.run(main())</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_visualización_del_flujo">5.3.2. Visualización del flujo</h4>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Asegúrate de tener instalados los paquetes necesarios:</span>
<span class="comment"># pip install llama-index-core llama-index-utils-workflow</span>

<span class="keyword">import</span> <span class="include">asyncio</span>
<span class="keyword">from</span> <span class="include">llama_index.core.workflow</span> <span class="keyword">import</span> <span class="include">Event</span>, <span class="include">StartEvent</span>, <span class="include">StopEvent</span>, <span class="include">Workflow</span>, <span class="include">step</span>
<span class="keyword">from</span> <span class="include">llama_index.utils.workflow</span> <span class="keyword">import</span> <span class="include">draw_all_possible_flows</span>, <span class="include">draw_most_recent_execution</span>

<span class="comment"># Definición de un evento personalizado para el paso intermedio</span>
<span class="keyword">class</span> <span class="class">ProcessingEvent</span>(Event):
    intermediate_result: <span class="predefined">str</span>

<span class="comment"># Definición del workflow con dos pasos</span>
<span class="keyword">class</span> <span class="class">MultiStepWorkflow</span>(Workflow):
    <span class="decorator">@step</span>
    async <span class="keyword">def</span> <span class="function">step_one</span>(<span class="predefined-constant">self</span>, ev: StartEvent) -&gt; ProcessingEvent:
        print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Ejecutando el primer paso...</span><span class="delimiter">&quot;</span></span>)
        <span class="keyword">return</span> ProcessingEvent(intermediate_result=<span class="string"><span class="delimiter">&quot;</span><span class="content">Primer paso completado</span><span class="delimiter">&quot;</span></span>)

    <span class="decorator">@step</span>
    async <span class="keyword">def</span> <span class="function">step_two</span>(<span class="predefined-constant">self</span>, ev: ProcessingEvent) -&gt; StopEvent:
        print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Ejecutando el segundo paso...</span><span class="delimiter">&quot;</span></span>)
        final_result = f<span class="string"><span class="delimiter">&quot;</span><span class="content">Procesamiento finalizado: {ev.intermediate_result}</span><span class="delimiter">&quot;</span></span>
        <span class="keyword">return</span> StopEvent(result=final_result)

<span class="comment"># Función principal para ejecutar el workflow y generar las visualizaciones</span>
async <span class="keyword">def</span> <span class="function">main</span>():
    <span class="comment"># Instancia del workflow</span>
    w = MultiStepWorkflow(timeout=<span class="integer">10</span>, verbose=<span class="predefined-constant">False</span>)

    <span class="comment"># Ejecutar el workflow</span>
    result = await w.run()
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Resultado final:</span><span class="delimiter">&quot;</span></span>, result)

    <span class="comment"># Visualizar todos los posibles flujos del workflow</span>
    draw_all_possible_flows(MultiStepWorkflow, filename=<span class="string"><span class="delimiter">&quot;</span><span class="content">multi_step_workflow.html</span><span class="delimiter">&quot;</span></span>)
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Visualización de todos los posibles flujos guardada en 'multi_step_workflow.html'</span><span class="delimiter">&quot;</span></span>)

    <span class="comment"># Visualizar la ejecución más reciente del workflow</span>
    draw_most_recent_execution(w, filename=<span class="string"><span class="delimiter">&quot;</span><span class="content">recent_execution.html</span><span class="delimiter">&quot;</span></span>)
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Visualización de la ejecución más reciente guardada en 'recent_execution.html'</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># Ejecutar el main</span>
<span class="keyword">if</span> __name__ == <span class="string"><span class="delimiter">&quot;</span><span class="content">__main__</span><span class="delimiter">&quot;</span></span>:
    asyncio.run(main())</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_casos_de_uso_comunes">5.3.3. Casos de uso comunes</h4>
<div class="ulist">
<ul>
<li>
<p>RAG avanzado: recuperación, reranking y generación.</p>
</li>
<li>
<p>Agentes colaborativos: sistemas multiagente con especialización.</p>
</li>
<li>
<p>Generación estructurada y verificación de formato.</p>
</li>
<li>
<p>Flujos interactivos con entrada humana en tiempo real.</p>
</li>
<li>
<p>Orquestación de agentes, herramientas y motores de consulta.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_ventajas_principales">5.3.4. Ventajas principales</h4>
<div class="ulist">
<ul>
<li>
<p>Modularidad y reutilización de pasos y agentes.</p>
</li>
<li>
<p>Observabilidad y trazabilidad de la ejecución.</p>
</li>
<li>
<p>Tolerancia a fallos y reintentos automáticos.</p>
</li>
<li>
<p>Escalabilidad y ejecución paralela de pasos independientes.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_ejemplo_práctico_desarrollo_de_un_asistente_de_documentación">5.4. Ejemplo práctico: desarrollo de un asistente de documentación</h3>
<div class="paragraph">
<p>QueryPipeline es un componente de LlamaIndex que permite construir flujos de consulta complejos, integrando múltiples módulos y herramientas para procesar consultas de manera eficiente. En este ejemplo, crearemos un asistente que responde preguntas sobre el dataset del Titanic utilizando un pipeline de consulta.</p>
</div>
<div class="paragraph">
<p>La tendencia es ir sustituyenndo los 'QueryPipeline' por 'Workflow' para aprovechar las ventajas de la arquitectura basada en eventos y la reutilización de pasos.</p>
</div>
<div class="paragraph">
<p><strong>Carga de documentos</strong>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Instala los paquetes necesarios (si aún no lo has hecho)</span>
<span class="comment"># %pip install llama-index llama-index-experimental llama-index-llms-ollama</span>

<span class="keyword">import</span> <span class="include">pandas</span> <span class="keyword">as</span> pd
<span class="keyword">from</span> <span class="include">llama_index.core.query_pipeline</span> <span class="keyword">import</span> <span class="include">QueryPipeline</span> <span class="keyword">as</span> QP, <span class="include">Link</span>, <span class="include">InputComponent</span>
<span class="keyword">from</span> <span class="include">llama_index.experimental.query_engine.pandas</span> <span class="keyword">import</span> <span class="include">PandasInstructionParser</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>

<span class="keyword">def</span> <span class="function">main</span>():
    <span class="comment"># Carga el CSV</span>
    df = pd.read_csv(<span class="string"><span class="delimiter">&quot;</span><span class="content">data/titanic.csv</span><span class="delimiter">&quot;</span></span>)

    <span class="comment"># Configuración de Ollama</span>
    llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>, base_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://localhost:11434</span><span class="delimiter">&quot;</span></span>)

    <span class="comment"># Define las instrucciones y prompts</span>
    instruction_str = (
        <span class="string"><span class="delimiter">&quot;</span><span class="content">1. Convierte la consulta a código Python ejecutable usando Pandas.</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">2. La última línea debe ser una expresión Python evaluable con eval().</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">3. El código debe resolver la consulta.</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">4. SOLO IMPRIMA LA EXPRESIÓN.</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">5. No ponga la expresión entre comillas.</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
    )

    pandas_prompt_str = (
        <span class="string"><span class="delimiter">&quot;</span><span class="content">Trabajas con un dataframe de pandas en Python.</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">El nombre del dataframe es `df`.</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">Esto es el resultado de `print(df.head())`:</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">{df_str}</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">Sigue estas instrucciones:</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">{instruction_str}</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">Consulta: {query_str}</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">Expresión:</span><span class="delimiter">&quot;</span></span>
    )

    response_synthesis_prompt_str = (
        <span class="string"><span class="delimiter">&quot;</span><span class="content">Dada una pregunta de entrada, sintetiza una respuesta a partir de los resultados de la consulta.</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">Consulta: {query_str}</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">Instrucciones de Pandas (opcional):</span><span class="char">\n</span><span class="content">{pandas_instructions}</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">Salida de Pandas: {pandas_output}</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
        <span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta: </span><span class="delimiter">&quot;</span></span>
    )

    <span class="comment"># Crea los prompts</span>
    pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(
        instruction_str=instruction_str, df_str=df.head(<span class="integer">5</span>)
    )
    pandas_output_parser = PandasInstructionParser(df)
    response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)

    <span class="comment"># Construye el pipeline de consulta</span>
    qp = QP(
        modules={
            <span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>: InputComponent(),
            <span class="string"><span class="delimiter">&quot;</span><span class="content">pandas_prompt</span><span class="delimiter">&quot;</span></span>: pandas_prompt,
            <span class="string"><span class="delimiter">&quot;</span><span class="content">llm1</span><span class="delimiter">&quot;</span></span>: llm,
            <span class="string"><span class="delimiter">&quot;</span><span class="content">pandas_output_parser</span><span class="delimiter">&quot;</span></span>: pandas_output_parser,
            <span class="string"><span class="delimiter">&quot;</span><span class="content">response_synthesis_prompt</span><span class="delimiter">&quot;</span></span>: response_synthesis_prompt,
            <span class="string"><span class="delimiter">&quot;</span><span class="content">llm2</span><span class="delimiter">&quot;</span></span>: llm,
        },
        verbose=<span class="predefined-constant">True</span>,
    )
    qp.add_chain([<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">pandas_prompt</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">llm1</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">pandas_output_parser</span><span class="delimiter">&quot;</span></span>])
    qp.add_links(
        [
            Link(<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">response_synthesis_prompt</span><span class="delimiter">&quot;</span></span>, dest_key=<span class="string"><span class="delimiter">&quot;</span><span class="content">query_str</span><span class="delimiter">&quot;</span></span>),
            Link(<span class="string"><span class="delimiter">&quot;</span><span class="content">llm1</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">response_synthesis_prompt</span><span class="delimiter">&quot;</span></span>, dest_key=<span class="string"><span class="delimiter">&quot;</span><span class="content">pandas_instructions</span><span class="delimiter">&quot;</span></span>),
            Link(<span class="string"><span class="delimiter">&quot;</span><span class="content">pandas_output_parser</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">response_synthesis_prompt</span><span class="delimiter">&quot;</span></span>, dest_key=<span class="string"><span class="delimiter">&quot;</span><span class="content">pandas_output</span><span class="delimiter">&quot;</span></span>),
        ]
    )
    qp.add_link(<span class="string"><span class="delimiter">&quot;</span><span class="content">response_synthesis_prompt</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">llm2</span><span class="delimiter">&quot;</span></span>)

    <span class="comment"># Ejemplo de consulta</span>
    response = qp.run(
        query_str=<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Cuál es la edad media de los pasajeros supervivientes?</span><span class="delimiter">&quot;</span></span>
    )

    print(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Respuesta final:</span><span class="delimiter">&quot;</span></span>)
    print(response.message.content)

<span class="keyword">if</span> __name__ == <span class="string"><span class="delimiter">&quot;</span><span class="content">__main__</span><span class="delimiter">&quot;</span></span>:
    main()</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_funcionalidades_avanzadas">6. Funcionalidades Avanzadas</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_uso_de_almacenes_vectoriales_para_búsquedas_semánticas_eficientes">6.1. Uso de almacenes vectoriales para búsquedas semánticas eficientes</h3>
<div class="paragraph">
<p>LlamaIndex soporta 20+ almacenes vectoriales para producción y desarrollo local:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 8. Comparativa de almacenes populares:</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 40%;">
<col style="width: 40%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Almacén</th>
<th class="tableblock halign-left valign-top">Ventajas</th>
<th class="tableblock halign-left valign-top">Caso de uso</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">FAISS</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Optimizado para CPU, local</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Desarrollo rápido</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">LanceDB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Open-source, versionado</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Datos multimodales</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Qdrant</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Escalable en Kubernetes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Entornos productivos</p></td>
</tr>
</tbody>
</table>
<div class="listingblock">
<div class="title">Implementación con FAISS y Ollama:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">VectorStoreIndex</span>
<span class="keyword">from</span> <span class="include">llama_index.vector_stores.faiss</span> <span class="keyword">import</span> <span class="include">FaissVectorStore</span>
<span class="keyword">from</span> <span class="include">llama_index.embeddings.ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbedding</span>

embed_model = OllamaEmbedding(model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>)
vector_store = FaissVectorStore(faiss_index=faiss.IndexFlatL2(<span class="integer">768</span>))
index = VectorStoreIndex.from_documents(
    documents,
    embed_model=embed_model,
    vector_store=vector_store
)

<span class="comment"># Búsqueda híbrida vector + metadatos</span>
retriever = index.as_retriever(
    vector_store_query_mode=<span class="string"><span class="delimiter">&quot;</span><span class="content">hybrid</span><span class="delimiter">&quot;</span></span>,
    filters=MetadataFilters(filters=[
        ExactMatchFilter(key=<span class="string"><span class="delimiter">&quot;</span><span class="content">departamento</span><span class="delimiter">&quot;</span></span>, value=<span class="string"><span class="delimiter">&quot;</span><span class="content">finanzas</span><span class="delimiter">&quot;</span></span>)
    ])
)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_implementación_de_agentes_inteligentes_react_openai_function_agents">6.2. Implementación de agentes inteligentes (ReAct, OpenAI Function Agents)</h3>
<div class="paragraph">
<p>Arquitecturas agentivas avanzadas usando Ollamad:</p>
</div>
<div class="listingblock">
<div class="title">Agente ReAct con herramientas múltiples:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core.agent</span> <span class="keyword">import</span> <span class="include">ReActAgent</span>
<span class="keyword">from</span> <span class="include">llama_index.tools.database</span> <span class="keyword">import</span> <span class="include">DatabaseToolSpec</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>

<span class="comment"># Herramienta 1: Acceso a base de datos</span>
db_tool = DatabaseToolSpec(uri=<span class="string"><span class="delimiter">&quot;</span><span class="content">sqlite:///datos.db</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># Herramienta 2: RAG interno</span>
rag_tool = QueryEngineTool.from_defaults(
    query_engine=index.as_query_engine()
)

agente = ReActAgent.from_tools(
    tools=[db_tool.to_tool_list()[<span class="integer">0</span>], rag_tool],
    llm=Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>),
    verbose=<span class="predefined-constant">True</span>
)

respuesta = agente.chat(<span class="string"><span class="delimiter">&quot;</span><span class="content">Total ventas 2023 y política de devoluciones</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Flujo de ejecución del agente:</div>
<ol class="arabic">
<li>
<p>Pensamiento: "Necesito consultar ventas en DB y políticas en documentos"</p>
</li>
<li>
<p>Acción 1: Ejecutar consulta SQL <code>SELECT SUM(monto) FROM ventas&#8230;&#8203;</code></p>
</li>
<li>
<p>Acción 2: Buscar "política de devoluciones" en índice RAG</p>
</li>
<li>
<p>Síntesis: Combinar resultados en respuesta natural</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_personalización_y_extensión_con_plugins_y_herramientas_externas">6.3. Personalización y extensión con plugins y herramientas externas</h3>
<div class="paragraph">
<p>Mecanismos de extensión avanzados:</p>
</div>
<div class="listingblock">
<div class="title">Plugin personalizado para GitHub:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">BaseReader</span>
<span class="keyword">from</span> <span class="include">github</span> <span class="keyword">import</span> <span class="include">Github</span>

<span class="keyword">class</span> <span class="class">GitHubRepoReader</span>(BaseReader):
    <span class="keyword">def</span> <span class="function">__init__</span>(<span class="predefined-constant">self</span>, access_token):
        <span class="predefined-constant">self</span>.g = Github(access_token)

    <span class="keyword">def</span> <span class="function">load_data</span>(<span class="predefined-constant">self</span>, repo_name):
        repo = <span class="predefined-constant">self</span>.g.get_repo(repo_name)
        contents = []
        <span class="keyword">for</span> <span class="predefined">file</span> <span class="keyword">in</span> repo.get_contents(<span class="string"><span class="delimiter">&quot;</span><span class="delimiter">&quot;</span></span>):
            <span class="keyword">if</span> <span class="predefined">file</span>.type == <span class="string"><span class="delimiter">&quot;</span><span class="content">file</span><span class="delimiter">&quot;</span></span>:
                contents.append(<span class="predefined">file</span>.decoded_content.decode())
        <span class="keyword">return</span> [Document(text=<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>.join(contents))]

<span class="comment"># Uso</span>
reader = GitHubRepoReader(<span class="string"><span class="delimiter">&quot;</span><span class="content">ghp_...</span><span class="delimiter">&quot;</span></span>)
docs = reader.load_data(<span class="string"><span class="delimiter">&quot;</span><span class="content">usuario/repo</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Pipeline personalizado con transformaciones:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">IngestionPipeline</span>
<span class="keyword">from</span> <span class="include">llama_index.core.node_parser</span> <span class="keyword">import</span> <span class="include">CodeSplitter</span>

pipeline = IngestionPipeline(
    transformations=[
        CodeSplitter(language=<span class="string"><span class="delimiter">&quot;</span><span class="content">python</span><span class="delimiter">&quot;</span></span>),
        OllamaEmbedding(model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>),
        MetadataExtractor(fields=[<span class="string"><span class="delimiter">&quot;</span><span class="content">lenguaje</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">clases</span><span class="delimiter">&quot;</span></span>])
    ]
)
nodes = pipeline.run(documents)</code></pre>
</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 9. Integración con herramientas externas:</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 66.6667%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Herramienta</th>
<th class="tableblock halign-left valign-top">Caso de uso</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Apache Airflow</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Orchestrar pipelines ETL</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">MLflow</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Tracking de experimentos</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Grafana</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Monitorización de queries</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_persistencia_y_gestión_de_datos">7. Persistencia y Gestión de Datos</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_métodos_de_almacenamiento_y_recuperación_de_índices">7.1. Métodos de almacenamiento y recuperación de índices</h3>
<div class="paragraph">
<p>LlamaIndex ofrece múltiples estrategias para almacenar y recuperar índices:</p>
</div>
<div class="paragraph">
<p><strong>1. Persistencia local básica:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">StorageContext</span>

<span class="comment"># Guardar índice en disco</span>
index.storage_context.persist(persist_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">mi_indice</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># Recuperar</span>
storage_context = StorageContext.from_defaults(persist_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">mi_indice</span><span class="delimiter">&quot;</span></span>)
index_recuperado = load_index_from_storage(storage_context)  <span class="comment">#</span></code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>2. Almacenamiento en la nube (AWS S3/R2):</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">s3fs</span>
s3 = s3fs.S3FileSystem(
    key=<span class="string"><span class="delimiter">&quot;</span><span class="content">AWS_ACCESS_KEY</span><span class="delimiter">&quot;</span></span>,
    secret=<span class="string"><span class="delimiter">&quot;</span><span class="content">AWS_SECRET_KEY</span><span class="delimiter">&quot;</span></span>,
    endpoint_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">https://tu_endpoint.cloud</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># Persistir en bucket S3</span>
index.storage_context.persist(
    persist_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">s3://bucket/indices/</span><span class="delimiter">&quot;</span></span>,
    fs=s3
)

<span class="comment"># Cargar desde S3</span>
storage_context = StorageContext.from_defaults(
    persist_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">s3://bucket/indices/</span><span class="delimiter">&quot;</span></span>,
    fs=s3
)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>3. Bases de datos especializadas:</strong></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 66.6667%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Sistema</th>
<th class="tableblock halign-left valign-top">Implementación</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">MongoDB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>MongoDocumentStore</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">PostgreSQL</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>PGVectorStore</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Redis</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>RedisVectorStore</code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_persistencia_de_datos_indexados_y_recarga_de_contextos">7.2. Persistencia de datos indexados y recarga de contextos</h3>
<div class="paragraph">
<p>Flujo completo para gestión de datos a largo plazo:</p>
</div>
<div class="paragraph">
<p><strong>1. Guardado con metadatos:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Configurar contexto de almacenamiento personalizado</span>
storage_context = StorageContext.from_defaults(
    docstore=SimpleDocumentStore(),
    vector_store=FaissVectorStore(),
    index_store=SimpleIndexStore()
)

<span class="comment"># Persistir todo el contexto</span>
storage_context.persist(persist_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">storage_full</span><span class="delimiter">&quot;</span></span>)  <span class="comment">#</span></code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>2. Carga incremental de documentos:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Cargar índice existente</span>
index_base = load_index_from_storage(storage_context)

<span class="comment"># Añadir nuevos documentos</span>
nuevos_docs = SimpleDirectoryReader(<span class="string"><span class="delimiter">&quot;</span><span class="content">nuevos_datos</span><span class="delimiter">&quot;</span></span>).load_data()
index_base.insert(Document.from_docs(nuevos_docs))  <span class="comment">#</span>

<span class="comment"># Actualizar persistencia</span>
index_base.storage_context.persist(persist_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">storage_full</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>3. Recarga con configuraciones personalizadas:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.embeddings.ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbedding</span>

<span class="comment"># Reconstruir con mismas configuraciones originales</span>
storage_context = StorageContext.from_defaults(
    persist_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">storage_full</span><span class="delimiter">&quot;</span></span>,
    embed_model=OllamaEmbedding(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>)  <span class="comment">#</span>
)

index = load_index_from_storage(
    storage_context,
    index_id=<span class="string"><span class="delimiter">&quot;</span><span class="content">mi_indice_principal</span><span class="delimiter">&quot;</span></span>  <span class="comment"># Requerido si hay múltiples índices</span>
)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Mejores prácticas:</strong>
- Usar <code>index.set_index_id()</code> para gestión multi-índice
- Implementar hashing de documentos para detectar cambios
- Combinar <code>persist()</code> con versionado manual para rollbacks</p>
</div>
<div class="paragraph">
<p><strong>Ejemplo de flujo completo:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># 1. Creación inicial</span>
index = VectorStoreIndex.from_documents(docs)
index.storage_context.persist(<span class="string"><span class="delimiter">&quot;</span><span class="content">indice_v1</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 2. Actualización mensual</span>
nuevos_docs = cargar_docs_nuevos()
index.insert(nuevos_docs)
index.storage_context.persist(<span class="string"><span class="delimiter">&quot;</span><span class="content">indice_v2</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 3. Recuperación de versión específica</span>
storage_context = StorageContext.from_defaults(
    persist_dir=<span class="string"><span class="delimiter">&quot;</span><span class="content">indice_v1</span><span class="delimiter">&quot;</span></span>,
    embed_model=OllamaEmbedding(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>)
)
index_anterior = load_index_from_storage(storage_context)</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_ingeniería_de_prompts_y_estrategias_de_consulta">8. Ingeniería de Prompts y Estrategias de Consulta</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_técnicas_cadena_de_pensamiento_few_shot_prompting_react">8.1. Técnicas: cadena de pensamiento, few-shot prompting, ReAct</h3>
<div class="paragraph">
<p><strong>Cadena de Pensamiento (Chain-of-Thought - CoT):</strong>
Técnica que guía al LLM a mostrar su proceso de razonamiento paso a paso:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">prompt = <span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">Calcula el IVA de un producto de 200€ con tasa del 21%.</span><span class="content">
</span><span class="content">Piensa paso a paso y muestra los cálculos intermedios.</span><span class="content">
</span><span class="delimiter">&quot;&quot;&quot;</span></span>
respuesta = query_engine.query(prompt)  <span class="comment"># Output: 200 * 0.21 = 42€</span></code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Few-Shot Prompting:</strong>
Proporciona ejemplos para enseñar el formato y lógica esperadosd:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">ejemplos = <span class="string"><span class="delimiter">'''</span><span class="content">
</span><span class="content">Pregunta: &quot;Clasifica: 'Odio este servicio'&quot;</span><span class="content">
</span><span class="content">Respuesta: Negativo</span><span class="content">
</span><span class="content">
</span><span class="content">Pregunta: &quot;Clasifica: 'Increíble atención al cliente'&quot;</span><span class="content">
</span><span class="content">Respuesta: Positivo</span><span class="content">
</span><span class="content">
</span><span class="content">Pregunta: &quot;Clasifica: 'El producto es regular'&quot;</span><span class="content">
</span><span class="content">Respuesta:</span><span class="delimiter">'''</span></span>
respuesta = llm.complete(ejemplos + <span class="string"><span class="delimiter">&quot;</span><span class="content"> Neutral</span><span class="delimiter">&quot;</span></span>)d</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>ReAct (Reasoning + Action):</strong>
Combina razonamiento lógico con ejecución de acciones:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">prompt_react = <span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">Pensamiento: Necesito comparar población de Madrid y Barcelona</span><span class="content">
</span><span class="content">Acción: Buscar población actual de Madrid</span><span class="content">
</span><span class="content">Observación: 3.3 millones</span><span class="content">
</span><span class="content">Acción: Buscar población actual de Barcelona</span><span class="content">
</span><span class="content">Observación: 1.6 millones</span><span class="content">
</span><span class="content">Respuesta: Madrid tiene mayor población</span><span class="content">
</span><span class="delimiter">&quot;&quot;&quot;</span></span></code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_mejores_prácticas_para_optimizar_la_interacción_con_el_llm">8.2. Mejores prácticas para optimizar la interacción con el LLM</h3>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 25%;">
<col style="width: 75%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Práctica</th>
<th class="tableblock halign-left valign-top">Implementación</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Claridad contextual</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Especificar rol y formato:
<code>Eres un experto en finanzas. Responde en JSON con {monto, iva, total}</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Gestión de tokens</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Usar <code>SentenceSplitter(chunk_size=512)</code> para documentos largos</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Validación estructurada</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Forzar salidas con Pydantic:
<code>response_model=IVAResponse</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Optimización iterativa</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pruebas A/B con diferentes temperaturas (0.1-0.7)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Control de sesgos</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Neutralizar lenguaje:
<code>Evita suposiciones sobre género o cultura</code></p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><strong>Ejemplo avanzado de pipeline:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>

template = PromptTemplate(<span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">Contexto: {context}</span><span class="content">
</span><span class="content">Instrucciones:</span><span class="content">
</span><span class="content">1. Identifica los conceptos clave</span><span class="content">
</span><span class="content">2. Explica en máximo 3 pasos</span><span class="content">
</span><span class="content">3. Usa analogías cotidianas</span><span class="content">
</span><span class="delimiter">&quot;&quot;&quot;</span></span>)

response = index.as_query_engine(
    text_qa_template=template
).query(<span class="string"><span class="delimiter">&quot;</span><span class="content">Explica la teoría de la relatividad</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Recomendaciones clave:</strong>
- Priorizar modelos instruction-tuned (ej: Llama3.1-instruct)
- Usar <code>temperature=0.3</code> para tareas técnicas
- Implementar <code>MetadataFilter</code> para precisión en RAG
- Versionar prompts con Git para control de cambios</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_implementación_escalado_y_optimización">9. Implementación, Escalado y Optimización</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_despliegue_de_aplicaciones_basadas_en_llamaindex">9.1. Despliegue de aplicaciones basadas en LlamaIndex</h3>
<div class="paragraph">
<p>Estrategias clave para entornos productivos:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 25%;">
<col style="width: 75%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Plataforma</th>
<th class="tableblock halign-left valign-top">Configuración</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Serverless (AWS Lambda)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">[source,python]
----
# Ejemplo AWS Lambda Handler
import os
from llama_index.core import StorageContext</p>
<p class="tableblock">def handler(event, context):
    storage = StorageContext.from_defaults(
        persist_dir="s3://bucket/indices/",
        fs=s3fs.S3FileSystem()
    )
    index = load_index_from_storage(storage)
    return index.as_query_engine().query(event["query"]).response
----</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Contenedores (Docker)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">[source,Dockerfile]
----
FROM nvidia/cuda:12.2.0-base
RUN pip install llama-index[gpu] faiss-gpu
COPY app.py .
CMD ["gunicorn", "-w 4", "-b :8080", "app:server"]
----</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Kubernetes</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
name: llamaindex-hpa
spec:
scaleTargetRef:
apiVersion: apps/v1
kind: Deployment
name: llamaindex
minReplicas: 3
maxReplicas: 20
metrics:
    type: Resource
    resource:
    name: cpu
    target:
    type: Utilization
    averageUtilization: 70
----</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_escalado_para_alto_rendimiento_y_grandes_volúmenes_de_datos">9.2. Escalado para alto rendimiento y grandes volúmenes de datos</h3>
<div class="paragraph">
<p>Técnicas avanzadas para datasets &gt;1TB:</p>
</div>
<div class="paragraph">
<p><strong>Arquitectura multi-nivel:</strong></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 66.6667%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Capa</th>
<th class="tableblock halign-left valign-top">Tecnología</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Almacenamiento</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">MinIO Cluster + Parquet</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Indexación</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Milvus/Pinecone con sharding</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Procesamiento</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Apache Spark + GPU Nodes</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><strong>Optimización de índices jerárquicos:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">VectorStoreIndex</span>
<span class="keyword">from</span> <span class="include">llama_index.core.indices.multi_modal</span> <span class="keyword">import</span> <span class="include">MultiModalVectorStoreIndex</span>

<span class="comment"># Configuración para 10M+ documentos</span>
index = MultiModalVectorStoreIndex.from_documents(
    documents,
    vector_store=QdrantVectorStore(
        url=<span class="string"><span class="delimiter">&quot;</span><span class="content">http://qdrant-cluster:6333</span><span class="delimiter">&quot;</span></span>,
        collection_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">enterprise_data</span><span class="delimiter">&quot;</span></span>,
        shard_number=<span class="integer">8</span>
    ),
    batch_size=<span class="integer">1000</span>,
    show_progress=<span class="predefined-constant">True</span>
)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Parámetros clave:</strong>
- <code>chunk_size=768</code> (óptimo para modelos españoles)d
- <code>similarity_top_k=5</code> (balance precisión-rendimiento)d
- <code>max_retries=5</code> con backoff exponencial</p>
</div>
</div>
<div class="sect2">
<h3 id="_monitorización_y_optimización_de_aplicaciones">9.3. Monitorización y optimización de aplicaciones</h3>
<div class="paragraph">
<p>Métricas esenciales y herramientas:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 25%;">
<col style="width: 75%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">KPI</th>
<th class="tableblock halign-left valign-top">Herramienta</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Latencia P95</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Datadog APM</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Precisión RAG</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TruLens + Ragas</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Uso Memoria</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Prometheus + Grafana</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Throughput</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">AWS CloudWatch</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><strong>Implementación de observabilidad:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">set_global_handler</span>
<span class="keyword">import</span> <span class="include">langfuse</span>

<span class="comment"># Configurar monitorización en tiempo real</span>
set_global_handler(
    <span class="string"><span class="delimiter">&quot;</span><span class="content">langfuse</span><span class="delimiter">&quot;</span></span>,
    public_key=<span class="string"><span class="delimiter">&quot;</span><span class="content">pk-lf-...</span><span class="delimiter">&quot;</span></span>,
    secret_key=<span class="string"><span class="delimiter">&quot;</span><span class="content">sk-lf-...</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># Ejemplo traza personalizada</span>
<span class="keyword">with</span> langfuse.trace(name=<span class="string"><span class="delimiter">&quot;</span><span class="content">consulta_compleja</span><span class="delimiter">&quot;</span></span>):
    response = agent.chat(<span class="string"><span class="delimiter">&quot;</span><span class="content">Analizar tendencias Q3 2025</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="olist arabic">
<div class="title"><strong>Checklist de optimización:</strong></div>
<ol class="arabic">
<li>
<p>Re-indexar datos cada 6h con jobs programados</p>
</li>
<li>
<p>Usar <code>BAAI/bge-large-es-v1.5</code> para embeddings en españold</p>
</li>
<li>
<p>Implementar caché L2 con Redis Cluster</p>
</li>
<li>
<p>Balancear carga entre 3+ instancias Ollama</p>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_futuro_de_llamaindex_y_próximos_pasos">10. Futuro de LlamaIndex y Próximos Pasos</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_tendencias_en_orquestación_de_datos_y_llms">10.1. Tendencias en orquestación de datos y LLMs</h3>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 10. El ecosistema evoluciona hacia arquitecturas multiagente y sistemas autónomos:</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 75%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Tendencia 2025-2030</th>
<th class="tableblock halign-left valign-top">Impacto en LlamaIndex</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>IA Agentiva</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Sistemas que planifican/ejecutan flujos complejos sin intervención humana</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Orquestación Humano-AI</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Colaboración en tiempo real usando <a href="#Digital Twins">[Digital Twins]</a> y <a href="#Cobots">[Cobots]</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Modelos Especializados</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Fine-tuning de LLMs para dominios específicos (legal, médico, financiero)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Gobernanza Automatizada</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Sistemas auto-auditables con trazabilidad completa</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Computación Neuro-Simbólica</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Combinación de redes neuronales y lógica formal para RAG preciso</p></td>
</tr>
</tbody>
</table>
<div class="listingblock">
<div class="title">Ejemplo de sistema multiagente:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">llama_index.core.agent</span> <span class="keyword">import</span> <span class="include">MultiAgentCollaboration</span>

agente_analista = FinancialAnalystAgent(llm=Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>))
agente_visual = DataVizAgent(llm=Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>))
agente_auditor = ComplianceAgent(llm=Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.1</span><span class="delimiter">&quot;</span></span>))

workflow = MultiAgentCollaboration(
    agents=[agente_analista, agente_visual, agente_auditor],
    orchestration_strategy=<span class="string"><span class="delimiter">&quot;</span><span class="content">hierarchical</span><span class="delimiter">&quot;</span></span>
)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_actualizaciones_y_roadmap_de_llamaindex">10.2. Actualizaciones y roadmap de LlamaIndex</h3>
<div class="ulist">
<div class="title">Próximos hitos tecnológicos (Q3 2025 - Q1 2026):</div>
<ul>
<li>
<p><strong>LlamaCloud EU</strong>: Implementación regional con compliance GDPR para empresas europeasd</p>
</li>
<li>
<p><strong>LlamaParse 2.0</strong>: Soporte nativo para 50+ formatos complejos (CAD, BIM, SEC filings)</p>
</li>
<li>
<p><strong>Auto-RAG Framework</strong>: Configuración automática de parámetros de indexación/consulta</p>
</li>
<li>
<p><strong>Quantum-Ready Indexing</strong>: Algoritmos preparados para hardware cuántico (colaboración IBM)</p>
</li>
<li>
<p><strong>Ethical AI Toolkit</strong>: Módulos para detección de sesgos y explicabilidad de respuestas</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Roadmap técnico 2025:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="text">2025-Q3: Lanzamiento LlamaIndex 3.0 con API estable y soporte LTS
2025-Q4: Integración nativa con modelos cuánticos (IBM Qiskit)
2026-Q1: Motor de ejecución WASM para edge computing</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_recursos_adicionales_y_comunidad">10.3. Recursos adicionales y comunidad</h3>
<div class="paragraph">
<p>Ecosistema para desarrolladores y empresasd:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>LlamaHub</strong>: 250+ conectores certificados (SAP, Salesforce, SWIFT)</p>
</li>
<li>
<p><strong>Formación Certificada</strong>: Programas de entrenamiento oficiales (Developer/Architect tracks)</p>
</li>
<li>
<p><strong>Comunidad Activa</strong>: 300K+ miembros en Discord, 15K+ proyectos en GitHubd</p>
</li>
<li>
<p><strong>Eventos Globales</strong>: LlamaCon 2025 (Madrid, CDMX, Singapore)</p>
</li>
<li>
<p><strong>Plantillas Empresariales</strong>: Soluciones preconfiguradas para:</p>
<div class="ulist">
<ul>
<li>
<p>Due Diligence Automatizado</p>
</li>
<li>
<p>Generación de Informes Regulatorios</p>
</li>
<li>
<p>Monitoreo de Riesgos en Tiempo Real</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Enlace rápido a recursos:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash"># Documentación oficial
https://docs.llamaindex.ai

# Acceso a LlamaCloud
https://cloud.llamaindex.ai

# Contribuir al código
https://github.com/run-llama/llama_index</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2025-06-09 01:47:38 +0200
</div>
</div>
</body>
</html>