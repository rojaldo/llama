<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.23">
<title>LangChain</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<style>
/*! Stylesheet for CodeRay to loosely match GitHub themes | MIT License */
pre.CodeRay{background:#f7f7f8}
.CodeRay .line-numbers{border-right:1px solid;opacity:.35;padding:0 .5em 0 0;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
.CodeRay span.line-numbers{display:inline-block;margin-right:.75em}
.CodeRay .line-numbers strong{color:#000}
table.CodeRay{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.CodeRay td{vertical-align:top;line-height:inherit}
table.CodeRay td.line-numbers{text-align:right}
table.CodeRay td.code{padding:0 0 0 .75em}
.CodeRay .debug{color:#fff!important;background:navy!important}
.CodeRay .annotation{color:#007}
.CodeRay .attribute-name{color:navy}
.CodeRay .attribute-value{color:#700}
.CodeRay .binary{color:#509}
.CodeRay .comment{color:#998;font-style:italic}
.CodeRay .char{color:#04d}
.CodeRay .char .content{color:#04d}
.CodeRay .char .delimiter{color:#039}
.CodeRay .class{color:#458;font-weight:bold}
.CodeRay .complex{color:#a08}
.CodeRay .constant,.CodeRay .predefined-constant{color:teal}
.CodeRay .color{color:#099}
.CodeRay .class-variable{color:#369}
.CodeRay .decorator{color:#b0b}
.CodeRay .definition{color:#099}
.CodeRay .delimiter{color:#000}
.CodeRay .doc{color:#970}
.CodeRay .doctype{color:#34b}
.CodeRay .doc-string{color:#d42}
.CodeRay .escape{color:#666}
.CodeRay .entity{color:#800}
.CodeRay .error{color:#808}
.CodeRay .exception{color:inherit}
.CodeRay .filename{color:#099}
.CodeRay .function{color:#900;font-weight:bold}
.CodeRay .global-variable{color:teal}
.CodeRay .hex{color:#058}
.CodeRay .integer,.CodeRay .float{color:#099}
.CodeRay .include{color:#555}
.CodeRay .inline{color:#000}
.CodeRay .inline .inline{background:#ccc}
.CodeRay .inline .inline .inline{background:#bbb}
.CodeRay .inline .inline-delimiter{color:#d14}
.CodeRay .inline-delimiter{color:#d14}
.CodeRay .important{color:#555;font-weight:bold}
.CodeRay .interpreted{color:#b2b}
.CodeRay .instance-variable{color:teal}
.CodeRay .label{color:#970}
.CodeRay .local-variable{color:#963}
.CodeRay .octal{color:#40e}
.CodeRay .predefined{color:#369}
.CodeRay .preprocessor{color:#579}
.CodeRay .pseudo-class{color:#555}
.CodeRay .directive{font-weight:bold}
.CodeRay .type{font-weight:bold}
.CodeRay .predefined-type{color:inherit}
.CodeRay .reserved,.CodeRay .keyword{color:#000;font-weight:bold}
.CodeRay .key{color:#808}
.CodeRay .key .delimiter{color:#606}
.CodeRay .key .char{color:#80f}
.CodeRay .value{color:#088}
.CodeRay .regexp .delimiter{color:#808}
.CodeRay .regexp .content{color:#808}
.CodeRay .regexp .modifier{color:#808}
.CodeRay .regexp .char{color:#d14}
.CodeRay .regexp .function{color:#404;font-weight:bold}
.CodeRay .string{color:#d20}
.CodeRay .string .string .string{background:#ffd0d0}
.CodeRay .string .content{color:#d14}
.CodeRay .string .char{color:#d14}
.CodeRay .string .delimiter{color:#d14}
.CodeRay .shell{color:#d14}
.CodeRay .shell .delimiter{color:#d14}
.CodeRay .symbol{color:#990073}
.CodeRay .symbol .content{color:#a60}
.CodeRay .symbol .delimiter{color:#630}
.CodeRay .tag{color:teal}
.CodeRay .tag-special{color:#d70}
.CodeRay .variable{color:#036}
.CodeRay .insert{background:#afa}
.CodeRay .delete{background:#faa}
.CodeRay .change{color:#aaf;background:#007}
.CodeRay .head{color:#f8f;background:#505}
.CodeRay .insert .insert{color:#080}
.CodeRay .delete .delete{color:#800}
.CodeRay .change .change{color:#66f}
.CodeRay .head .head{color:#f4f}
</style>
</head>
<body class="article">
<div id="header">
<h1>LangChain</h1>
<div id="toc" class="toc">
<div id="toctitle">Índice de contenidos</div>
<ul class="sectlevel1">
<li><a href="#_introducción_a_langchain_y_llms">1. Introducción a LangChain y LLMs</a>
<ul class="sectlevel2">
<li><a href="#_qué_es_langchain_y_cuáles_son_sus_componentes_principales">1.1. ¿Qué es LangChain y cuáles son sus componentes principales?</a></li>
<li><a href="#_casos_de_uso_de_langchain_en_la_industria">1.2. Casos de uso de LangChain en la industria</a></li>
<li><a href="#_instalación_de_python_y_la_librería_langchain">1.3. Instalación de Python y la librería LangChain</a></li>
<li><a href="#_integración_de_langchain_con_modelos_locales_usando_ollama">1.4. Integración de LangChain con modelos locales usando Ollama</a>
<ul class="sectlevel3">
<li><a href="#_configuración_básica">1.4.1. Configuración básica</a></li>
<li><a href="#_integración_con_langchain">1.4.2. Integración con LangChain</a></li>
<li><a href="#_caso_de_uso_avanzado_rag_local">1.4.3. Caso de uso avanzado: RAG local</a></li>
<li><a href="#_optimización_y_mejores_prácticas">1.4.4. Optimización y mejores prácticas</a></li>
<li><a href="#_solución_de_problemas_comunes">1.4.5. Solución de problemas comunes</a></li>
</ul>
</li>
<li><a href="#_primeros_pasos_entorno_de_desarrollo_y_recursos_recomendados_en_langchain">1.5. Primeros pasos: entorno de desarrollo y recursos recomendados en LangChain</a>
<ul class="sectlevel3">
<li><a href="#_configuración_del_entorno_de_desarrollo">1.5.1. Configuración del entorno de desarrollo</a></li>
<li><a href="#_ejemplo_básico_de_uso">1.5.2. Ejemplo básico de uso</a></li>
<li><a href="#_recursos_recomendados">1.5.3. Recursos recomendados</a></li>
</ul>
</li>
<li><a href="#_estructura_y_filosofía_de_langchain">1.6. Estructura y filosofía de LangChain</a></li>
<li><a href="#_cómo_usar_ollama_en_local">1.7. Cómo usar Ollama en local</a></li>
</ul>
</li>
<li><a href="#_fundamentos_de_langchain">2. Fundamentos de LangChain</a>
<ul class="sectlevel2">
<li><a href="#_estructura_y_filosofía_de_langchain_2">2.1. Estructura y filosofía de LangChain</a>
<ul class="sectlevel3">
<li><a href="#_filosofía_central">2.1.1. Filosofía central</a></li>
<li><a href="#_arquitectura_y_componentes_principales">2.1.2. Arquitectura y componentes principales</a></li>
<li><a href="#_ventajas_y_propósito">2.1.3. Ventajas y propósito</a></li>
<li><a href="#_resumen_gráfico_de_la_arquitectura">2.1.4. Resumen gráfico de la arquitectura</a></li>
</ul>
</li>
<li><a href="#_componentes_principales_de_langchain">2.2. Componentes principales de LangChain</a>
<ul class="sectlevel3">
<li><a href="#_tools">2.2.1. Tools</a></li>
<li><a href="#_tool_calling">2.2.2. Tool Calling</a></li>
<li><a href="#_structured_output">2.2.3. Structured Output</a></li>
</ul>
</li>
<li><a href="#_memory">2.3. Memory</a>
<ul class="sectlevel3">
<li><a href="#_document_loader">2.3.1. Document Loader</a></li>
<li><a href="#_retrieval">2.3.2. Retrieval</a></li>
<li><a href="#_text_splitters">2.3.3. Text Splitters</a></li>
<li><a href="#_embedding_models_en_langchain">2.3.4. Embedding Models en LangChain</a></li>
<li><a href="#_vector_stores_en_langchain_markdown">2.3.5. Vector Stores en LangChain (Markdown)</a></li>
<li><a href="#_retriever_en_langchain">2.3.6. Retriever en LangChain</a></li>
<li><a href="#_output_parser_en_langchain">2.3.7. Output Parser en LangChain</a></li>
<li><a href="#_few_shot_prompting">2.3.8. Few-shot Prompting</a></li>
<li><a href="#_example_selectors_en_langchain">2.3.9. Example Selectors en LangChain</a></li>
<li><a href="#_evaluation_en_langchain">2.3.10. Evaluation en LangChain</a></li>
</ul>
</li>
<li><a href="#_flujo_de_trabajo_típico_en_langchain">2.4. Flujo de trabajo típico en LangChain</a>
<ul class="sectlevel3">
<li><a href="#_entrada_del_usuario">2.4.1. Entrada del usuario</a></li>
<li><a href="#_orquestación_por_agentes_y_cadenas">2.4.2. Orquestación por agentes y cadenas</a></li>
<li><a href="#_uso_de_herramientas_y_conectores">2.4.3. Uso de herramientas y conectores</a></li>
<li><a href="#_procesamiento_por_el_modelo_de_lenguaje">2.4.4. Procesamiento por el modelo de lenguaje</a></li>
<li><a href="#_gestión_de_memoria_y_contexto">2.4.5. Gestión de memoria y contexto</a></li>
<li><a href="#_iteración_y_refinamiento">2.4.6. Iteración y refinamiento</a></li>
<li><a href="#_respuesta_al_usuario">2.4.7. Respuesta al usuario</a></li>
</ul>
</li>
<li><a href="#_introducción_al_encadenamiento_y_ejemplos_básicos">2.5. Introducción al encadenamiento y ejemplos básicos</a>
<ul class="sectlevel3">
<li><a href="#_qué_es_el_encadenamiento_en_langchain">2.5.1. ¿Qué es el encadenamiento en LangChain?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_modelos_y_prompts">3. Modelos y Prompts</a>
<ul class="sectlevel2">
<li><a href="#_interacción_con_llms_y_modelos_de_chat">3.1. Interacción con LLMs y modelos de chat</a>
<ul class="sectlevel3">
<li><a href="#_qué_es_un_llm_y_cómo_interactúa">3.1.1. ¿Qué es un LLM y cómo interactúa?</a></li>
<li><a href="#_modelos_de_chat_estructura_y_ventajas">3.1.2. Modelos de chat: estructura y ventajas</a></li>
<li><a href="#_personalización_y_ajuste_fine_tuning">3.1.3. Personalización y ajuste (fine-tuning)</a></li>
</ul>
</li>
<li><a href="#_plantillas_de_prompts_y_técnicas_de_prompt_engineering">3.2. Plantillas de prompts y técnicas de prompt engineering</a>
<ul class="sectlevel3">
<li><a href="#_qué_es_una_plantilla_de_prompt_en_langchain">3.2.1. ¿Qué es una plantilla de prompt en LangChain?</a></li>
</ul>
</li>
<li><a href="#_procesamiento_y_parseo_de_la_salida_del_modelo_en_langchain">3.3. Procesamiento y parseo de la salida del modelo en LangChain</a>
<ul class="sectlevel3">
<li><a href="#_por_qué_es_necesario_el_parseo_de_salidas">3.3.1. ¿Por qué es necesario el parseo de salidas?</a></li>
<li><a href="#_tipos_principales_de_parsers">3.3.2. Tipos principales de parsers</a></li>
</ul>
</li>
<li><a href="#_serialización_y_reutilización_de_prompts_en_langchain">3.4. Serialización y reutilización de prompts en LangChain</a>
<ul class="sectlevel3">
<li><a href="#_opciones_y_formatos_soportados">3.4.1. Opciones y formatos soportados</a></li>
<li><a href="#_reutilización_y_composición_de_prompts">3.4.2. Reutilización y composición de prompts</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_conectores_y_fuentes_de_datos">4. Conectores y Fuentes de Datos</a>
<ul class="sectlevel2">
<li><a href="#_cargadores_de_documentos_en_langchain_pdf_txt_web_y_apis_externas">4.1. Cargadores de documentos en LangChain: PDF, TXT, web y APIs externas</a>
<ul class="sectlevel3">
<li><a href="#_txt_archivos_de_texto_plano">4.1.1. TXT: Archivos de texto plano</a></li>
<li><a href="#_pdf_documentos_pdf">4.1.2. PDF: Documentos PDF</a></li>
<li><a href="#_web_páginas_y_sitios_web">4.1.3. Web: Páginas y sitios web</a></li>
<li><a href="#_apis_externas_y_cargadores_personalizados">4.1.4. APIs externas y cargadores personalizados</a></li>
</ul>
</li>
<li><a href="#_integraciones_con_plataformas_google_drive_wikipedia_etc_en_langchain">4.2. Integraciones con plataformas (Google Drive, Wikipedia, etc.) en LangChain</a>
<ul class="sectlevel3">
<li><a href="#_google_drive">4.2.1. Google Drive</a></li>
<li><a href="#_wikipedia">4.2.2. Wikipedia</a></li>
<li><a href="#_otras_integraciones_y_fuentes_externas">4.2.3. Otras integraciones y fuentes externas</a></li>
</ul>
</li>
<li><a href="#_transformación_y_preprocesamiento_de_documentos_en_langchain">4.3. Transformación y preprocesamiento de documentos en LangChain</a>
<ul class="sectlevel3">
<li><a href="#_1_limpieza_y_normalización_de_texto">4.3.1. 1. Limpieza y normalización de texto</a></li>
<li><a href="#_2_división_de_texto_chunking_y_preservación_de_contexto">4.3.2. 2. División de texto (chunking) y preservación de contexto</a></li>
<li><a href="#_3_enriquecimiento_y_uso_de_metadatos">4.3.3. 3. Enriquecimiento y uso de metadatos</a></li>
<li><a href="#_4_tokenización_lematización_y_filtrado_de_stopwords">4.3.4. 4. Tokenización, lematización y filtrado de stopwords</a></li>
<li><a href="#_5_preprocesamiento_específico_con_clases_y_transformadores_en_langchain">4.3.5. 5. Preprocesamiento específico con clases y transformadores en LangChain</a></li>
<li><a href="#_6_preprocesamiento_de_preguntas_de_usuario">4.3.6. 6. Preprocesamiento de preguntas de usuario</a></li>
</ul>
</li>
<li><a href="#_ejemplo_completo_de_flujo_de_procesamiento_de_documentos_en_langchain">4.4. Ejemplo completo de flujo de procesamiento de documentos en LangChain</a></li>
<li><a href="#_embeddings_incrustación_de_texto_y_creación_de_vectores_en_langchain">4.5. Embeddings: incrustación de texto y creación de vectores en LangChain</a>
<ul class="sectlevel3">
<li><a href="#_conceptos_clave">4.5.1. Conceptos clave</a></li>
<li><a href="#_componentes_principales">4.5.2. Componentes principales</a></li>
<li><a href="#_implementación_básica">4.5.3. Implementación básica</a></li>
<li><a href="#_proceso_completo_de_creación_de_vectores">4.5.4. Proceso completo de creación de vectores</a></li>
<li><a href="#_buenas_prácticas">4.5.5. Buenas prácticas</a></li>
<li><a href="#_ejemplo_avanzado_con_embeddings_personalizados">4.5.6. Ejemplo avanzado con embeddings personalizados</a></li>
</ul>
</li>
<li><a href="#_almacenamiento_y_búsqueda_en_bases_de_datos_vectoriales_con_langchain">4.6. Almacenamiento y búsqueda en bases de datos vectoriales con LangChain</a>
<ul class="sectlevel3">
<li><a href="#_principales_opciones_y_características_comparadas">4.6.1. Principales opciones y características comparadas</a></li>
<li><a href="#_configuración_básica_con_langchain">4.6.2. Configuración básica con LangChain</a></li>
<li><a href="#_búsquedas_avanzadas">4.6.3. Búsquedas avanzadas</a></li>
<li><a href="#_rendimiento_y_mejores_prácticas">4.6.4. Rendimiento y mejores prácticas</a></li>
<li><a href="#_casos_de_uso_recomendados">4.6.5. Casos de uso recomendados</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_agents_en_langchain">5. Agents en LangChain</a>
<ul class="sectlevel2">
<li><a href="#_introducción_a_los_agentes_definición_tipos_y_casos_de_uso">5.1. Introducción a los agentes: definición, tipos y casos de uso</a>
<ul class="sectlevel3">
<li><a href="#_qué_es_un_agente_en_langchain">5.1.1. ¿Qué es un agente en LangChain?</a></li>
<li><a href="#_componentes_principales_de_un_agente">5.1.2. Componentes principales de un agente</a></li>
<li><a href="#_tipos_de_agentes_en_langchain">5.1.3. Tipos de agentes en LangChain</a></li>
<li><a href="#_casos_de_uso_destacados">5.1.4. Casos de uso destacados</a></li>
</ul>
</li>
<li><a href="#_implementación_de_agentes_autónomos_en_langchain">5.2. Implementación de agentes autónomos en LangChain</a>
<ul class="sectlevel3">
<li><a href="#_componentes_clave">5.2.1. Componentes clave</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_aplicaciones_avanzadas">6. Aplicaciones Avanzadas</a>
<ul class="sectlevel2">
<li><a href="#_ejemplo_avanzado_de_workflow_en_llamaindex_con_integración_de_modelos_y_almacenamiento_vectorial">6.1. Ejemplo Avanzado de Workflow en LlamaIndex con Integración de Modelos y Almacenamiento Vectorial</a>
<ul class="sectlevel3">
<li><a href="#_1_requisitos_previos">6.1.1. 1. Requisitos Previos</a></li>
<li><a href="#_2_configuración_inicial">6.1.2. 2. Configuración Inicial</a></li>
<li><a href="#_3_código_completo_del_workflow_de_langchain_usando_los_datos_de_env">6.1.3. 3. Código Completo del Workflow de Langchain usando los  datos de .env</a></li>
</ul>
</li>
<li><a href="#_chatbots_personalizados_y_asistentes_virtuales_con_langchain">6.2. Chatbots personalizados y asistentes virtuales con LangChain</a>
<ul class="sectlevel3">
<li><a href="#_por_qué_usar_langchain_para_chatbots_y_asistentes_virtuales">6.2.1. ¿Por qué usar LangChain para chatbots y asistentes virtuales?</a></li>
<li><a href="#_características_principales">6.2.2. Características principales</a></li>
<li><a href="#_ejemplo_básico_de_implementación_en_python">6.2.3. Ejemplo básico de implementación en Python</a></li>
<li><a href="#_ejemplo_avanzado_chatbot_multimodal_y_personalizado">6.2.4. Ejemplo avanzado: chatbot multimodal y personalizado</a></li>
<li><a href="#_casos_de_uso_reales">6.2.5. Casos de uso reales</a></li>
<li><a href="#_mejores_prácticas">6.2.6. Mejores prácticas</a></li>
</ul>
</li>
<li><a href="#_resumen_y_análisis_de_grandes_volúmenes_de_texto">6.3. Resumen y análisis de grandes volúmenes de texto</a>
<ul class="sectlevel3">
<li><a href="#_introducción">6.3.1. Introducción</a></li>
<li><a href="#_técnicas_clave_para_el_procesamiento_de_textos_extensos">6.3.2. Técnicas clave para el procesamiento de textos extensos</a></li>
<li><a href="#_integración_con_sistemas_ia_avanzados">6.3.3. Integración con sistemas IA avanzados</a></li>
</ul>
</li>
<li><a href="#_integración_con_frontends_gradio_y_apis_rest">6.4. Integración con frontends: Gradio y APIs REST</a>
<ul class="sectlevel3">
<li><a href="#_integración_con_gradio_para_interfaces_de_chat">6.4.1. Integración con Gradio para interfaces de chat</a></li>
<li><a href="#_integración_con_apis_rest">6.4.2. Integración con APIs REST</a></li>
<li><a href="#_caso_de_uso_avanzado_sistema_multimodal">6.4.3. Caso de uso avanzado: Sistema multimodal</a></li>
<li><a href="#_recursos_y_mejores_prácticas">6.4.4. Recursos y mejores prácticas</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_buenas_prácticas_y_despliegue">7. Buenas Prácticas y Despliegue</a>
<ul class="sectlevel2">
<li><a href="#_seguridad_y_gestión_de_claves_api">7.1. Seguridad y gestión de claves API</a></li>
<li><a href="#_optimización_de_costes_y_rendimiento">7.2. Optimización de costes y rendimiento</a></li>
<li><a href="#_control_de_versiones_y_pruebas">7.3. Control de versiones y pruebas</a></li>
<li><a href="#_despliegue_de_aplicaciones_langchain_en_producción">7.4. Despliegue de aplicaciones LangChain en producción</a></li>
</ul>
</li>
<li><a href="#_recursos_y_comunidad">8. Recursos y Comunidad</a>
<ul class="sectlevel2">
<li><a href="#_documentación_oficial_y_recursos_de_aprendizaje_de_langchain">8.1. Documentación oficial y recursos de aprendizaje de LangChain</a>
<ul class="sectlevel3">
<li><a href="#_documentación_oficial">8.1.1. Documentación oficial</a></li>
<li><a href="#_recursos_clave_en_español">8.1.2. Recursos clave en español</a></li>
<li><a href="#_cursos_recomendados">8.1.3. Cursos recomendados</a></li>
<li><a href="#_comunidad_y_recursos_adicionales">8.1.4. Comunidad y recursos adicionales</a></li>
<li><a href="#_herramientas_para_desarrollo_avanzado">8.1.5. Herramientas para desarrollo avanzado</a></li>
<li><a href="#_consejos_para_el_aprendizaje">8.1.6. Consejos para el aprendizaje</a></li>
</ul>
</li>
<li><a href="#_repositorios_y_ejemplos_prácticos_de_langchain">8.2. Repositorios y ejemplos prácticos de LangChain</a>
<ul class="sectlevel3">
<li><a href="#_repositorios_destacados">8.2.1. Repositorios destacados</a></li>
<li><a href="#_ejemplos_prácticos_y_tutoriales">8.2.2. Ejemplos prácticos y tutoriales</a></li>
<li><a href="#_ejemplo_básico_en_python">8.2.3. Ejemplo básico en Python:</a></li>
<li><a href="#_casos_de_uso_frecuentes">8.2.4. Casos de uso frecuentes</a></li>
<li><a href="#_cómo_empezar_con_los_ejemplos">8.2.5. Cómo empezar con los ejemplos</a></li>
</ul>
</li>
<li><a href="#_comunidad_y_foros_de_soporte_de_langchain">8.3. Comunidad y foros de soporte de LangChain</a>
<ul class="sectlevel3">
<li><a href="#_espacios_oficiales_de_la_comunidad">8.3.1. Espacios oficiales de la comunidad</a></li>
<li><a href="#_modalidades_de_participación">8.3.2. Modalidades de participación</a></li>
<li><a href="#_buenas_prácticas_para_aprovechar_la_comunidad">8.3.3. Buenas prácticas para aprovechar la comunidad</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="_introducción_a_langchain_y_llms">1. Introducción a LangChain y LLMs</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_qué_es_langchain_y_cuáles_son_sus_componentes_principales">1.1. ¿Qué es LangChain y cuáles son sus componentes principales?</h3>
<div class="paragraph">
<p>LangChain es un marco de trabajo de código abierto diseñado para facilitar la creación de aplicaciones basadas en modelos de lenguaje de gran tamaño (LLM), como chatbots, asistentes virtuales, sistemas de preguntas y respuestas o automatización inteligente. Su objetivo es proporcionar herramientas y abstracciones que permitan a los desarrolladores integrar LLMs con flujos de trabajo, fuentes de datos externas y herramientas, mejorando la personalización, precisión y relevancia de las respuestas generadas.</p>
</div>
<div class="paragraph">
<p><strong>Componentes principales de LangChain</strong></p>
</div>
<div class="paragraph">
<p>LangChain está construido sobre una arquitectura modular, donde cada componente aborda una necesidad específica en el desarrollo de aplicaciones de IA generativa:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Models (Modelos):</strong> Permiten interactuar con diferentes LLMs, tanto comerciales (como GPT-3/4 de OpenAI) como de código abierto (BERT, Llama, etc.), a través de una interfaz unificada.</p>
</li>
<li>
<p><strong>Prompts (Plantillas de peticiones):</strong> Definen la estructura de las instrucciones que se envían al modelo. Incluyen plantillas reutilizables, ejemplos (few-shot learning) y combinaciones de prompts para obtener respuestas más precisas.</p>
</li>
<li>
<p><strong>Chains (Cadenas):</strong> Secuencias de pasos estructurados donde la salida de un componente es la entrada del siguiente. Permiten orquestar flujos de trabajo complejos, como consultar una base de datos y luego redactar una respuesta con el LLM.</p>
</li>
<li>
<p><strong>Agents (Agentes):</strong> Programas inteligentes capaces de decidir dinámicamente qué acción realizar, eligiendo entre varias herramientas o cadenas para resolver tareas complejas, como buscar información o realizar cálculos.</p>
</li>
<li>
<p><strong>Memory (Memoria):</strong> Permite almacenar y recuperar información del historial de la conversación, lo cual es esencial para mantener el contexto en interacciones prolongadas, como en chatbots conversacionales.</p>
</li>
<li>
<p><strong>Tools (Herramientas):</strong> Funciones o servicios externos que los agentes pueden invocar, como búsquedas web, APIs, calculadoras, etc.</p>
</li>
<li>
<p><strong>Retrievers (Recuperadores):</strong> Componentes que extraen información relevante desde fuentes vectoriales o bases de datos, facilitando la integración de datos externos y la búsqueda semántica.</p>
</li>
<li>
<p><strong>Conectores de datos:</strong> Permiten integrar la aplicación con diversas fuentes de datos (APIs, bases de datos, almacenamiento en la nube), asegurando un flujo de datos fluido.</p>
</li>
<li>
<p><strong>pipes de procesamiento:</strong> Gestionan flujos de trabajo para tareas como limpieza, transformación y preparación de datos.</p>
</li>
<li>
<p><strong>Módulos de despliegue y supervisión:</strong> Automatizan el despliegue y monitorización de las aplicaciones para facilitar su escalado y mantenimiento en producción.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>En resumen, LangChain proporciona un conjunto de bloques de construcción modulares que permiten a los desarrolladores crear aplicaciones LLM avanzadas, integrando modelos, flujos de trabajo, memoria, herramientas externas y datos, todo de manera flexible y escalable.</p>
</div>
</div>
<div class="sect2">
<h3 id="_casos_de_uso_de_langchain_en_la_industria">1.2. Casos de uso de LangChain en la industria</h3>
<div class="paragraph">
<p>LangChain se utiliza en múltiples industrias para crear aplicaciones potentes que integran modelos de lenguaje con datos y flujos de trabajo empresariales, mejorando la eficiencia y la experiencia del usuario.</p>
</div>
<div class="paragraph">
<p><strong>Ejemplos destacados:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Retail:</strong> LangChain permite desarrollar asistentes virtuales que gestionan hasta el 80% de las consultas de clientes, mejoran las recomendaciones de productos y automatizan la gestión de inventarios, incrementando las ventas y la satisfacción del cliente.</p>
</li>
<li>
<p><strong>Salud:</strong> Se emplea para simplificar la programación de citas, automatizar el análisis de historiales médicos y asistir en diagnósticos, reduciendo cargas administrativas hasta en un 40% y mejorando la atención al paciente.</p>
</li>
<li>
<p><strong>Finanzas:</strong> Facilita la detección de fraudes en tiempo real, ofrece asesoría financiera personalizada mediante chatbots y optimiza los procesos de cumplimiento normativo, aumentando la seguridad y eficiencia operativa.</p>
</li>
<li>
<p><strong>Educación:</strong> Permite personalizar la experiencia de aprendizaje, automatizar tareas administrativas y ofrecer traducción en tiempo real, adaptándose a las necesidades individuales de los estudiantes.</p>
</li>
<li>
<p><strong>Atención al cliente:</strong> Creación de chatbots y sistemas automáticos para gestionar consultas y tickets, mejorando tiempos de respuesta y satisfacción.</p>
</li>
<li>
<p><strong>Generación de contenidos:</strong> Automatiza la producción de textos para blogs, artículos y marketing, ahorrando tiempo y recursos.</p>
</li>
<li>
<p><strong>Análisis de sentimientos:</strong> Analiza grandes volúmenes de texto para comprender la opinión pública y apoyar decisiones empresariales.</p>
</li>
<li>
<p><strong>Resumen y traducción de documentos:</strong> Extrae información clave y traduce textos manteniendo contexto y precisión.</p>
</li>
<li>
<p><strong>Sistemas de recomendación y búsqueda:</strong> Personaliza sugerencias y mejora la relevancia en motores de búsqueda mediante consultas en lenguaje natural.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Empresas como Rakuten y Morningstar han implementado LangChain para acelerar el desarrollo de plataformas internas y chatbots avanzados que integran datos estructurados y no estructurados, demostrando la rapidez y eficacia del marco para crear soluciones inteligentes a gran escala.</p>
</div>
<div class="paragraph">
<p>En resumen, LangChain impulsa la transformación digital en sectores diversos al facilitar la integración de modelos de lenguaje con datos empresariales, optimizando procesos, mejorando la interacción con usuarios y aumentando la productividad.</p>
</div>
</div>
<div class="sect2">
<h3 id="_instalación_de_python_y_la_librería_langchain">1.3. Instalación de Python y la librería LangChain</h3>
<div class="paragraph">
<p>Para comenzar a trabajar con LangChain es necesario tener Python instalado (versión 3.7 o superior). A continuación se detallan los pasos recomendados para la instalación:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Verifica si tienes Python instalado ejecutando en la terminal:</p>
<div class="listingblock">
<div class="content">
<pre>python --version</pre>
</div>
</div>
<div class="paragraph">
<p>Si no lo tienes, descárgalo desde la web oficial de Python e instálalo siguiendo las instrucciones para tu sistema operativo.</p>
</div>
</li>
<li>
<p>(Opcional, pero recomendado) Crea un entorno virtual para aislar las dependencias del proyecto:</p>
<div class="listingblock">
<div class="content">
<pre>python -m venv langchain_env</pre>
</div>
</div>
<div class="paragraph">
<p>Activa el entorno virtual:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>En Windows:</p>
<div class="listingblock">
<div class="content">
<pre>langchain_env\Scripts\activate</pre>
</div>
</div>
</li>
<li>
<p>En macOS/Linux:</p>
<div class="listingblock">
<div class="content">
<pre>source langchain_env/bin/activate</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Asegúrate de tener pip instalado (el gestor de paquetes de Python). Verifica con:</p>
<div class="listingblock">
<div class="content">
<pre>pip --version</pre>
</div>
</div>
<div class="paragraph">
<p>Si no está instalado, sigue la guía oficial para instalar pip.</p>
</div>
</li>
<li>
<p>Instala la librería LangChain ejecutando:</p>
<div class="listingblock">
<div class="content">
<pre>pip install langchain</pre>
</div>
</div>
</li>
<li>
<p>Si necesitas integraciones específicas (por ejemplo, con OpenAI o herramientas de la comunidad), instala los paquetes adicionales:</p>
<div class="listingblock">
<div class="content">
<pre>pip install langchain-openai
pip install langchain-community</pre>
</div>
</div>
</li>
<li>
<p>(Opcional) Instala otras dependencias según los modelos o servicios que vayas a utilizar, como:</p>
<div class="listingblock">
<div class="content">
<pre>pip install openai
pip install google-search-results</pre>
</div>
</div>
</li>
<li>
<p>Verifica que la instalación fue exitosa importando LangChain en Python:</p>
<div class="listingblock">
<div class="content">
<pre>python -c "import langchain; print(langchain.__version__)"</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Si el comando anterior muestra la versión instalada sin errores, LangChain está listo para usarse en tu entorno. Para comenzar a desarrollar, consulta la documentación oficial y explora ejemplos básicos de uso.</p>
</div>
</div>
<div class="sect2">
<h3 id="_integración_de_langchain_con_modelos_locales_usando_ollama">1.4. Integración de LangChain con modelos locales usando Ollama</h3>
<div class="sect3">
<h4 id="_configuración_básica">1.4.1. Configuración básica</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Instalar Ollama según tu sistema operativo:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash"># Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows/macOS: Descargar instalador desde https://ollama.com</code></pre>
</div>
</div>
</li>
<li>
<p>Descargar un modelo LLM local (ej: Llama 3.2):</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">ollama pull llama3.2</code></pre>
</div>
</div>
</li>
<li>
<p>Iniciar el servicio Ollama en segundo plano:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">ollama serve</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_integración_con_langchain">1.4.2. Integración con LangChain</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Instalar dependencias necesarias:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">pip install langchain-ollama python-dotenv</code></pre>
</div>
</div>
</li>
<li>
<p>Código básico de integración:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain_core.prompts</span> <span class="keyword">import</span> <span class="include">ChatPromptTemplate</span>

<span class="comment"># Inicializar modelo local</span>
llm = OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># Crear cadena de procesamiento</span>
prompt = ChatPromptTemplate.from_template(<span class="string"><span class="delimiter">&quot;</span><span class="content">Responde sobre {tema}:</span><span class="delimiter">&quot;</span></span>)
chain = prompt | llm

<span class="comment"># Ejecutar la cadena</span>
response = chain.invoke({<span class="string"><span class="delimiter">&quot;</span><span class="content">tema</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">el futuro de la IA</span><span class="delimiter">&quot;</span></span>})
print(response)</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_caso_de_uso_avanzado_rag_local">1.4.3. Caso de uso avanzado: RAG local</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Estructura para Retrieval Augmented Generation:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.vectorstores</span> <span class="keyword">import</span> <span class="include">FAISS</span>
<span class="keyword">from</span> <span class="include">langchain_community.embeddings</span> <span class="keyword">import</span> <span class="include">OllamaEmbeddings</span>

<span class="comment"># 1. Crear embeddings locales</span>
embeddings = OllamaEmbeddings(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">mxbai-embed-large</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 2. Cargar documentos y crear vector store</span>
documents = [<span class="string"><span class="delimiter">&quot;</span><span class="content">Texto documento 1</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">Texto documento 2</span><span class="delimiter">&quot;</span></span>]
vector_store = FAISS.from_texts(documents, embeddings)

<span class="comment"># 3. Configurar cadena RAG</span>
retriever = vector_store.as_retriever()
prompt_template = <span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Responde usando este contexto:</span><span class="content">
</span><span class="content">{context}</span><span class="content">
</span><span class="content">
</span><span class="content">Pregunta: {question}</span><span class="delimiter">&quot;&quot;&quot;</span></span>

chain = (
    {<span class="string"><span class="delimiter">&quot;</span><span class="content">context</span><span class="delimiter">&quot;</span></span>: retriever, <span class="string"><span class="delimiter">&quot;</span><span class="content">question</span><span class="delimiter">&quot;</span></span>: RunnablePassthrough()}
    | ChatPromptTemplate.from_template(prompt_template)
    | OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
)</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_optimización_y_mejores_prácticas">1.4.4. Optimización y mejores prácticas</h4>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. <strong>Modelos recomendados:</strong></caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Modelo</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Tamaño</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Uso de RAM</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Caso de uso</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Llama3.2:8b</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">4.7GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">8GB+</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Chat general</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Phi-4:14b</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">9.1GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">16GB+</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">RAG complejo</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CodeLlama:7b</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">3.8GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">6GB+</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Generación de código</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><strong>Comandos útiles:</strong></p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash"># Listar modelos disponibles
ollama list

# Eliminar modelo
ollama rm &lt;nombre_modelo&gt;

# Actualizar Ollama
brew upgrade ollama  # macOS
sudo apt upgrade ollama  # Linux</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_solución_de_problemas_comunes">1.4.5. Solución de problemas comunes</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Error "Model not found":</strong></p>
<div class="ulist">
<ul>
<li>
<p>Verificar modelo descargado con <code>ollama list</code></p>
</li>
<li>
<p>Ejecutar <code>ollama pull &lt;nombre_modelo&gt;</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Lentitud en respuestas:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Reducir parámetros del modelo: <code>llm = OllamaLLM(model="llama3.2", num_ctx=512)</code></p>
</li>
<li>
<p>Usar quantización: <code>ollama pull llama3.2:8b-q4_0</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Integración con Docker:</strong></p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 ollama/ollama</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre>Esta configuración permite desarrollar aplicaciones AI locales con total privacidad, combinando la flexibilidad de LangChain con el rendimiento de Ollama.</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_primeros_pasos_entorno_de_desarrollo_y_recursos_recomendados_en_langchain">1.5. Primeros pasos: entorno de desarrollo y recursos recomendados en LangChain</h3>
<div class="sect3">
<h4 id="_configuración_del_entorno_de_desarrollo">1.5.1. Configuración del entorno de desarrollo</h4>
<div class="ulist">
<ul>
<li>
<p>Instala Python 3.7 o superior.</p>
</li>
<li>
<p>Crea un entorno virtual:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">python -m venv langchain-env
source langchain-env/bin/activate   # Windows: langchain-env\Scripts\activate</code></pre>
</div>
</div>
</li>
<li>
<p>Instala LangChain y dependencias:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">pip install langchain faiss-cpu langchain_community openai python-dotenv</code></pre>
</div>
</div>
</li>
<li>
<p>Crea un archivo <code>.env</code> y añade tu clave API:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code>OPENAI_API_KEY=tu_clave_api_aquí</code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_ejemplo_básico_de_uso">1.5.2. Ejemplo básico de uso</h4>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">os</span>
<span class="keyword">from</span> <span class="include">dotenv</span> <span class="keyword">import</span> <span class="include">load_dotenv</span>
<span class="keyword">from</span> <span class="include">langchain.llms</span> <span class="keyword">import</span> <span class="include">OpenAI</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">LLMChain</span>

load_dotenv()
llm = OpenAI(temperature=<span class="float">0.7</span>)
prompt = PromptTemplate(
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">tema</span><span class="delimiter">&quot;</span></span>],
    template=<span class="string"><span class="delimiter">&quot;</span><span class="content">Escribe un párrafo corto sobre {tema}.</span><span class="delimiter">&quot;</span></span>
)
cadena = LLMChain(llm=llm, prompt=prompt)
resultado = cadena.run(<span class="string"><span class="delimiter">&quot;</span><span class="content">inteligencia artificial</span><span class="delimiter">&quot;</span></span>)
print(resultado)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_recursos_recomendados">1.5.3. Recursos recomendados</h4>
<div class="ulist">
<ul>
<li>
<p>Documentación oficial: <a href="https://python.langchain.com/docs/" class="bare">https://python.langchain.com/docs/</a></p>
</li>
<li>
<p>Curso DeepLearning.ai: <a href="https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/" class="bare">https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/</a></p>
</li>
<li>
<p>Comunidad GitHub: <a href="https://github.com/langchain-ai/langchain" class="bare">https://github.com/langchain-ai/langchain</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_estructura_y_filosofía_de_langchain">1.6. Estructura y filosofía de LangChain</h3>
<div class="ulist">
<ul>
<li>
<p>Filosofía: modularidad, reutilización y escalabilidad para construir aplicaciones con LLMs.</p>
</li>
<li>
<p>Componentes principales:</p>
<div class="ulist">
<ul>
<li>
<p>Enlaces (Links): unidades básicas de procesamiento.</p>
</li>
<li>
<p>Cadenas (Chains): secuencias de enlaces para flujos de trabajo.</p>
</li>
<li>
<p>Agentes: deciden dinámicamente qué acciones o herramientas usar.</p>
</li>
<li>
<p>Herramientas (Tools): funcionalidades externas (APIs, bases de datos, etc.).</p>
</li>
<li>
<p>Memoria: almacena contexto e historial.</p>
</li>
<li>
<p>Integración de datos: conecta fuentes externas y soporta RAG.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Ventajas:</p>
<div class="ulist">
<ul>
<li>
<p>Componentes intercambiables y reutilizables.</p>
</li>
<li>
<p>Integración sencilla de modelos y datos.</p>
</li>
<li>
<p>Soporte para agentes autónomos y flujos complejos.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_cómo_usar_ollama_en_local">1.7. Cómo usar Ollama en local</h3>
<div class="ulist">
<ul>
<li>
<p>Requisitos: CPU 64 bits, 8-16 GB RAM, Windows/macOS/Linux, espacio libre en disco.</p>
</li>
<li>
<p>Instalación:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash"># Linux
curl -fsSL https://ollama.com/install.sh | sh

# macOS/Windows: descarga el instalador desde la web oficial y ejecútalo.
ollama --version</code></pre>
</div>
</div>
</li>
<li>
<p>Descargar y ejecutar modelo:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">ollama pull llama3.2
ollama run llama3.2</code></pre>
</div>
</div>
</li>
<li>
<p>Comandos útiles:</p>
<div class="ulist">
<ul>
<li>
<p><code>ollama list</code> (listar modelos)</p>
</li>
<li>
<p><code>ollama rm &lt;modelo&gt;</code> (eliminar modelo)</p>
</li>
<li>
<p><code>/clear</code> (limpiar contexto en chat)</p>
</li>
<li>
<p><code>/bye</code> (salir)</p>
</li>
</ul>
</div>
</li>
<li>
<p>Uso avanzado:</p>
<div class="ulist">
<ul>
<li>
<p>Docker: <code>docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama</code></p>
</li>
<li>
<p>Integración con Python mediante API local.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Consejos:</p>
<div class="ulist">
<ul>
<li>
<p>Ideal para trabajar offline y mantener privacida.</p>
</li>
<li>
<p>Prueba modelos ligeros si tienes poca RAM.</p>
</li>
<li>
<p>Consulta la documentación y comunidad para soporte y ejemplos.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>Este bloque cubre: configuración y recursos de LangChain, su estructura y filosofía, y el uso de Ollama en local, todo en formato asciidoctor.</pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_fundamentos_de_langchain">2. Fundamentos de LangChain</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_estructura_y_filosofía_de_langchain_2">2.1. Estructura y filosofía de LangChain</h3>
<div class="sect3">
<h4 id="_filosofía_central">2.1.1. Filosofía central</h4>
<div class="paragraph">
<p>LangChain es un framework de código abierto diseñado para facilitar la creación de aplicaciones avanzadas basadas en modelos de lenguaje de gran tamaño (LLM), como chatbots, asistentes virtuales y sistemas de consulta inteligente. Su filosofía se basa en la modularidad, la flexibilidad y la integración, permitiendo a los desarrolladores construir soluciones personalizadas, escalables y adaptadas a contextos específicos.</p>
</div>
</div>
<div class="sect3">
<h4 id="_arquitectura_y_componentes_principales">2.1.2. Arquitectura y componentes principales</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Modularidad por bloques:</strong> LangChain organiza su arquitectura en bloques o módulos independientes, cada uno encargado de una función específica. Estos bloques pueden combinarse y encadenarse según la lógica de la aplicación, lo que facilita la personalización y el mantenimiento.</p>
</li>
<li>
<p><strong>Componentes clave:</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Enlaces (Links):</strong> Unidades básicas de procesamiento, responsables de tareas como transformación de datos, acceso a fuentes externas o invocación de modelos.</p>
</li>
<li>
<p><strong>Cadenas (Chains):</strong> Secuencias de enlaces que definen flujos de trabajo completos, desde la entrada del usuario hasta la generación de la respuesta. Las cadenas permiten conectar modelos, herramientas y datos externos en procesos automatizados y sensibles al contexto.</p>
</li>
<li>
<p><strong>Agentes:</strong> Programas que ejecutan cadenas y pueden tomar decisiones dinámicas sobre qué acciones realizar o qué herramientas utilizar, dotando a la aplicación de autonomía y razonamiento avanzado.</p>
</li>
<li>
<p><strong>Gestión de prompts:</strong> Herramientas para diseñar y afinar cómo se comunica la aplicación con los LLM, optimizando la interacción y la calidad de las respuestas.</p>
</li>
<li>
<p><strong>Memoria contextual:</strong> Mecanismos para mantener el estado y el contexto de la conversación, logrando experiencias más naturales y personalizadas.</p>
</li>
<li>
<p><strong>Cargadores de documentos e integración de datos:</strong> Permiten conectar la aplicación con bases de conocimiento, documentos y fuentes externas, enriqueciendo la información disponible para el modelo.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_ventajas_y_propósito">2.1.3. Ventajas y propósito</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Optimización del diálogo:</strong> Facilita la gestión de conversaciones complejas y coherentes, mejorando la experiencia del usuario.</p>
</li>
<li>
<p><strong>Escalabilidad y personalización:</strong> Permite adaptar y escalar aplicaciones fácilmente mediante la combinación de módulos reutilizables.</p>
</li>
<li>
<p><strong>Integración de datos externos:</strong> Conecta los LLM con fuentes de datos específicas del dominio o del usuario, superando las limitaciones de los modelos puros.</p>
</li>
<li>
<p><strong>Desarrollo ágil:</strong> Proporciona herramientas y estructuras que aceleran el desarrollo de aplicaciones inteligentes y contextuales.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_resumen_gráfico_de_la_arquitectura">2.1.4. Resumen gráfico de la arquitectura</h4>
<div class="listingblock">
<div class="content">
<pre>Usuario
  ↓
Gestión de Prompt
  ↓
Cadenas (Chains)
  ↓
Agentes (opcional)
  ↓
Fuentes externas / Memoria / Herramientas
  ↓
Respuesta al usuario</pre>
</div>
</div>
<div class="paragraph">
<p>LangChain actúa como una capa de orquestación entre los LLM y el mundo real, permitiendo construir aplicaciones inteligentes, contextuales y conectadas con datos y herramientas externas, todo bajo una filosofía modular y flexible.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_componentes_principales_de_langchain">2.2. Componentes principales de LangChain</h3>
<div class="sect3">
<h4 id="_tools">2.2.1. Tools</h4>
<div class="paragraph">
<p>Las <strong>Tools</strong> en LangChain son funciones Python encapsuladas con un esquema (nombre, descripción, argumentos) que pueden ser llamadas por modelos de lenguaje, como Llama3.2 en Ollama, para ejecutar tareas externas (cálculos, búsquedas, consultas API, etc.). Esto amplía las capacidades del LLM más allá de la simple generación de texto, permitiendo agentes y cadenas que interactúan con el mundo real. Las tools se definen fácilmente usando el decorador <code>@tool</code>, y pueden ser conectadas a modelos que soportan "tool calling".</p>
</div>
<div class="olist arabic">
<div class="title">¿Cómo se trabaja con Tools?</div>
<ol class="arabic">
<li>
<p><strong>Definir la tool</strong>: Usando el decorador <code>@tool</code>, se crea una función con nombre, descripción y argumentos inferidos automáticamente.</p>
</li>
<li>
<p><strong>Vincular la tool al modelo</strong>: Se utiliza <code>.bind_tools([tool])</code> para que el modelo pueda decidir cuándo y cómo llamar a la tool.</p>
</li>
<li>
<p><strong>Invocar el modelo</strong>: El usuario interactúa con el modelo y, si es necesario, el LLM solicita la ejecución de la tool y usa el resultado en su respuesta.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="title">Ejemplo completo: sumar dos números con una tool y Llama3.2 local en Ollama</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_core.tools</span> <span class="keyword">import</span> <span class="include">tool</span>
<span class="keyword">from</span> <span class="include">langchain_ollama.chat_models</span> <span class="keyword">import</span> <span class="include">ChatOllama</span>
<span class="keyword">from</span> <span class="include">langgraph.prebuilt</span> <span class="keyword">import</span> <span class="include">create_react_agent</span>

<span class="decorator">@tool</span>
<span class="keyword">def</span> <span class="function">sumar</span>(a: <span class="predefined">int</span>, b: <span class="predefined">int</span>) -&gt; <span class="predefined">int</span>:
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Suma dos números y devuelve el resultado.</span><span class="delimiter">&quot;&quot;&quot;</span></span>
    <span class="keyword">return</span> a + b

<span class="decorator">@tool</span>
<span class="keyword">def</span> <span class="function">restar</span>(a: <span class="predefined">int</span>, b: <span class="predefined">int</span>) -&gt; <span class="predefined">int</span>:
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Resta dos números y devuelve el resultado.</span><span class="delimiter">&quot;&quot;&quot;</span></span>
    <span class="keyword">return</span> a - b

<span class="comment"># Crear el agente que ejecutará automáticamente las tools</span>
llm = ChatOllama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>, temperature=<span class="integer">0</span>)
agente = create_react_agent(llm, [sumar, restar])

<span class="comment"># Usar el agente</span>
user_input = <span class="string"><span class="delimiter">&quot;</span><span class="content">¿Cuánto es 15 más 27?</span><span class="delimiter">&quot;</span></span>
resultado = agente.invoke({<span class="string"><span class="delimiter">&quot;</span><span class="content">messages</span><span class="delimiter">&quot;</span></span>: [(<span class="string"><span class="delimiter">&quot;</span><span class="content">user</span><span class="delimiter">&quot;</span></span>, user_input)]})

print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Resultado final:</span><span class="delimiter">&quot;</span></span>, resultado[<span class="string"><span class="delimiter">&quot;</span><span class="content">messages</span><span class="delimiter">&quot;</span></span>][-<span class="integer">1</span>].content)</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Detalles técnicos y ventajas</div>
<ul>
<li>
<p>El decorador <code>@tool</code> infiere automáticamente el nombre, descripción y argumentos a partir de la función y su docstring.</p>
</li>
<li>
<p>El modelo puede inspeccionar el esquema de la tool (nombre, descripción, args) y decidir cuándo llamarla.</p>
</li>
<li>
<p>El resultado de la tool se integra en la respuesta final del modelo, permitiendo respuestas precisas y acciones automatizadas.</p>
</li>
<li>
<p>Se pueden definir tools para tareas mucho más avanzadas: llamadas API, scraping, consultas a bases de datos, análisis de sentimiento, etc..</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_tool_calling">2.2.2. Tool Calling</h4>
<div class="paragraph">
<p>El <strong>Tool Calling</strong> permite a los LLMs llamar funciones externas (tools) para realizar acciones específicas. Los modelos deciden cuándo y cómo usar estas herramientas basándose en el contexto de la consulta, combinando generación de texto con ejecución de código.</p>
</div>
<div class="olist arabic">
<div class="title">Componentes principales</div>
<ol class="arabic">
<li>
<p><strong>Tool</strong>: Función Python con esquema definido (nombre, descripción, parámetros)</p>
</li>
<li>
<p><strong>Modelo con soporte para tool calling</strong>: Debe entender cómo generar llamadas estructuradas</p>
</li>
<li>
<p><strong>Ejecutor</strong>: Mecanismo para invocar la herramienta y devolver resultados al modelo</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="title">Ejemplo completo: Calculadora con Llama3.2 local</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">ChatOllama</span>
<span class="keyword">from</span> <span class="include">langchain_core.tools</span> <span class="keyword">import</span> <span class="include">tool</span>

<span class="comment"># 1. Definir herramienta de suma</span>
<span class="decorator">@tool</span>
<span class="keyword">def</span> <span class="function">sumar</span>(a: <span class="predefined">int</span>, b: <span class="predefined">int</span>) -&gt; <span class="predefined">int</span>:
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Suma dos números enteros y devuelve el resultado.</span><span class="delimiter">&quot;&quot;&quot;</span></span>
    <span class="keyword">return</span> a + b

<span class="comment"># 2. Configurar modelo local</span>
llm = ChatOllama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>, temperature=<span class="integer">0</span>)

<span class="comment"># 3. Vincular herramienta al modelo</span>
modelo_con_tools = llm.bind_tools([sumar])

<span class="comment"># 4. Consulta que requiere usar la tool</span>
respuesta = modelo_con_tools.invoke(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Cuánto es 15 más 27? Por favor usa la calculadora.</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 5. Extraer y ejecutar tool call</span>
<span class="keyword">if</span> <span class="predefined">hasattr</span>(respuesta, <span class="string"><span class="delimiter">'</span><span class="content">tool_calls</span><span class="delimiter">'</span></span>):
    <span class="keyword">for</span> tool_call <span class="keyword">in</span> respuesta.tool_calls:
        <span class="keyword">if</span> tool_call[<span class="string"><span class="delimiter">'</span><span class="content">name</span><span class="delimiter">'</span></span>] == <span class="string"><span class="delimiter">'</span><span class="content">sumar</span><span class="delimiter">'</span></span>:
            args = tool_call[<span class="string"><span class="delimiter">'</span><span class="content">args</span><span class="delimiter">'</span></span>]
            resultado = sumar.invoke(args)  <span class="comment"># Use invoke instead</span>
            print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Resultado de la suma: {resultado}</span><span class="delimiter">&quot;</span></span>)
<span class="keyword">else</span>:
    print(respuesta)</code></pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Flujo de ejecución</div>
<ol class="arabic">
<li>
<p>El modelo detecta que la consulta requiere una operación matemática</p>
</li>
<li>
<p>Genera una llamada estructurada a la tool <code>sumar</code> con parámetros (a=15, b=27)</p>
</li>
<li>
<p>El sistema ejecuta la función con los argumentos proporcionados</p>
</li>
<li>
<p>Se muestra el resultado: "Resultado de la suma: 42"</p>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Características clave</div>
<ul>
<li>
<p>El decorador <code>@tool</code> genera automáticamente el esquema de la función</p>
</li>
<li>
<p><code>bind_tools</code> vincula las herramientas al modelo para su reconocimiento</p>
</li>
<li>
<p>Los modelos deciden dinámicamente cuándo usar tools</p>
</li>
<li>
<p>Soporte para ejecución local y privada con Ollama</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_structured_output">2.2.3. Structured Output</h4>
<div class="paragraph">
<p>El <strong>Structured Output</strong> en LangChain permite que los modelos de lenguaje generen respuestas en formatos estructurados (como JSON o modelos Pydantic) en vez de solo texto libre. Esto facilita la integración directa con sistemas, la validación automática de datos y el procesamiento consistente de la información, siendo ideal para aplicaciones empresariales, extracción de datos, generación de informes y workflows automatizados.</p>
</div>
<div class="olist arabic">
<div class="title">¿Cómo se trabaja con Structured Output?</div>
<ol class="arabic">
<li>
<p><strong>Definir un esquema estructurado</strong> (por ejemplo, con Pydantic) que describe el formato de la respuesta esperada.</p>
</li>
<li>
<p><strong>Configurar un parser</strong> que convierte la salida textual del modelo en un objeto Python validado.</p>
</li>
<li>
<p><strong>Instruir al modelo</strong> para que siga el formato estructurado usando instrucciones de formato en el prompt.</p>
</li>
<li>
<p><strong>Invocar el modelo</strong> y parsear la respuesta para obtener el objeto estructurado.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="title">Ejemplo completo: reporte meteorológico estructurado con Llama3.2 local en Ollama</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">pydantic</span> <span class="keyword">import</span> <span class="include">BaseModel</span>, <span class="include">Field</span>
<span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain_core.output_parsers</span> <span class="keyword">import</span> <span class="include">PydanticOutputParser</span>

<span class="keyword">class</span> <span class="class">DatosMeteorologicos</span>(BaseModel):
    ciudad: str = Field(description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Nombre de la ciudad analizada</span><span class="delimiter">&quot;</span></span>)
    temperatura_actual: float = Field(description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Temperatura en grados Celsius</span><span class="delimiter">&quot;</span></span>)
    condiciones: str = Field(description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Descripción del clima</span><span class="delimiter">&quot;</span></span>)
    pronostico: dict = Field(description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Pronóstico para las próximas 24 horas</span><span class="delimiter">&quot;</span></span>)

llm = OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>, temperature=<span class="float">0.3</span>)
parser = PydanticOutputParser(pydantic_object=DatosMeteorologicos)

prompt_template = <span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">Genera un reporte meteorológico REAL para {ubicacion}. NO devuelvas un esquema, devuelve datos reales.</span><span class="content">
</span><span class="content">
</span><span class="content">{format_instructions}</span><span class="content">
</span><span class="content">
</span><span class="content">IMPORTANTE: Devuelve un JSON válido con datos meteorológicos REALES para {ubicacion}, no un esquema o descripción del formato.</span><span class="content">
</span><span class="content">
</span><span class="content">Ejemplo de respuesta esperada:</span><span class="content">
</span><span class="content">{{</span><span class="content">
</span><span class="content">    &quot;ciudad&quot;: &quot;Madrid&quot;,</span><span class="content">
</span><span class="content">    &quot;temperatura_actual&quot;: 22.5,</span><span class="content">
</span><span class="content">    &quot;condiciones&quot;: &quot;Soleado&quot;,</span><span class="content">
</span><span class="content">    &quot;pronostico&quot;: {{&quot;temperatura_maxima&quot;: 25, &quot;temperatura_minima&quot;: 18, &quot;descripcion&quot;: &quot;Parcialmente nublado&quot;}}</span><span class="content">
</span><span class="content">}}</span><span class="content">
</span><span class="delimiter">&quot;&quot;&quot;</span></span>

prompt = prompt_template.format(
    ubicacion=<span class="string"><span class="delimiter">&quot;</span><span class="content">Madrid</span><span class="delimiter">&quot;</span></span>,
    format_instructions=parser.get_format_instructions()
)

respuesta_cruda = llm.invoke(prompt)
datos_estructurados = parser.parse(respuesta_cruda)

print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Ciudad: {datos_estructurados.ciudad}</span><span class="delimiter">&quot;</span></span>)
print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Temperatura: {datos_estructurados.temperatura_actual}°C</span><span class="delimiter">&quot;</span></span>)
print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Condiciones: {datos_estructurados.condiciones}</span><span class="delimiter">&quot;</span></span>)
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Pronóstico:</span><span class="delimiter">&quot;</span></span>, datos_estructurados.pronostico)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_memory">2.3. Memory</h3>
<div class="paragraph">
<p>La memoria permite a los modelos mantener el contexto entre interacciones, esencial para chatbots y asistentes virtuales.</p>
</div>
<div class="ulist">
<div class="title">Funciona almacenando:</div>
<ul>
<li>
<p>Historial completo de conversaciones (buffer)</p>
</li>
<li>
<p>Resúmenes de interacciones largas</p>
</li>
<li>
<p>Entidades clave y sus relaciones</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Implementación básica con ConversationBufferMemory</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">ConversationChain</span>
<span class="keyword">from</span> <span class="include">langchain.memory</span> <span class="keyword">import</span> <span class="include">ConversationBufferMemory</span>

<span class="comment"># Configurar modelo local con memoria</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>, temperature=<span class="float">0.7</span>)
memory = ConversationBufferMemory()
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=<span class="predefined-constant">True</span>  <span class="comment"># Muestra el proceso interno</span>
)

<span class="comment"># Primera interacción</span>
response1 = conversation.predict(input=<span class="string"><span class="delimiter">&quot;</span><span class="content">Hola, me llamo Juan</span><span class="delimiter">&quot;</span></span>)
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Asistente:</span><span class="delimiter">&quot;</span></span>, response1)

<span class="comment"># Segunda interacción con contexto preservado</span>
response2 = conversation.predict(input=<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Recuerdas mi nombre?</span><span class="delimiter">&quot;</span></span>)
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Asistente:</span><span class="delimiter">&quot;</span></span>, response2)</code></pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Tipos avanzados de memoria</div>
<ol class="arabic">
<li>
<p><strong>ConversationSummaryMemory</strong>: Reduce el uso de tokens con resúmenes</p>
</li>
<li>
<p><strong>ConversationBufferWindowMemory</strong>: Limita el historial a las últimas K interacciones</p>
</li>
<li>
<p><strong>EntityMemory</strong>: Recuerda entidades específicas (nombres, fechas)</p>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Mejores prácticas</div>
<ul>
<li>
<p>Usar <code>return_messages=True</code> para formato de chat estructurado</p>
</li>
<li>
<p>Limitar el buffer a 2000 tokens para evitar sobrecarga</p>
</li>
<li>
<p>Combinar con RAG para contexto ampliado</p>
</li>
<li>
<p>Persistir en disco con <code>FileChatMessageHistory</code> para conversaciones largas</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_document_loader">2.3.1. Document Loader</h4>
<div class="paragraph">
<p>Los Document Loaders son componentes de LangChain que cargan datos de diversas fuentes (PDFs, CSVs, webs, etc.) al formato estándar <code>Document</code> de LangChain, permitiendo su posterior procesamiento y análisis. Son esenciales para construir sistemas de Recuperación Aumentada por Generación (RAG), donde se integra conocimiento específico en aplicaciones de IA locales con modelos como Llama3.2.</p>
</div>
<div class="listingblock">
<div class="title">Implementación con modelo local Llama3.2</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">PyPDFLoader</span>
<span class="keyword">from</span> <span class="include">langchain_text_splitters</span> <span class="keyword">import</span> <span class="include">RecursiveCharacterTextSplitter</span>
<span class="keyword">from</span> <span class="include">langchain_community.embeddings</span> <span class="keyword">import</span> <span class="include">OllamaEmbeddings</span>
<span class="keyword">from</span> <span class="include">langchain_community.vectorstores</span> <span class="keyword">import</span> <span class="include">FAISS</span>
<span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">RetrievalQA</span>

<span class="comment"># Cargar documento PDF (4 páginas)</span>
loader = PyPDFLoader(<span class="string"><span class="delimiter">&quot;</span><span class="content">data/kotlin.pdf</span><span class="delimiter">&quot;</span></span>)
pages = loader.load()

<span class="comment"># Dividir en chunks semánticos</span>
splitter = RecursiveCharacterTextSplitter(
    chunk_size=<span class="integer">1000</span>,
    chunk_overlap=<span class="integer">200</span>,
    separators=[<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">(?&lt;=</span><span class="content">\.</span><span class="content"> )</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content"> </span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="delimiter">&quot;</span></span>]
)
chunks = splitter.split_documents(pages)

<span class="comment"># Usar modelo local para embeddings</span>
embeddings = OllamaEmbeddings(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
vector_store = FAISS.from_documents(chunks, embeddings)

llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>, temperature=<span class="float">0.3</span>)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vector_store.as_retriever(search_kwargs={<span class="string"><span class="delimiter">&quot;</span><span class="content">k</span><span class="delimiter">&quot;</span></span>: <span class="integer">3</span>}),
    chain_type=<span class="string"><span class="delimiter">&quot;</span><span class="content">stuff</span><span class="delimiter">&quot;</span></span>
)

respuesta = qa_chain.invoke(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿De qué trata el documento?</span><span class="delimiter">&quot;</span></span>)
print(respuesta[<span class="string"><span class="delimiter">&quot;</span><span class="content">result</span><span class="delimiter">&quot;</span></span>])</code></pre>
</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 2. Tipos comunes de Document Loaders</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Tipo</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Fuente</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ejemplo de uso</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>PyPDFLoader</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Archivos PDF</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>PyPDFLoader("manual.pdf").load()</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>CSVLoader</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Archivos CSV</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>CSVLoader("datos.csv").load()</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>WebBaseLoader</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Páginas web</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>WebBaseLoader(["https://ejemplo.com"]).load()</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>TextLoader</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Archivos de texto</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>TextLoader("notas.txt").load()</code></p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<div class="title">Buenas prácticas</div>
<ul>
<li>
<p>Usar <code>chunk_size</code> entre 500-1500 tokens para equilibrio contexto/eficiencia</p>
</li>
<li>
<p>Incluir metadatos relevantes para trazabilidad (<code>source</code>, <code>page</code>, etc.)</p>
</li>
<li>
<p>Combinar con modelos locales para privacidad total</p>
</li>
<li>
<p>Usar <code>lazy_load</code> para grandes datasets</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_retrieval">2.3.2. Retrieval</h4>
<div class="paragraph">
<p>El <strong>Retrieval</strong> en LangChain se refiere al proceso de recuperar información relevante de fuentes externas para mejorar las respuestas de los LLMs. Es fundamental en sistemas RAG (Retrieval-Augmented Generation).</p>
</div>
<div class="ulist">
<div class="title">Componentes clave:</div>
<ul>
<li>
<p><strong>Document Loaders</strong>: Carga de documentos (PDFs, webs, BD)</p>
</li>
<li>
<p><strong>Text Splitters</strong>: División de textos en fragmentos manejables</p>
</li>
<li>
<p><strong>Embeddings</strong>: Vectorización del contenido usando modelos como Nomic-embed-text</p>
</li>
<li>
<p><strong>Vector Stores</strong>: Almacenamiento eficiente de vectores (Chroma, Milvus)</p>
</li>
<li>
<p><strong>Retrievers</strong>: Algoritmos de búsqueda semántica</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Proceso de implementación</div>
<ol class="arabic">
<li>
<p>Cargar documentos con <code>PyPDFLoader</code></p>
</li>
<li>
<p>Dividir textos con <code>RecursiveCharacterTextSplitter</code></p>
</li>
<li>
<p>Generar embeddings con Ollama</p>
</li>
<li>
<p>Almacenar en base de datos vectorial</p>
</li>
<li>
<p>Configurar cadena de recuperación con LangChain</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="title">Ejemplo completo: Búsqueda semántica en documentos técnicos con Llama3.2 local</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">PyPDFLoader</span>
<span class="keyword">from</span> <span class="include">langchain_text_splitters</span> <span class="keyword">import</span> <span class="include">RecursiveCharacterTextSplitter</span>
<span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbeddings</span>, <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain_community.vectorstores</span> <span class="keyword">import</span> <span class="include">Chroma</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">RetrievalQA</span>

<span class="comment"># 1. Cargar documento</span>
loader = PyPDFLoader(<span class="string"><span class="delimiter">&quot;</span><span class="content">data/kotlin.pdf</span><span class="delimiter">&quot;</span></span>)
documents = loader.load()

<span class="comment"># 2. Dividir texto</span>
text_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="integer">1000</span>, chunk_overlap=<span class="integer">200</span>)
texts = text_splitter.split_documents(documents)

<span class="comment"># 3. Crear embeddings con modelo local</span>
embeddings = OllamaEmbeddings(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">nomic-embed-text:latest</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 4. Almacenar en ChromaDB</span>
vectorstore = Chroma.from_documents(
    texts,
    embeddings,
    collection_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">tech-docs</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 5. Configurar retriever</span>
retriever = vectorstore.as_retriever(search_kwargs={<span class="string"><span class="delimiter">&quot;</span><span class="content">k</span><span class="delimiter">&quot;</span></span>: <span class="integer">3</span>})

<span class="comment"># 6. Crear cadena QA con Llama3.2</span>
llm = OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=retriever,
    chain_type=<span class="string"><span class="delimiter">&quot;</span><span class="content">stuff</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># Ejemplo de uso</span>
query = <span class="string"><span class="delimiter">&quot;</span><span class="content">¿Se trata la programacion orientada a objetos kotlin?</span><span class="delimiter">&quot;</span></span>
result = qa_chain({<span class="string"><span class="delimiter">&quot;</span><span class="content">query</span><span class="delimiter">&quot;</span></span>: query})
print(result[<span class="string"><span class="delimiter">&quot;</span><span class="content">result</span><span class="delimiter">&quot;</span></span>])</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Este sistema permite:</div>
<ul>
<li>
<p>Búsqueda semántica en documentos técnicos</p>
</li>
<li>
<p>Respuestas contextualizadas usando Llama3.2</p>
</li>
<li>
<p>Operación local sin dependencia de servicios externos</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">La arquitectura combinada de LangChain + Ollama + Chroma muestra cómo:</div>
<ol class="arabic">
<li>
<p>Los <code>Document Loaders</code> ingieren datos en crudo</p>
</li>
<li>
<p>Los <code>Embeddings</code> crean representaciones vectoriales</p>
</li>
<li>
<p>El <code>Vector Store</code> permite búsquedas eficientes</p>
</li>
<li>
<p>El LLM local genera respuestas enriquecidas</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_text_splitters">2.3.3. Text Splitters</h4>
<div class="paragraph">
<p>Los <strong>Text Splitters</strong> son componentes críticos en el procesamiento de documentos para LLMs, diseñados para dividir textos en fragmentos manejables manteniendo la coherencia semántica. Su función principal es adaptar contenidos extensos a los límites de contexto de los modelos, mejorando la eficiencia en tareas como RAG (Retrieval-Augmented Generation).</p>
</div>
<div class="ulist">
<div class="title">Componentes clave:</div>
<ul>
<li>
<p><strong>Estrategia recursiva</strong>: Divide jerárquicamente (párrafos → oraciones → palabras)</p>
</li>
<li>
<p><strong>Control de tamaño</strong>: <code>chunk_size</code> define longitud máxima por fragmento</p>
</li>
<li>
<p><strong>Solapamiento contextual</strong>: <code>chunk_overlap</code> preserva contexto entre chunks</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Implementación práctica</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Ejemplo completo con Llama3.2 y Ollama</span>
<span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">PyPDFLoader</span>
<span class="keyword">from</span> <span class="include">langchain_text_splitters</span> <span class="keyword">import</span> <span class="include">RecursiveCharacterTextSplitter</span>
<span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>

<span class="comment"># 1. Cargar documento técnico</span>
loader = PyPDFLoader(<span class="string"><span class="delimiter">&quot;</span><span class="content">data/kotlin.pdf</span><span class="delimiter">&quot;</span></span>)
documents = loader.load()

<span class="comment"># 2. Configurar splitter recursivo</span>
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=<span class="integer">1000</span>,
    chunk_overlap=<span class="integer">200</span>,
    separators=[<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">. </span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content"> </span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="delimiter">&quot;</span></span>]
)
text_chunks = text_splitter.split_documents(documents)

<span class="comment"># 3. Procesar chunks con modelo local</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)  <span class="comment"># Versión 8B para mejor rendimiento</span>

<span class="comment"># Analizar primer chunk</span>
primer_chunk = text_chunks[<span class="integer">0</span>].page_content
analisis = llm.invoke(
    f<span class="string"><span class="delimiter">&quot;</span><span class="content">Resume este fragmento técnico manteniendo términos clave:</span><span class="char">\n</span><span class="content">{primer_chunk}</span><span class="delimiter">&quot;</span></span>
)

print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Chunk dividido: {len(primer_chunk)} caracteres</span><span class="delimiter">&quot;</span></span>)
print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Resumen modelo:</span><span class="char">\n</span><span class="content">{analisis}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Este flujo muestra:</div>
<ul>
<li>
<p>Carga de PDFs con <code>PyPDFLoader</code></p>
</li>
<li>
<p>División inteligente preservando estructura jerárquica</p>
</li>
<li>
<p>Procesamiento local con modelo Llama3 mediante Ollama</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 3. Parámetros clave para <code>RecursiveCharacterTextSplitter</code>:</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Parámetro</th>
<th class="tableblock halign-left valign-top">Función</th>
<th class="tableblock halign-left valign-top">Valor típico</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">chunk_size</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Tamaño máximo por fragmento</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">500-1500 caracteres</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">chunk_overlap</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Contexto entre fragments</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">10-20% del chunk_size</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">separators</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Prioridad de división</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">["\n\n", "\n", ". "]</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<div class="title"><strong>Optimización para manuales técnicos:</strong></div>
<ul>
<li>
<p>Usar <code>separators=["\n\n## ", "\n\n", "\n"]</code> para secciones Markdown</p>
</li>
<li>
<p>Ajustar <code>chunk_size</code> según densidad de información</p>
</li>
<li>
<p>Mantener <code>chunk_overlap</code> bajo para evitar redundancias</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_embedding_models_en_langchain">2.3.4. Embedding Models en LangChain</h4>
<div class="paragraph">
<p>Los <strong>Embedding Models</strong> transforman texto en representaciones vectoriales que capturan significado semántico, permitiendo comparaciones matemáticas entre contenidos. En LangChain facilitan operaciones como búsqueda semántica y RAG (Retrieval-Augmented Generation).</p>
</div>
<div class="ulist">
<div class="title">Características clave:</div>
<ul>
<li>
<p><strong>Vectorización contextual</strong>: Conversión de texto a arrays numéricos densos</p>
</li>
<li>
<p><strong>Preservación semántica</strong>: Relaciones espaciales reflejan similitudes conceptuales</p>
</li>
<li>
<p><strong>Interoperabilidad</strong>: Interface unificada para múltiples proveedores</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Implementación con Ollama</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Ejemplo completo usando Llama3.2 y Ollama</span>
<span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">PyPDFLoader</span>
<span class="keyword">from</span> <span class="include">langchain_text_splitters</span> <span class="keyword">import</span> <span class="include">RecursiveCharacterTextSplitter</span>
<span class="keyword">from</span> <span class="include">langchain_community.embeddings</span> <span class="keyword">import</span> <span class="include">OllamaEmbeddings</span>
<span class="keyword">from</span> <span class="include">langchain_community.vectorstores</span> <span class="keyword">import</span> <span class="include">Chroma</span>
<span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>

<span class="comment"># 1. Cargar documento técnico</span>
loader = PyPDFLoader(<span class="string"><span class="delimiter">&quot;</span><span class="content">data/kotlin.pdf</span><span class="delimiter">&quot;</span></span>)
documents = loader.load()

<span class="comment"># 2. Dividir texto</span>
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=<span class="integer">800</span>,
    chunk_overlap=<span class="integer">150</span>
)
texts = text_splitter.split_documents(documents)

<span class="comment"># 3. Generar embeddings locales</span>
embeddings = OllamaEmbeddings(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">nomic-embed-text:latest</span><span class="delimiter">&quot;</span></span>)  <span class="comment"># Modelo especializado</span>

<span class="comment"># 4. Almacenar vectores</span>
vectorstore = Chroma.from_documents(
    texts,
    embeddings,
    collection_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">iot-embeddings</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 5. Búsqueda semántica</span>
query = <span class="string"><span class="delimiter">&quot;</span><span class="content">¿En qué tema se ve el testing en kotlin?</span><span class="delimiter">&quot;</span></span>
docs = vectorstore.similarity_search(query, k=<span class="integer">2</span>)

<span class="comment"># 6. Integración con LLM</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
contexto = <span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>.join([doc.page_content <span class="keyword">for</span> doc <span class="keyword">in</span> docs])
respuesta = llm.invoke(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Contexto:</span><span class="char">\n</span><span class="content">{contexto}</span><span class="char">\n</span><span class="char">\n</span><span class="content">Pregunta: {query}</span><span class="delimiter">&quot;</span></span>)

print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta contextualizada:</span><span class="char">\n</span><span class="content">{respuesta}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Flujo de trabajo</div>
<ol class="arabic">
<li>
<p><strong>Ingestión</strong>: <code>PyPDFLoader</code> carga documentos PDF</p>
</li>
<li>
<p><strong>Fragmentación</strong>: Splitter divide texto preservando contexto</p>
</li>
<li>
<p><strong>Vectorización</strong>: Ollama genera embeddings con modelo local</p>
</li>
<li>
<p><strong>Almacenamiento</strong>: Chroma guarda vectores para búsqueda eficiente</p>
</li>
<li>
<p><strong>Consulta</strong>: Búsqueda semántica encuentra fragmentos relevantes</p>
</li>
<li>
<p><strong>Generación</strong>: LLM local produce respuesta usando contexto</p>
</li>
</ol>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 4. Configuración avanzada</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Parámetro</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Función</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Valor óptimo</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Modelo embedding</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Determina calidad vectorial</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>nomic-embed-text</code> o <code>mxbai-embed-large</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Tamaño de chunk</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Balance contexto/eficiencia</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">500-1000 tokens</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Dimensión vector</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Densidad información</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">768-1024 (modelos modernos)</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<div class="title"><strong>Ventajas clave:</strong></div>
<ul>
<li>
<p>Operación 100% local sin APIs externas</p>
</li>
<li>
<p>Baja latencia en generación de embeddings</p>
</li>
<li>
<p>Integración nativa con ecosistema LangChain</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_vector_stores_en_langchain_markdown">2.3.5. Vector Stores en LangChain (Markdown)</h4>
<div class="paragraph">
<p>Los <strong>Vector Stores</strong> almacenan y permiten búsquedas eficientes sobre embeddings, fundamentales para sistemas RAG en LangChain.</p>
</div>
<div class="ulist">
<div class="title">Características principales:</div>
<ul>
<li>
<p>Almacenamiento vectorial de fragmentos de texto procesados</p>
</li>
<li>
<p>Búsqueda semántica por similitud de vectores</p>
</li>
<li>
<p>Integración con bases como Chroma para uso local</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Implementación con Chroma y Ollama (Markdown)</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Ejemplo: procesamiento de un documento Markdown local</span>
<span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">TextLoader</span>
<span class="keyword">from</span> <span class="include">langchain_text_splitters</span> <span class="keyword">import</span> <span class="include">RecursiveCharacterTextSplitter</span>
<span class="keyword">from</span> <span class="include">langchain_community.embeddings</span> <span class="keyword">import</span> <span class="include">OllamaEmbeddings</span>
<span class="keyword">from</span> <span class="include">langchain_community.vectorstores</span> <span class="keyword">import</span> <span class="include">Chroma</span>
<span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>

<span class="comment"># 1. Cargar documento Markdown</span>
loader = TextLoader(<span class="string"><span class="delimiter">&quot;</span><span class="content">data/ia.md</span><span class="delimiter">&quot;</span></span>, encoding=<span class="string"><span class="delimiter">&quot;</span><span class="content">utf-8</span><span class="delimiter">&quot;</span></span>)
documents = loader.load()

<span class="comment"># 2. Dividir texto en fragmentos</span>
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=<span class="integer">1200</span>,
    chunk_overlap=<span class="integer">240</span>
)
texts = text_splitter.split_documents(documents)

<span class="comment"># 3. Generar embeddings locales</span>
embeddings = OllamaEmbeddings(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">nomic-embed-text:latest</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 4. Configurar ChromaDB</span>
vector_store = Chroma.from_documents(
    documents=texts,
    embedding=embeddings,
    persist_directory=<span class="string"><span class="delimiter">&quot;</span><span class="content">vector_db</span><span class="delimiter">&quot;</span></span>,
    collection_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">markdown-ia</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 5. Búsqueda semántica</span>
query = <span class="string"><span class="delimiter">&quot;</span><span class="content">¿Qué es la inteligencia artificial?</span><span class="delimiter">&quot;</span></span>
results = vector_store.similarity_search(query, k=<span class="integer">3</span>)

<span class="comment"># 6. Integración con LLM local</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
contexto = <span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>.join([doc.page_content <span class="keyword">for</span> doc <span class="keyword">in</span> results])
respuesta = llm.invoke(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Contexto:</span><span class="char">\n</span><span class="content">{contexto}</span><span class="char">\n</span><span class="char">\n</span><span class="content">Pregunta: {query}</span><span class="delimiter">&quot;</span></span>)

print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta contextualizada:</span><span class="char">\n</span><span class="content">{respuesta}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Flujo de trabajo</div>
<ol class="arabic">
<li>
<p>Carga de Markdown con <code>TextLoader</code></p>
</li>
<li>
<p>Fragmentación semántica con splitter recursivo</p>
</li>
<li>
<p>Vectorización local de textos con Ollama</p>
</li>
<li>
<p>Almacenamiento y consulta en ChromaDB</p>
</li>
<li>
<p>Respuesta contextualizada con Llama3.2</p>
</li>
</ol>
</div>
<div class="ulist">
<div class="title"><strong>Ventajas</strong>:</div>
<ul>
<li>
<p>Procesamiento y búsqueda local sobre documentos Markdown</p>
</li>
<li>
<p>Integración nativa con LangChain y Ollama</p>
</li>
<li>
<p>Ejemplo práctico y reproducible para flujos de IA documentados en AsciiDoc</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_retriever_en_langchain">2.3.6. Retriever en LangChain</h4>
<div class="paragraph">
<p>El <strong>Retriever</strong> es el componente central en sistemas RAG que recupera documentos relevantes para una consulta usando búsqueda semántica. Su función principal es conectar la consulta del usuario con la información almacenada en bases vectoriales.</p>
</div>
<div class="ulist">
<div class="title">Características clave:</div>
<ul>
<li>
<p><strong>Interfaz estandarizada</strong>: Recibe strings y devuelve documentos</p>
</li>
<li>
<p><strong>Búsqueda contextual</strong>: Utiliza embeddings y metadatos para relevancia</p>
</li>
<li>
<p><strong>Integración modular</strong>: Funciona con múltiples bases vectoriales (Chroma, Redis)</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Implementación con Ollama</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Ejemplo completo usando Llama3.2 y ChromaDB</span>
<span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">TextLoader</span>
<span class="keyword">from</span> <span class="include">langchain_text_splitters</span> <span class="keyword">import</span> <span class="include">RecursiveCharacterTextSplitter</span>
<span class="keyword">from</span> <span class="include">langchain_community.embeddings</span> <span class="keyword">import</span> <span class="include">OllamaEmbeddings</span>
<span class="keyword">from</span> <span class="include">langchain_community.vectorstores</span> <span class="keyword">import</span> <span class="include">Chroma</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">RetrievalQA</span>
<span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>

<span class="comment"># 1. Cargar documento técnico</span>
loader = TextLoader(<span class="string"><span class="delimiter">&quot;</span><span class="content">data/ia.md</span><span class="delimiter">&quot;</span></span>, encoding=<span class="string"><span class="delimiter">&quot;</span><span class="content">utf-8</span><span class="delimiter">&quot;</span></span>)
documents = loader.load()

<span class="comment"># 2. Dividir texto</span>
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=<span class="integer">200</span>,
    chunk_overlap=<span class="integer">20</span>
)
texts = text_splitter.split_documents(documents)

<span class="comment"># 3. Generar embeddings locales</span>
embeddings = OllamaEmbeddings(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">nomic-embed-text:latest</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 4. Configurar vector store</span>
vector_db = Chroma.from_documents(
    texts,
    embeddings,
    persist_directory=<span class="string"><span class="delimiter">&quot;</span><span class="content">retriever_db</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 5. Crear retriever personalizado (remove score_threshold)</span>
retriever = vector_db.as_retriever(
    search_type=<span class="string"><span class="delimiter">&quot;</span><span class="content">similarity</span><span class="delimiter">&quot;</span></span>,
    search_kwargs={<span class="string"><span class="delimiter">&quot;</span><span class="content">k</span><span class="delimiter">&quot;</span></span>: <span class="integer">3</span>}
)

<span class="comment"># 6. Integrar con modelo local</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>, temperature=<span class="float">0.1</span>)
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=retriever,
    chain_type=<span class="string"><span class="delimiter">&quot;</span><span class="content">stuff</span><span class="delimiter">&quot;</span></span>,
    return_source_documents=<span class="predefined-constant">True</span>
)

<span class="comment"># Ejemplo de uso (use invoke instead of __call__)</span>
consulta = <span class="string"><span class="delimiter">&quot;</span><span class="content">qué hizo Douglas Lenat en 1979?</span><span class="delimiter">&quot;</span></span>
resultado = qa_chain.invoke({<span class="string"><span class="delimiter">&quot;</span><span class="content">query</span><span class="delimiter">&quot;</span></span>: consulta})
print(resultado[<span class="string"><span class="delimiter">&quot;</span><span class="content">result</span><span class="delimiter">&quot;</span></span>])</code></pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Flujo de trabajo</div>
<ol class="arabic">
<li>
<p><strong>Carga</strong>: <code>TextLoader</code> ingiere documentos Markdown/PDF</p>
</li>
<li>
<p><strong>Fragmentación</strong>: Splitter recursivo mantiene contexto</p>
</li>
<li>
<p><strong>Vectorización</strong>: Modelo Nomic-embed-text local via Ollama</p>
</li>
<li>
<p><strong>Almacenamiento</strong>: ChromaDB persiste vectores y metadatos</p>
</li>
<li>
<p><strong>Búsqueda</strong>: Retriever filtra por similitud y umbral</p>
</li>
<li>
<p><strong>Generación</strong>: Llama3.2 crea respuesta contextualizada</p>
</li>
</ol>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 5. Configuración avanzada</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Parámetro</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Función</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Valor óptimo</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">search_type</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Algoritmo de búsqueda</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>similarity</code>/<code>mmr</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">k</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Resultados a retornar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">3-5 según precisión</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">score_threshold</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Filtro de relevancia</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.6-0.8</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<div class="title"><strong>Optimizaciones:</strong></div>
<ul>
<li>
<p>Usar <code>search_type="mmr"</code> para diversidad en resultados</p>
</li>
<li>
<p>Combinar filtros de metadatos con búsqueda semántica</p>
</li>
<li>
<p>Implementar caché de embeddings para consultas recurrentes</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_output_parser_en_langchain">2.3.7. Output Parser en LangChain</h4>
<div class="paragraph">
<p>Los <strong>Output Parsers</strong> transforman respuestas textuales de LLMs en formatos estructurados (JSON, listas, objetos Python). Esenciales para integrar salidas de modelos en aplicaciones, permiten:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Normalización de respuestas</p>
</li>
<li>
<p>Validación de estructuras de datos</p>
</li>
<li>
<p>Conversión a formatos consumibles por sistemas externos</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Tipos principales</div>
<ul>
<li>
<p><strong>StructuredOutputParser</strong>: Crea diccionarios según esquema personalizado</p>
</li>
<li>
<p><strong>ListOutputParser</strong>: Convierte textos en listas Python</p>
</li>
<li>
<p><strong>DatetimeOutputParser</strong>: Extrae fechas/horas en objetos datetime</p>
</li>
<li>
<p><strong>PydanticOutputParser</strong>: Valida salidas contra modelos Pydantic</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Implementación con Ollama</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Ejemplo completo con Llama3.2 y StructuredOutputParser</span>
<span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">langchain.output_parsers</span> <span class="keyword">import</span> <span class="include">StructuredOutputParser</span>, <span class="include">ResponseSchema</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>

<span class="comment"># 1. Definir esquema de respuesta</span>
response_schemas = [
    ResponseSchema(name=<span class="string"><span class="delimiter">&quot;</span><span class="content">concepto</span><span class="delimiter">&quot;</span></span>, description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Concepto técnico explicado</span><span class="delimiter">&quot;</span></span>),
    ResponseSchema(name=<span class="string"><span class="delimiter">&quot;</span><span class="content">aplicacion</span><span class="delimiter">&quot;</span></span>, description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Uso práctico en IA</span><span class="delimiter">&quot;</span></span>),
    ResponseSchema(name=<span class="string"><span class="delimiter">&quot;</span><span class="content">ejemplo</span><span class="delimiter">&quot;</span></span>, description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Ejemplo de código breve</span><span class="delimiter">&quot;</span></span>)
]

<span class="comment"># 2. Configurar parser y modelo local</span>
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 3. Crear prompt con instrucciones</span>
format_instructions = output_parser.get_format_instructions()
template = <span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">Explica el concepto de {tema} en inteligencia artificial.</span><span class="content">
</span><span class="content">Incluye una aplicación práctica y un ejemplo de código.</span><span class="content">
</span><span class="content">
</span><span class="content">IMPORTANTE: Responde ÚNICAMENTE con un JSON válido en el formato exacto especificado.</span><span class="content">
</span><span class="content">
</span><span class="content">{format_instructions}</span><span class="content">
</span><span class="content">
</span><span class="content">Responde solo con el JSON, sin texto adicional antes o después.</span><span class="content">
</span><span class="delimiter">&quot;&quot;&quot;</span></span>

prompt = PromptTemplate(
    template=template,
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">tema</span><span class="delimiter">&quot;</span></span>],
    partial_variables={<span class="string"><span class="delimiter">&quot;</span><span class="content">format_instructions</span><span class="delimiter">&quot;</span></span>: format_instructions}
)

<span class="comment"># 4. Ejecutar y parsear</span>
consulta = prompt.format(tema=<span class="string"><span class="delimiter">&quot;</span><span class="content">attention mechanism</span><span class="delimiter">&quot;</span></span>)
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Prompt enviado:</span><span class="delimiter">&quot;</span></span>)
print(consulta)
print(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span> + <span class="string"><span class="delimiter">&quot;</span><span class="content">=</span><span class="delimiter">&quot;</span></span>*<span class="integer">50</span> + <span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)

respuesta_cruda = llm.invoke(consulta)
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta cruda del modelo:</span><span class="delimiter">&quot;</span></span>)
print(<span class="predefined">repr</span>(respuesta_cruda))  <span class="comment"># Use repr to see exact content</span>
print(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span> + <span class="string"><span class="delimiter">&quot;</span><span class="content">=</span><span class="delimiter">&quot;</span></span>*<span class="integer">50</span> + <span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># Try to parse only if we have content</span>
<span class="keyword">if</span> respuesta_cruda <span class="keyword">and</span> respuesta_cruda.strip():
    <span class="keyword">try</span>:
        respuesta_estructurada = output_parser.parse(respuesta_cruda)
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Concepto: {respuesta_estructurada['concepto']}</span><span class="delimiter">&quot;</span></span>)
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Aplicación: {respuesta_estructurada['aplicacion']}</span><span class="delimiter">&quot;</span></span>)
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Ejemplo:</span><span class="char">\n</span><span class="content">{respuesta_estructurada['ejemplo']}</span><span class="delimiter">&quot;</span></span>)
    <span class="keyword">except</span> <span class="exception">Exception</span> <span class="keyword">as</span> e:
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Error parsing: {e}</span><span class="delimiter">&quot;</span></span>)
        print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Raw response:</span><span class="delimiter">&quot;</span></span>, respuesta_cruda)
<span class="keyword">else</span>:
    print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Empty response from model</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Flujo de trabajo</div>
<ol class="arabic">
<li>
<p><strong>Definición de esquema</strong>: Especifica estructura requerida</p>
</li>
<li>
<p><strong>Generación de instrucciones</strong>: <code>get_format_instructions()</code> crea texto para prompt</p>
</li>
<li>
<p><strong>Ejecución del modelo</strong>: Llama3.2 genera respuesta textual</p>
</li>
<li>
<p><strong>Validación y parsing</strong>: Convierte texto a estructura validada</p>
</li>
</ol>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 6. Configuración avanzada</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Parámetro</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Función</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ejemplo</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ResponseSchema</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Define campos y descripciones</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>name="email", description="dirección de correo"</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">search_kwargs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Controla comportamiento de búsqueda</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>k=3</code> (número de resultados)</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<div class="title"><strong>Ventajas clave:</strong></div>
<ul>
<li>
<p>Integración con modelos locales via Ollama</p>
</li>
<li>
<p>Validación automática de estructuras</p>
</li>
<li>
<p>Compatibilidad con múltiples formatos de salida</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_few_shot_prompting">2.3.8. Few-shot Prompting</h4>
<div class="paragraph">
<p>El <strong>few-shot prompting</strong> es una técnica que proporciona ejemplos contextuales al LLM para guiar su comportamiento en tareas específicas.</p>
</div>
<div class="ulist">
<div class="title">En LangChain se implementa mediante:</div>
<ul>
<li>
<p><strong>Ejemplos demostrativos</strong>: Pares entrada-salida que ilustran la tarea</p>
</li>
<li>
<p><strong>Plantillas estructuradas</strong>: Formateo consistente usando <code>FewShotPromptTemplate</code></p>
</li>
<li>
<p><strong>Selectores dinámicos</strong>: Mecanismos para optimizar ejemplos usados (ej: <code>LengthBasedExampleSelector</code>)</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Implementación con Ollama</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Ejemplo completo usando Llama3.2 y few-shot prompting</span>
<span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">FewShotPromptTemplate</span>, <span class="include">PromptTemplate</span>

<span class="comment"># 1. Definir ejemplos demostrativos</span>
examples = [
    {
        <span class="string"><span class="delimiter">&quot;</span><span class="content">pregunta</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">¿Cómo calibrar un sensor de temperatura?</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">respuesta</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1. Encender el dispositivo</span><span class="char">\n</span><span class="content">2. Aplicar fuente de referencia</span><span class="char">\n</span><span class="content">3. Ajustar hasta coincidir valor esperado</span><span class="delimiter">&quot;</span></span>
    },
    {
        <span class="string"><span class="delimiter">&quot;</span><span class="content">pregunta</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">¿Procedimiento para reinicio de fábrica?</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">respuesta</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1. Mantener botón power 10s</span><span class="char">\n</span><span class="content">2. Seleccionar 'Restablecer'</span><span class="char">\n</span><span class="content">3. Confirmar con contraseña admin</span><span class="delimiter">&quot;</span></span>
    }
]

<span class="comment"># 2. Crear plantilla de formato para ejemplos</span>
example_template = <span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">Pregunta: {pregunta}</span><span class="content">
</span><span class="content">Respuesta: {respuesta}</span><span class="content">
</span><span class="delimiter">&quot;&quot;&quot;</span></span>
example_prompt = PromptTemplate(
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">pregunta</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">respuesta</span><span class="delimiter">&quot;</span></span>],
    template=example_template
)

<span class="comment"># 3. Configurar prompt few-shot</span>
few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix=<span class="string"><span class="delimiter">&quot;</span><span class="content">Eres un asistente técnico especializado en manuales de equipo. Responde con procedimientos paso a paso:</span><span class="delimiter">&quot;</span></span>,
    suffix=<span class="string"><span class="delimiter">&quot;</span><span class="content">Pregunta: {input}</span><span class="char">\n</span><span class="content">Respuesta:</span><span class="delimiter">&quot;</span></span>,
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>],
    example_separator=<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 4. Integrar con modelo local</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
consulta = few_shot_prompt.format(input=<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Cómo realizar mantenimiento preventivo?</span><span class="delimiter">&quot;</span></span>)
respuesta = llm.invoke(consulta)

print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta generada:</span><span class="char">\n</span><span class="content">{respuesta}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Flujo de trabajo</div>
<ol class="arabic">
<li>
<p><strong>Definición de ejemplos</strong>: Selección de casos representativos</p>
</li>
<li>
<p><strong>Estructuración</strong>: Formateo consistente con <code>PromptTemplate</code></p>
</li>
<li>
<p><strong>Ensamblaje</strong>: Combinación prefijo/ejemplos/sufijo</p>
</li>
<li>
<p><strong>Generación</strong>: Modelo local produce respuesta contextualizada</p>
</li>
</ol>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 7. Configuración avanzada</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Parámetro</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Función</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Valor óptimo</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">examples</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Casos demostrativos</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2-5 ejemplos variados</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">example_separator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Divisor entre ejemplos</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">"\n\n" para claridad</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">max_length</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Longitud máxima total</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1500-2000 tokens</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<div class="title"><strong>Ventajas clave:</strong></div>
<ul>
<li>
<p>Adaptabilidad a nuevas tareas sin reentrenamiento</p>
</li>
<li>
<p>Mejora precisión en tareas complejas (58%↑ según tests)</p>
</li>
<li>
<p>Operación 100% local con Ollama y modelos Llama3</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_example_selectors_en_langchain">2.3.9. Example Selectors en LangChain</h4>
<div class="paragraph">
<p>Los <strong>Example Selectors</strong> son componentes que seleccionan ejemplos relevantes para incluir en prompts, optimizando el contexto proporcionado a los LLMs.</p>
</div>
<div class="ulist">
<div class="title">Los Example Selectors permiten:</div>
<ul>
<li>
<p><strong>Selección dinámica</strong>: Eligen ejemplos basados en similitud semántica o características del input</p>
</li>
<li>
<p><strong>Gestión de contexto</strong>: Controlan la cantidad de ejemplos según límites de longitud</p>
</li>
<li>
<p><strong>Integración con RAG</strong>: Mejoran respuestas en sistemas Retrieval-Augmented Generation</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 8. Tipos principales</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Selector</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Función</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Caso de uso</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">LengthBased</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Selecciona por longitud del texto</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Optimización de tokens</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">MMR</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Balance relevancia/diversidad</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Evitar redundancias</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SemanticSimilarity</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Búsqueda por embeddings</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Respuestas contextualizadas</p></td>
</tr>
</tbody>
</table>
<div class="listingblock">
<div class="title">Implementación con Ollama</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Ejemplo completo usando LengthBasedExampleSelector</span>
<span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">TextLoader</span>
<span class="keyword">from</span> <span class="include">langchain_text_splitters</span> <span class="keyword">import</span> <span class="include">RecursiveCharacterTextSplitter</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">FewShotPromptTemplate</span>, <span class="include">PromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain.prompts.example_selector</span> <span class="keyword">import</span> <span class="include">LengthBasedExampleSelector</span>
<span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>

<span class="comment"># 1. Cargar y dividir documento técnico</span>
loader = TextLoader(<span class="string"><span class="delimiter">&quot;</span><span class="content">data/ia.md</span><span class="delimiter">&quot;</span></span>, encoding=<span class="string"><span class="delimiter">&quot;</span><span class="content">utf-8</span><span class="delimiter">&quot;</span></span>)
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=<span class="integer">500</span>,
    chunk_overlap=<span class="integer">100</span>
)
texts = text_splitter.split_documents(documents)

<span class="comment"># 2. Crear ejemplos demostrativos</span>
examples = [
    {<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>: chunk.page_content[:<span class="integer">100</span>], <span class="string"><span class="delimiter">&quot;</span><span class="content">output</span><span class="delimiter">&quot;</span></span>: chunk.page_content[<span class="integer">100</span>:<span class="integer">200</span>]}
    <span class="keyword">for</span> chunk <span class="keyword">in</span> texts[:<span class="integer">10</span>]
]

<span class="comment"># 3. Configurar selector de ejemplos</span>
example_selector = LengthBasedExampleSelector(
    examples=examples,
    example_prompt=PromptTemplate(
        input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">output</span><span class="delimiter">&quot;</span></span>],
        template=<span class="string"><span class="delimiter">&quot;</span><span class="content">Entrada: {input}</span><span class="char">\n</span><span class="content">Salida: {output}</span><span class="delimiter">&quot;</span></span>
    ),
    max_length=<span class="integer">1000</span>
)

<span class="comment"># 4. Construir prompt dinámico</span>
prompt_template = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=PromptTemplate(
        input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">output</span><span class="delimiter">&quot;</span></span>],
        template=<span class="string"><span class="delimiter">&quot;</span><span class="content">Ejemplo:</span><span class="char">\n</span><span class="content">Entrada: {input}</span><span class="char">\n</span><span class="content">Salida: {output}</span><span class="delimiter">&quot;</span></span>
    ),
    prefix=<span class="string"><span class="delimiter">&quot;</span><span class="content">Analiza estos ejemplos técnicos:</span><span class="delimiter">&quot;</span></span>,
    suffix=<span class="string"><span class="delimiter">&quot;</span><span class="content">Nueva entrada: {input}</span><span class="char">\n</span><span class="content">Genera salida:</span><span class="delimiter">&quot;</span></span>,
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>]
)

<span class="comment"># 5. Integrar con modelo local</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
consulta = prompt_template.format(input=<span class="string"><span class="delimiter">&quot;</span><span class="content">Redes neuronales convolucionales</span><span class="delimiter">&quot;</span></span>)
respuesta = llm.invoke(consulta)

print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta contextualizada:</span><span class="char">\n</span><span class="content">{respuesta}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Flujo de trabajo</div>
<ol class="arabic">
<li>
<p><strong>Carga de datos</strong>: <code>TextLoader</code> ingiere documentos Markdown</p>
</li>
<li>
<p><strong>Preparación de ejemplos</strong>: Splitter crea fragmentos contextuales</p>
</li>
<li>
<p><strong>Selección dinámica</strong>: El selector elige ejemplos por longitud</p>
</li>
<li>
<p><strong>Ensamblaje de prompt</strong>: Combina ejemplos y consulta</p>
</li>
<li>
<p><strong>Generación</strong>: Modelo local produce respuesta contextual</p>
</li>
</ol>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 9. Configuración avanzada</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Parámetro</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Función</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Valor óptimo</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">max_length</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Límite tokens contexto</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1000-2000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">example_count</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ejemplos iniciales</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5-10</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">similarity_threshold</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Filtro relevancia</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.7-0.85</p></td>
</tr>
</tbody>
</table>
<div class="ulist">
<div class="title"><strong>Optimizaciones:</strong></div>
<ul>
<li>
<p>Combinar con embeddings locales para selección semántica</p>
</li>
<li>
<p>Usar <code>MMRExampleSelector</code> para diversidad en respuestas</p>
</li>
<li>
<p>Implementar caché de ejemplos para consultas recurrentes</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_evaluation_en_langchain">2.3.10. Evaluation en LangChain</h4>
<div class="paragraph">
<p>El módulo <strong>Evaluation</strong> permite evaluar cualitativa y cuantitativamente el rendimiento de aplicaciones LLM mediante métricas personalizadas.</p>
</div>
<div class="ulist">
<div class="title">Componente clave para:</div>
<ul>
<li>
<p>Validar respuestas contra estándares de calidad</p>
</li>
<li>
<p>Comparar modelos y configuraciones</p>
</li>
<li>
<p>Detectar <em>drift</em> en producción</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 10. Tipos de evaluadores</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Tipo</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Función</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ejemplo</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Criterios</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Valida cumplimiento de normas</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Concisión, precisión</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">QA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Evalúa respuestas contra referencia</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Exactitud factual</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Custom</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Métricas específicas del dominio</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Coherencia técnica</p></td>
</tr>
</tbody>
</table>
<div class="listingblock">
<div class="title">Implementación con Ollama</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Ejemplo completo: Evaluador técnico con Llama3.2</span>
<span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain.evaluation</span> <span class="keyword">import</span> <span class="include">load_evaluator</span>

<span class="comment"># 1. Configurar modelo local</span>
llm = OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 2. Crear evaluador de coherencia técnica</span>
evaluator = load_evaluator(
    <span class="string"><span class="delimiter">&quot;</span><span class="content">criteria</span><span class="delimiter">&quot;</span></span>,
    llm=llm,
    criteria={
        <span class="string"><span class="delimiter">&quot;</span><span class="content">technical_accuracy</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Exactitud de términos técnicos</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">logical_flow</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Estructura lógica en la explicación</span><span class="delimiter">&quot;</span></span>
    },
    prompt_template=<span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">    Evalúa la respuesta técnica considerando:</span><span class="content">
</span><span class="content">    - Precisión de conceptos</span><span class="content">
</span><span class="content">    - Secuencia lógica</span><span class="content">
</span><span class="content">    Contexto: {input}</span><span class="content">
</span><span class="content">    Respuesta: {prediction}</span><span class="content">
</span><span class="content">    Calificación (1-5):</span><span class="delimiter">&quot;&quot;&quot;</span></span>
)

<span class="comment"># 3. Ejecutar evaluación</span>
ejemplo_tecnico = {
    <span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Explicar backpropagation en redes neuronales</span><span class="delimiter">&quot;</span></span>,
    <span class="string"><span class="delimiter">&quot;</span><span class="content">output</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Algoritmo de optimización que ajusta pesos mediante gradientes descendentes</span><span class="delimiter">&quot;</span></span>
}

resultado = evaluator.invoke(ejemplo_tecnico)
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Keys in result:</span><span class="delimiter">&quot;</span></span>, resultado.keys())
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Full result:</span><span class="delimiter">&quot;</span></span>, resultado)

<span class="comment"># Try different possible key names</span>
<span class="keyword">if</span> <span class="string"><span class="delimiter">'</span><span class="content">score</span><span class="delimiter">'</span></span> <span class="keyword">in</span> resultado:
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Puntaje: {resultado['score']}/5</span><span class="delimiter">&quot;</span></span>)
<span class="keyword">elif</span> <span class="string"><span class="delimiter">'</span><span class="content">value</span><span class="delimiter">'</span></span> <span class="keyword">in</span> resultado:
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Puntaje: {resultado['value']}/5</span><span class="delimiter">&quot;</span></span>)

<span class="keyword">if</span> <span class="string"><span class="delimiter">'</span><span class="content">reasoning</span><span class="delimiter">'</span></span> <span class="keyword">in</span> resultado:
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Razón: {resultado['reasoning']}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Flujo de trabajo</div>
<ol class="arabic">
<li>
<p><strong>Definición de métricas</strong>: Seleccionar criterios relevantes</p>
</li>
<li>
<p><strong>Configuración</strong>: Cargar evaluador con modelo local</p>
</li>
<li>
<p><strong>Ejecución</strong>: Comparar respuestas contra estándares</p>
</li>
<li>
<p><strong>Análisis</strong>: Identificar mejoras con métricas cuantificables</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="title">Evaluación comparativa entre modelos</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain.evaluation</span> <span class="keyword">import</span> <span class="include">load_evaluator</span>

<span class="comment"># Use OllamaLLM instead of Ollama</span>
modelo_base = OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
modelo_mejorado = OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">gemma3</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># Also define llm for the evaluator</span>
llm = OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)

evaluador_comparativo = load_evaluator(
    <span class="string"><span class="delimiter">&quot;</span><span class="content">labeled_pairwise_string</span><span class="delimiter">&quot;</span></span>,
    llm=llm,
    criteria={<span class="string"><span class="delimiter">&quot;</span><span class="content">profundidad</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Nivel de detalle técnico</span><span class="delimiter">&quot;</span></span>}
)

resultado_comparacion = evaluador_comparativo.invoke({
    <span class="string"><span class="delimiter">&quot;</span><span class="content">prediction</span><span class="delimiter">&quot;</span></span>: modelo_mejorado.invoke(<span class="string"><span class="delimiter">&quot;</span><span class="content">Explicar atención en transformers</span><span class="delimiter">&quot;</span></span>),
    <span class="string"><span class="delimiter">&quot;</span><span class="content">prediction_b</span><span class="delimiter">&quot;</span></span>: modelo_base.invoke(<span class="string"><span class="delimiter">&quot;</span><span class="content">Explicar atención en transformers</span><span class="delimiter">&quot;</span></span>),
    <span class="string"><span class="delimiter">&quot;</span><span class="content">reference</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Una explicación técnica ideal sobre el mecanismo de atención</span><span class="delimiter">&quot;</span></span>,
    <span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Mecanismo de atención en NLP</span><span class="delimiter">&quot;</span></span>
})
print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Comparación: {resultado_comparacion}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="ulist">
<div class="title"><strong>Métricas clave para sistemas RAG:</strong></div>
<ul>
<li>
<p>Precisión contextual (<code>context_relevance</code>)</p>
</li>
<li>
<p>Fidelidad a la fuente (<code>faithfulness</code>)</p>
</li>
<li>
<p>Utilidad práctica (<code>practical_utility</code>)</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_flujo_de_trabajo_típico_en_langchain">2.4. Flujo de trabajo típico en LangChain</h3>
<div class="sect3">
<h4 id="_entrada_del_usuario">2.4.1. Entrada del usuario</h4>
<div class="paragraph">
<p>El usuario envía una consulta o solicitud a la aplicación construida con LangChain.</p>
</div>
</div>
<div class="sect3">
<h4 id="_orquestación_por_agentes_y_cadenas">2.4.2. Orquestación por agentes y cadenas</h4>
<div class="ulist">
<ul>
<li>
<p>Un <strong>agente</strong> recibe la entrada y decide cómo procesarla, determinando qué herramientas, cadenas o modelos utilizar según el contexto y la intención del usuario.</p>
</li>
<li>
<p>Una <strong>cadena</strong> (chain) define la secuencia de operaciones a ejecutar. Cada paso puede ser un procesamiento de texto, una llamada a un modelo LLM, o la interacción con fuentes externas de datos.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_uso_de_herramientas_y_conectores">2.4.3. Uso de herramientas y conectores</h4>
<div class="ulist">
<ul>
<li>
<p>El agente o la cadena puede invocar <strong>herramientas</strong> (tools) para tareas específicas como buscar documentos, consultar bases de datos, hacer cálculos o acceder a APIs externas.</p>
</li>
<li>
<p>Si la consulta requiere información adicional, se utilizan <strong>conectores</strong> para recuperar datos relevantes de fuentes externas, como bases de datos, sistemas de archivos o repositorios documentales.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_procesamiento_por_el_modelo_de_lenguaje">2.4.4. Procesamiento por el modelo de lenguaje</h4>
<div class="ulist">
<ul>
<li>
<p>Los datos recopilados y el contexto se envían al <strong>modelo de lenguaje</strong> (LLM) para generar una respuesta o ejecutar una tarea específica.</p>
</li>
<li>
<p>El modelo puede recibir instrucciones personalizadas a través de plantillas de prompt (PromptTemplate).</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_gestión_de_memoria_y_contexto">2.4.5. Gestión de memoria y contexto</h4>
<div class="ulist">
<ul>
<li>
<p><strong>La memoria</strong> almacena el historial de la conversación y el contexto relevante, permitiendo que la aplicación mantenga coherencia en interacciones prolongadas y personalizadas.</p>
</li>
<li>
<p>El agente puede consultar o actualizar la memoria para mejorar la calidad y pertinencia de las respuestas.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_iteración_y_refinamiento">2.4.6. Iteración y refinamiento</h4>
<div class="ulist">
<ul>
<li>
<p>El agente puede evaluar la calidad de la respuesta y, si es necesario, iterar el proceso utilizando herramientas o pasos adicionales hasta obtener un resultado satisfactorio.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_respuesta_al_usuario">2.4.7. Respuesta al usuario</h4>
<div class="ulist">
<ul>
<li>
<p>Finalmente, la respuesta generada se entrega al usuario, cerrando el ciclo del flujo de trabajo.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Esquema visual del flujo</div>
<div class="content">
<pre>Usuario
  ↓
Agente
  ↓
[Cadenas → Herramientas/Conectores → LLM]
  ↓
Memoria (opcional)
  ↓
Iteración/refinamiento (opcional)
  ↓
Respuesta al usuario</pre>
</div>
</div>
<div class="paragraph">
<p>LangChain destaca por su arquitectura modular y flexible, que permite orquestar de manera dinámica cadenas, agentes, herramientas y memoria para crear aplicaciones inteligentes, adaptables y contextuales.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_introducción_al_encadenamiento_y_ejemplos_básicos">2.5. Introducción al encadenamiento y ejemplos básicos</h3>
<div class="sect3">
<h4 id="_qué_es_el_encadenamiento_en_langchain">2.5.1. ¿Qué es el encadenamiento en LangChain?</h4>
<div class="paragraph">
<p>El encadenamiento (chaining) en LangChain consiste en conectar varios modelos de lenguaje (LLMs), prompts y herramientas, de manera que la salida de un componente se utiliza como entrada del siguiente. Esto permite dividir tareas complejas en pasos más pequeños y manejables, guiando al modelo a través de un proceso estructurado y mejorando la precisión, claridad y explicabilidad de las respuestas.</p>
</div>
<div class="ulist">
<div class="title">Ventajas del encadenamiento</div>
<ul>
<li>
<p>Descompone tareas complejas en subtareas simples y secuenciales.</p>
</li>
<li>
<p>Mejora la precisión y relevancia de las respuestas al guiar el razonamiento del LLM paso a paso.</p>
</li>
<li>
<p>Hace el proceso más transparente y fácil de depurar.</p>
</li>
<li>
<p>Permite flujos de trabajo flexibles, incluyendo bifurcaciones y lógica condicional.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Tipos de cadenas en LangChain</div>
<ul>
<li>
<p><strong>SimpleSequentialChain:</strong> Ejecuta una secuencia lineal de pasos, donde la salida de cada uno es la entrada del siguiente.</p>
</li>
<li>
<p><strong>SequentialChain:</strong> Permite manejar múltiples entradas y salidas, así como pasos intermedios más complejos y condicionales.</p>
</li>
<li>
<p><strong>Conditional Chains:</strong> Introducen lógica condicional, permitiendo bifurcar el flujo según la salida de un paso.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Ejemplo básico: SimpleSequentialChain</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">LLMChain</span>, <span class="include">SimpleSequentialChain</span>

<span class="comment"># 1. Definir los prompts</span>
prompt1 = PromptTemplate(
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">concepto</span><span class="delimiter">&quot;</span></span>],
    template=<span class="string"><span class="delimiter">&quot;</span><span class="content">Explica brevemente el concepto de {concepto} en IA.</span><span class="delimiter">&quot;</span></span>
)

prompt2 = PromptTemplate(
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">concepto</span><span class="delimiter">&quot;</span></span>],
    template=<span class="string"><span class="delimiter">&quot;</span><span class="content">Resume la explicación anterior de {concepto} como si tuvieras que explicárselo a un niño de 5 años.</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 2. Crear el modelo local</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 3. Crear las cadenas individuales</span>
chain1 = LLMChain(llm=llm, prompt=prompt1)
chain2 = LLMChain(llm=llm, prompt=prompt2)

<span class="comment"># 4. Unirlas en una SimpleSequentialChain</span>
sequential_chain = SimpleSequentialChain(
    chains=[chain1, chain2],
    verbose=<span class="predefined-constant">True</span>
)

<span class="comment"># 5. Ejecutar la cadena secuencial</span>
resultado = sequential_chain.run(<span class="string"><span class="delimiter">&quot;</span><span class="content">aprendizaje automático</span><span class="delimiter">&quot;</span></span>)
print(resultado)</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Ejemplo avanzado: SequentialChain con múltiples pasos</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">LLMChain</span>, <span class="include">SequentialChain</span>

<span class="comment"># 1. Configurar modelos locales</span>
llm_tecnico = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
llm_simple = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">gemma3</span><span class="delimiter">&quot;</span></span>)
llm_evaluador = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 2. Definir cadenas individuales</span>
<span class="comment">## Generación técnica</span>
prompt_tecnico = PromptTemplate(
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">tema</span><span class="delimiter">&quot;</span></span>],
    template=<span class="string"><span class="delimiter">&quot;</span><span class="content">Genera explicación técnica detallada sobre {tema}</span><span class="delimiter">&quot;</span></span>
)
chain_tecnica = LLMChain(llm=llm_tecnico, prompt=prompt_tecnico, output_key=<span class="string"><span class="delimiter">&quot;</span><span class="content">explicacion_tecnica</span><span class="delimiter">&quot;</span></span>)

<span class="comment">## Simplificación</span>
prompt_simple = PromptTemplate(
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">explicacion_tecnica</span><span class="delimiter">&quot;</span></span>],
    template=<span class="string"><span class="delimiter">&quot;</span><span class="content">Simplifica este texto para principiantes: {explicacion_tecnica}</span><span class="delimiter">&quot;</span></span>
)
chain_simple = LLMChain(llm=llm_simple, prompt=prompt_simple, output_key=<span class="string"><span class="delimiter">&quot;</span><span class="content">explicacion_simple</span><span class="delimiter">&quot;</span></span>)

<span class="comment">## Evaluación</span>
prompt_eval = PromptTemplate(
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">explicacion_tecnica</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">explicacion_simple</span><span class="delimiter">&quot;</span></span>],
    template=<span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">    Evalúa la calidad de estas explicaciones:</span><span class="content">
</span><span class="content">    Técnica: {explicacion_tecnica}</span><span class="content">
</span><span class="content">    Simple: {explicacion_simple}</span><span class="content">
</span><span class="content">    Calificación (1-10) y razones:</span><span class="content">
</span><span class="content">    </span><span class="delimiter">&quot;&quot;&quot;</span></span>
)
chain_eval = LLMChain(llm=llm_evaluador, prompt=prompt_eval, output_key=<span class="string"><span class="delimiter">&quot;</span><span class="content">evaluacion</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 3. Crear SequentialChain</span>
pipeline_completo = SequentialChain(
    chains=[chain_tecnica, chain_simple, chain_eval],
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">tema</span><span class="delimiter">&quot;</span></span>],
    output_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">explicacion_tecnica</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">explicacion_simple</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">evaluacion</span><span class="delimiter">&quot;</span></span>],
    verbose=<span class="predefined-constant">True</span>
)

<span class="comment"># 4. Ejecutar flujo completo</span>
resultado = pipeline_completo({<span class="string"><span class="delimiter">&quot;</span><span class="content">tema</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">mecanismo de atención en transformers</span><span class="delimiter">&quot;</span></span>})
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Explicación técnica:</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, resultado[<span class="string"><span class="delimiter">&quot;</span><span class="content">explicacion_tecnica</span><span class="delimiter">&quot;</span></span>])
print(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Explicación simple:</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, resultado[<span class="string"><span class="delimiter">&quot;</span><span class="content">explicacion_simple</span><span class="delimiter">&quot;</span></span>])
print(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Evaluación:</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, resultado[<span class="string"><span class="delimiter">&quot;</span><span class="content">evaluacion</span><span class="delimiter">&quot;</span></span>])</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Ejemplo avanzado: encadenamiento condicional con Conditional Chains</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain.schema.runnable</span> <span class="keyword">import</span> <span class="include">RunnableBranch</span>

<span class="comment"># 1. Definir subcadenas especializadas</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)

cadena_tecnica = (
    PromptTemplate.from_template(<span class="string"><span class="delimiter">&quot;</span><span class="content">Responde como experto: {input}</span><span class="delimiter">&quot;</span></span>)
    | llm
)

cadena_simple = (
    PromptTemplate.from_template(<span class="string"><span class="delimiter">&quot;</span><span class="content">Explica como a un niño: {input}</span><span class="delimiter">&quot;</span></span>)
    | llm
)

<span class="comment"># 2. Función condicional</span>
<span class="keyword">def</span> <span class="function">necesita_explicacion_compleja</span>(input_dict):
    texto = input_dict[<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>].lower()
    <span class="keyword">return</span> <span class="predefined">any</span>(palabra <span class="keyword">in</span> texto <span class="keyword">for</span> palabra <span class="keyword">in</span> [<span class="string"><span class="delimiter">&quot;</span><span class="content">avanzado</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">técnico</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">detallado</span><span class="delimiter">&quot;</span></span>])

<span class="comment"># 3. Crear cadena ramificada</span>
cadena_condicional = RunnableBranch(
    (necesita_explicacion_compleja, cadena_tecnica),
    cadena_simple
)

<span class="comment"># 4. Ejecución condicional</span>
consulta_tecnica = {<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Explica el mecanismo de atención en transformers (respuesta técnica avanzada)</span><span class="delimiter">&quot;</span></span>}
consulta_simple = {<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">¿Qué es un transformer? Explica simple</span><span class="delimiter">&quot;</span></span>}

print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta técnica:</span><span class="delimiter">&quot;</span></span>, cadena_condicional.invoke(consulta_tecnica))
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta simple:</span><span class="delimiter">&quot;</span></span>, cadena_condicional.invoke(consulta_simple))</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_modelos_y_prompts">3. Modelos y Prompts</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_interacción_con_llms_y_modelos_de_chat">3.1. Interacción con LLMs y modelos de chat</h3>
<div class="sect3">
<h4 id="_qué_es_un_llm_y_cómo_interactúa">3.1.1. ¿Qué es un LLM y cómo interactúa?</h4>
<div class="paragraph">
<p>Un LLM (Large Language Model) es un modelo de lenguaje entrenado con grandes cantidades de texto para comprender, procesar y generar lenguaje natural. Funciona en tres etapas principales: tokenización (divide el texto en unidades pequeñas llamadas tokens), entrenamiento (aprende patrones y relaciones lingüísticas) y generación de respuestas (produce texto relevante y coherente en función del contexto de entrada).</p>
</div>
</div>
<div class="sect3">
<h4 id="_modelos_de_chat_estructura_y_ventajas">3.1.2. Modelos de chat: estructura y ventajas</h4>
<div class="paragraph">
<p>Los modelos de chat son una variante de los LLM que utilizan una estructura de mensajes para simular conversaciones realistas. Esta estructura incluye:
- <strong>Mensajes del sistema:</strong> Instrucciones para guiar el comportamiento del modelo.
- <strong>Mensajes de usuario:</strong> Preguntas o solicitudes del usuario.
- <strong>Mensajes de IA:</strong> Respuestas generadas por el modelo.</p>
</div>
<div class="paragraph">
<p>Ventajas de los modelos de chat:
- Interacción dinámica y realista.
- Simulación de conversaciones completas.
- Respuestas coherentes y relevantes al contexto.
- Útiles para chatbots, asistentes virtuales y sistemas de recomendación.</p>
</div>
</div>
<div class="sect3">
<h4 id="_personalización_y_ajuste_fine_tuning">3.1.3. Personalización y ajuste (fine-tuning)</h4>
<div class="paragraph">
<p>Para tareas generales, los modelos preentrenados suelen ser suficientes. Sin embargo, para dominios específicos o aplicaciones empresariales, es recomendable afinar el modelo añadiendo datos adicionales (ejemplos de conversaciones, preguntas frecuentes, etc.) y realizando pruebas para garantizar la pertinencia y precisión de las respuestas. Además, técnicas como prompt engineering o adaptadores permiten personalizar el comportamiento del modelo sin necesidad de un reentrenamiento completo.</p>
</div>
<div class="listingblock">
<div class="title">Ejemplo básico de interacción con un modelo de chat en LangChain</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.chat_models.ollama</span> <span class="keyword">import</span> <span class="include">ChatOllama</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">ChatPromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">LLMChain</span>

<span class="comment"># 1. Configurar modelo local</span>
llm = ChatOllama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Versión 1B para uso eficiente</span>
    temperature=<span class="float">0.7</span>  <span class="comment"># Balance entre creatividad y precisión</span>
)

<span class="comment"># 2. Crear plantilla de chat</span>
prompt_template = ChatPromptTemplate.from_messages([
    (<span class="string"><span class="delimiter">&quot;</span><span class="content">system</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">Eres un asistente técnico especializado en IA.</span><span class="delimiter">&quot;</span></span>),
    (<span class="string"><span class="delimiter">&quot;</span><span class="content">human</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">{input}</span><span class="delimiter">&quot;</span></span>)
])

<span class="comment"># 3. Construir cadena de conversación</span>
chat_chain = LLMChain(
    llm=llm,
    prompt=prompt_template
)

<span class="comment"># 4. Ejecutar interacción</span>
<span class="keyword">while</span> <span class="predefined-constant">True</span>:
    user_input = <span class="predefined">input</span>(<span class="string"><span class="delimiter">&quot;</span><span class="content">Tú: </span><span class="delimiter">&quot;</span></span>)
    <span class="keyword">if</span> user_input.lower() == <span class="string"><span class="delimiter">'</span><span class="content">salir</span><span class="delimiter">'</span></span>:
        <span class="keyword">break</span>
    response = chat_chain.invoke({<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>: user_input})
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Asistente: {response['text']}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Mejores prácticas</div>
<ul>
<li>
<p>Sé explícito y claro en las preguntas para obtener respuestas útiles y precisas.</p>
</li>
<li>
<p>Utiliza mensajes del sistema para contextualizar el modelo según el dominio o la tarea.</p>
</li>
<li>
<p>Proporciona ejemplos y datos de base si necesitas respuestas especializadas.</p>
</li>
<li>
<p>Combina NLP tradicional para preguntas simples y LLMs para conversaciones complejas y adaptativas.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_plantillas_de_prompts_y_técnicas_de_prompt_engineering">3.2. Plantillas de prompts y técnicas de prompt engineering</h3>
<div class="sect3">
<h4 id="_qué_es_una_plantilla_de_prompt_en_langchain">3.2.1. ¿Qué es una plantilla de prompt en LangChain?</h4>
<div class="paragraph">
<p>Una plantilla de prompt (PromptTemplate) es una estructura que define cómo se construye la instrucción que se enviará a un modelo de lenguaje. Utiliza variables dinámicas y permite reutilizar, adaptar y gestionar prompts de forma eficiente y segura.</p>
</div>
<div class="listingblock">
<div class="title">Un ejemplo básico de plantilla de prompt</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.chat_models</span> <span class="keyword">import</span> <span class="include">ChatOllama</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">ChatPromptTemplate</span>

<span class="comment"># 1. Plantilla conversacional estructurada</span>
template = ChatPromptTemplate.from_messages([
    (<span class="string"><span class="delimiter">&quot;</span><span class="content">system</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">Eres un experto en IA especializado en NLP. Responde de forma técnica pero clara.</span><span class="delimiter">&quot;</span></span>),
    (<span class="string"><span class="delimiter">&quot;</span><span class="content">human</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">Explica el concepto de {concepto} con un ejemplo práctico de código en {lenguaje}.</span><span class="delimiter">&quot;</span></span>)
])

<span class="comment"># 2. Configurar modelo local</span>
llm = ChatOllama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,
    temperature=<span class="float">0.3</span>  <span class="comment"># Control de creatividad</span>
)

<span class="comment"># 3. Crear cadena conversacional</span>
chain = template | llm

<span class="comment"># 4. Ejecutar con parámetros dinámicos</span>
response = chain.invoke({
    <span class="string"><span class="delimiter">&quot;</span><span class="content">concepto</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">attention mechanism</span><span class="delimiter">&quot;</span></span>,
    <span class="string"><span class="delimiter">&quot;</span><span class="content">lenguaje</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Python</span><span class="delimiter">&quot;</span></span>
})

print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta guiada:</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, response.content)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Las plantillas pueden tener cualquier número de variables de entrada, y se pueden combinar para crear prompts más complejos o adaptables a diferentes tareas.</p>
</div>
<div class="ulist">
<div class="title">Técnicas clave de prompt engineering</div>
<ul>
<li>
<p><strong>Zero-shot prompting:</strong> El modelo recibe solo la instrucción, sin ejemplos previos.</p>
</li>
<li>
<p><strong>Few-shot prompting:</strong> Se incluyen ejemplos en el prompt para guiar la respuesta del modelo.</p>
</li>
<li>
<p><strong>Chain-of-thought prompting:</strong> El prompt guía al modelo a razonar paso a paso, útil para problemas complejos.</p>
</li>
<li>
<p><strong>Meta prompting:</strong> Se estructura el prompt en pasos lógicos o abstractos, ayudando al modelo a generalizar y optimizando el uso de tokens.</p>
</li>
<li>
<p><strong>Prompt composition:</strong> Combinación de varias plantillas para tareas complejas o flujos conversacionales.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Buenas prácticas para crear prompts efectivos</div>
<ul>
<li>
<p>Proporciona contexto claro y detallado sobre la tarea.</p>
</li>
<li>
<p>Personaliza el prompt para cada caso de uso, incluyendo términos o formatos relevantes.</p>
</li>
<li>
<p>Divide tareas complejas en pasos secuenciales y explícitos.</p>
</li>
<li>
<p>Especifica el formato, tono y longitud de la respuesta deseada.</p>
</li>
<li>
<p>Incluye ejemplos cuando sea necesario para orientar la respuesta (few-shot).</p>
</li>
<li>
<p>Valida y limpia las entradas antes de enviarlas al modelo.</p>
</li>
<li>
<p>Itera y ajusta los prompts en función de la calidad de las respuestas.</p>
</li>
<li>
<p>Utiliza versionado y pruebas continuas para mantener la calidad en producción.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Ejemplo avanzado: plantilla few-shot en LangChain</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">FewShotPromptTemplate</span>, <span class="include">PromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain.prompts.example_selector</span> <span class="keyword">import</span> <span class="include">LengthBasedExampleSelector</span>

<span class="comment"># 1. Definir ejemplos técnicos</span>
ejemplos_ia = [
    {
        <span class="string"><span class="delimiter">&quot;</span><span class="content">pregunta</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">¿Qué es el aprendizaje por refuerzo?</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">respuesta</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Paradigma donde un agente aprende mediante interacción y recompensas. Ej: AlphaGo</span><span class="delimiter">&quot;</span></span>
    },
    {
        <span class="string"><span class="delimiter">&quot;</span><span class="content">pregunta</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Explica las GANs</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">respuesta</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Redes generativas adversariales: Dos redes (generador/discriminador) compitiendo. Aplicación: Generación de imágenes</span><span class="delimiter">&quot;</span></span>
    }
]

<span class="comment"># 2. Crear plantilla de ejemplo</span>
plantilla_ejemplo = PromptTemplate(
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">pregunta</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">respuesta</span><span class="delimiter">&quot;</span></span>],
    template=<span class="string"><span class="delimiter">&quot;</span><span class="content">P: {pregunta}</span><span class="char">\n</span><span class="content">R: {respuesta}</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 3. Configurar selector dinámico</span>
selector = LengthBasedExampleSelector(
    examples=ejemplos_ia,
    example_prompt=plantilla_ejemplo,
    max_length=<span class="integer">300</span>  <span class="comment"># Límite de tokens para contexto</span>
)

<span class="comment"># 4. Construir plantilla Few-Shot</span>
plantilla_final = FewShotPromptTemplate(
    example_selector=selector,
    example_prompt=plantilla_ejemplo,
    prefix=<span class="string"><span class="delimiter">&quot;</span><span class="content">Eres un experto en IA. Responde con ejemplos técnicos:</span><span class="delimiter">&quot;</span></span>,
    suffix=<span class="string"><span class="delimiter">&quot;</span><span class="content">P: {input}</span><span class="char">\n</span><span class="content">R:</span><span class="delimiter">&quot;</span></span>,
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>],
    example_separator=<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 5. Integrar con modelo local</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
cadena = plantilla_final | llm

<span class="comment"># 6. Ejecutar con consulta técnica</span>
consulta = <span class="string"><span class="delimiter">&quot;</span><span class="content">Explica el mecanismo de atención en transformers</span><span class="delimiter">&quot;</span></span>
respuesta = cadena.invoke({<span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>: consulta})

print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Prompt generado:</span><span class="char">\n</span><span class="content">{plantilla_final.format(input=consulta)}</span><span class="delimiter">&quot;</span></span>)
print(f<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Respuesta del modelo:</span><span class="char">\n</span><span class="content">{respuesta}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_procesamiento_y_parseo_de_la_salida_del_modelo_en_langchain">3.3. Procesamiento y parseo de la salida del modelo en LangChain</h3>
<div class="sect3">
<h4 id="_por_qué_es_necesario_el_parseo_de_salidas">3.3.1. ¿Por qué es necesario el parseo de salidas?</h4>
<div class="paragraph">
<p>Los modelos de lenguaje generan texto no estructurado, pero las aplicaciones reales requieren datos estructurados. LangChain ofrece <em>Output Parsers</em> para convertir respuestas textuales en formatos útiles como JSON, listas u objetos Python.</p>
</div>
</div>
<div class="sect3">
<h4 id="_tipos_principales_de_parsers">3.3.2. Tipos principales de parsers</h4>
<div class="paragraph">
<p><strong>1. PydanticOutputParser</strong>
.Transforma respuestas en objetos Pydantic validados con PydanticOutputParser</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain_core.prompts</span> <span class="keyword">import</span> <span class="include">ChatPromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain_core.output_parsers</span> <span class="keyword">import</span> <span class="include">PydanticOutputParser</span>
<span class="keyword">from</span> <span class="include">pydantic</span> <span class="keyword">import</span> <span class="include">BaseModel</span>, <span class="include">Field</span>

<span class="comment"># 1. Definir modelo Pydantic</span>
<span class="keyword">class</span> <span class="class">ConceptoTecnico</span>(BaseModel):
    nombre: str = Field(description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Nombre del concepto técnico</span><span class="delimiter">&quot;</span></span>)
    explicacion: str = Field(description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Explicación detallada en 50 palabras</span><span class="delimiter">&quot;</span></span>)
    aplicaciones: <span class="predefined">list</span>[<span class="predefined">str</span>] = Field(description=<span class="string"><span class="delimiter">&quot;</span><span class="content">3 aplicaciones prácticas</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 2. Configurar parser</span>
parser = PydanticOutputParser(pydantic_object=ConceptoTecnico)

<span class="comment"># 3. Crear prompt más específico</span>
prompt = ChatPromptTemplate.from_messages([
    (<span class="string"><span class="delimiter">&quot;</span><span class="content">system</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">    Eres un experto técnico. Responde con un JSON válido que contenga datos REALES, no un esquema.</span><span class="content">
</span><span class="content">
</span><span class="content">    IMPORTANTE: Devuelve únicamente el JSON con los datos, sin texto adicional.</span><span class="content">
</span><span class="content">
</span><span class="content">    {format_instructions}</span><span class="content">
</span><span class="content">
</span><span class="content">    Ejemplo de respuesta correcta:</span><span class="content">
</span><span class="content">    {{</span><span class="content">
</span><span class="content">        &quot;nombre&quot;: &quot;Redes Neuronales Convolucionales&quot;,</span><span class="content">
</span><span class="content">        &quot;explicacion&quot;: &quot;Tipo de red neuronal especialmente eficaz para procesamiento de imágenes que utiliza operaciones de convolución para detectar características locales como bordes, texturas y patrones, manteniendo la información espacial de los datos de entrada.&quot;,</span><span class="content">
</span><span class="content">        &quot;aplicaciones&quot;: [&quot;Reconocimiento de imágenes&quot;, &quot;Diagnóstico médico por imagen&quot;, &quot;Vehículos autónomos&quot;]</span><span class="content">
</span><span class="content">    }}</span><span class="content">
</span><span class="content">    </span><span class="delimiter">&quot;&quot;&quot;</span></span>),
    (<span class="string"><span class="delimiter">&quot;</span><span class="content">human</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">Explica el concepto de {concepto} en IA</span><span class="delimiter">&quot;</span></span>)
]).partial(format_instructions=parser.get_format_instructions())

<span class="comment"># 4. Cadena completa con modelo local actualizado</span>
model = OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
chain = prompt | model | parser

<span class="comment"># 5. Ejecutar y validar</span>
resultado = chain.invoke({<span class="string"><span class="delimiter">&quot;</span><span class="content">concepto</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">redes neuronales convolucionales</span><span class="delimiter">&quot;</span></span>})
print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Objeto validado: {resultado}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>2. StructuredOutputParser</strong>
Para esquemas dinámicos sin clases Pydantic:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain.output_parsers</span> <span class="keyword">import</span> <span class="include">StructuredOutputParser</span>, <span class="include">ResponseSchema</span>
<span class="keyword">from</span> <span class="include">pydantic</span> <span class="keyword">import</span> <span class="include">BaseModel</span>, <span class="include">ValidationError</span>, <span class="include">Field</span>

<span class="comment"># 1. Definir esquema Pydantic</span>
<span class="keyword">class</span> <span class="class">ConceptoTecnico</span>(BaseModel):
    nombre: <span class="predefined">str</span>
    explicacion: <span class="predefined">str</span>
    aplicaciones: <span class="predefined">list</span>[<span class="predefined">str</span>]
    complejidad: int = Field(ge=<span class="integer">1</span>, le=<span class="integer">5</span>)

<span class="comment"># 2. Configurar ResponseSchemas</span>
response_schemas = [
    ResponseSchema(name=<span class="string"><span class="delimiter">&quot;</span><span class="content">nombre</span><span class="delimiter">&quot;</span></span>, description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Nombre del concepto técnico</span><span class="delimiter">&quot;</span></span>),
    ResponseSchema(name=<span class="string"><span class="delimiter">&quot;</span><span class="content">explicacion</span><span class="delimiter">&quot;</span></span>, description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Explicación en 50 palabras</span><span class="delimiter">&quot;</span></span>),
    ResponseSchema(name=<span class="string"><span class="delimiter">&quot;</span><span class="content">aplicaciones</span><span class="delimiter">&quot;</span></span>, description=<span class="string"><span class="delimiter">&quot;</span><span class="content">3 aplicaciones prácticas</span><span class="delimiter">&quot;</span></span>, type=<span class="string"><span class="delimiter">&quot;</span><span class="content">list[string]</span><span class="delimiter">&quot;</span></span>),
    ResponseSchema(name=<span class="string"><span class="delimiter">&quot;</span><span class="content">complejidad</span><span class="delimiter">&quot;</span></span>, description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Nivel de complejidad (1-5)</span><span class="delimiter">&quot;</span></span>, type=<span class="string"><span class="delimiter">&quot;</span><span class="content">integer</span><span class="delimiter">&quot;</span></span>)
]

<span class="comment"># 3. Crear parser y prompt</span>
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
format_instructions = output_parser.get_format_instructions()

prompt = PromptTemplate(
    template=<span class="string"><span class="delimiter">&quot;</span><span class="content">Explica {tema} en IA usando este formato:</span><span class="char">\n</span><span class="content">{format_instructions}</span><span class="delimiter">&quot;</span></span>,
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">tema</span><span class="delimiter">&quot;</span></span>],
    partial_variables={<span class="string"><span class="delimiter">&quot;</span><span class="content">format_instructions</span><span class="delimiter">&quot;</span></span>: format_instructions}
)

<span class="comment"># 4. Cadena completa con validación</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
chain = prompt | llm | output_parser

<span class="comment"># 5. Ejecutar y validar</span>
<span class="keyword">try</span>:
    resultado = chain.invoke({<span class="string"><span class="delimiter">&quot;</span><span class="content">tema</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">redes neuronales recurrentes</span><span class="delimiter">&quot;</span></span>})
    concepto = ConceptoTecnico(**resultado)
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Objeto validado:</span><span class="char">\n</span><span class="content">{concepto}</span><span class="delimiter">&quot;</span></span>)
<span class="keyword">except</span> ValidationError <span class="keyword">as</span> e:
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Error de validación: {e}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_serialización_y_reutilización_de_prompts_en_langchain">3.4. Serialización y reutilización de prompts en LangChain</h3>
<div class="paragraph">
<p>Serializar prompts permite guardar, compartir, versionar y reutilizar plantillas de instrucciones fuera del código Python, facilitando la colaboración y el mantenimiento en proyectos de IA. LangChain soporta formatos legibles como JSON y YAML, ideales para este propósito.</p>
</div>
<div class="sect3">
<h4 id="_opciones_y_formatos_soportados">3.4.1. Opciones y formatos soportados</h4>
<div class="ulist">
<ul>
<li>
<p><strong>JSON y YAML:</strong> Ambos formatos son soportados para prompts, cadenas y otros objetos serializables de LangChain.</p>
</li>
<li>
<p><strong>Almacenamiento flexible:</strong> Puedes guardar todo en un solo archivo o separar plantillas, ejemplos y componentes en archivos distintos para mayor modularida.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Guardar un prompt o cadena en disco</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain.llms</span> <span class="keyword">import</span> <span class="include">OpenAI</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">LLMChain</span>

<span class="comment"># Crear plantilla y cadena</span>
prompt = PromptTemplate(template=<span class="string"><span class="delimiter">&quot;</span><span class="content">Pregunta: {question}</span><span class="char">\n</span><span class="content">Respuesta:</span><span class="delimiter">&quot;</span></span>, input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">question</span><span class="delimiter">&quot;</span></span>])
llm_chain = LLMChain(prompt=prompt, llm=OpenAI(model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">text-davinci-003</span><span class="delimiter">&quot;</span></span>))

<span class="comment"># Guardar la cadena en JSON</span>
llm_chain.save(<span class="string"><span class="delimiter">&quot;</span><span class="content">llm_chain.json</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Cargar un prompt o cadena desde disco</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">load_chain</span>

<span class="comment"># Cargar la cadena desde el archivo JSON</span>
llm_chain = load_chain(<span class="string"><span class="delimiter">&quot;</span><span class="content">llm_chain.json</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Serialización y deserialización avanzada</div>
<ul>
<li>
<p><strong>API de bajo nivel:</strong> Usa <code>dumpd</code>, <code>dumps</code>, <code>load</code>, y <code>loads</code> para serializar objetos como diccionarios o cadenas JSON, y volver a cargarlos en memoria.</p>
</li>
<li>
<p><strong>Separación de secretos:</strong> Las claves API y otros secretos no se guardan en los archivos serializados; se deben proporcionar al cargar usando el parámetro <code>secrets_map</code>.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Un ejemplo de serialización y deserialización</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_core.load</span> <span class="keyword">import</span> <span class="include">dumps</span>, <span class="include">loads</span>

<span class="comment"># Serializar a string JSON</span>
json_str = dumps(llm_chain, pretty=<span class="predefined-constant">True</span>)

<span class="comment"># Deserializar desde string JSON, añadiendo secretos</span>
llm_chain = loads(json_str, secrets_map={<span class="string"><span class="delimiter">&quot;</span><span class="content">OPENAI_API_KEY</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">tu-api-key</span><span class="delimiter">&quot;</span></span>})</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Serialización de prompts individuales</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>

prompt = PromptTemplate(template=<span class="string"><span class="delimiter">&quot;</span><span class="content">Dime un chiste sobre {topic}</span><span class="delimiter">&quot;</span></span>, input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">topic</span><span class="delimiter">&quot;</span></span>])
prompt.save(<span class="string"><span class="delimiter">&quot;</span><span class="content">prompt_template.json</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># Cargar prompt</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">load_prompt</span>
prompt_loaded = load_prompt(<span class="string"><span class="delimiter">&quot;</span><span class="content">prompt_template.json</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_reutilización_y_composición_de_prompts">3.4.2. Reutilización y composición de prompts</h4>
<div class="ulist">
<ul>
<li>
<p>Puedes componer prompts complejos a partir de plantillas reutilizables usando <code>PipelinePromptTemplate</code>.</p>
</li>
<li>
<p>Para adaptar prompts a distintas variables, puedes crear variantes manualmente o extender la clase para soportar alias de variables.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Un ejemplo de composición de prompts con <code>PipelinePromptTemplate</code></div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">PipelinePromptTemplate</span>, <span class="include">PromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>

<span class="comment"># 1. Definir plantillas base</span>
plantilla_intro = PromptTemplate.from_template(
    <span class="string"><span class="delimiter">&quot;</span><span class="content">Eres un experto en {tema}. Explica el concepto clave:</span><span class="delimiter">&quot;</span></span>
)

plantilla_ejemplo = PromptTemplate.from_template(<span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">{intro}</span><span class="content">
</span><span class="content">Proporciona un ejemplo práctico en {lenguaje} relacionado con:</span><span class="content">
</span><span class="content">{concepto}</span><span class="content">
</span><span class="delimiter">&quot;&quot;&quot;</span></span>)

<span class="comment"># 2. Configurar pipeline</span>
pipeline_prompts = [
    (<span class="string"><span class="delimiter">&quot;</span><span class="content">intro</span><span class="delimiter">&quot;</span></span>, plantilla_intro),
    (<span class="string"><span class="delimiter">&quot;</span><span class="content">ejemplo</span><span class="delimiter">&quot;</span></span>, plantilla_ejemplo)
]

prompt_final = PipelinePromptTemplate(
    final_prompt=plantilla_ejemplo,
    pipeline_prompts=pipeline_prompts
)

<span class="comment"># 3. Ejecutar con modelo local</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
input_vars = {<span class="string"><span class="delimiter">&quot;</span><span class="content">tema</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">redes neuronales</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">lenguaje</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Python</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">concepto</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">backpropagation</span><span class="delimiter">&quot;</span></span>}
respuesta = llm.invoke(prompt_final.format(**input_vars))

print(respuesta)</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Buenas prácticas</div>
<ul>
<li>
<p>Versiona tus prompts guardándolos en archivos separados y usando control de versiones.</p>
</li>
<li>
<p>Utiliza formatos legibles para facilitar revisiones y colaboración.</p>
</li>
<li>
<p>Carga prompts con <code>load_prompt</code> para mantener la compatibilidad y simplicidad en tu flujo de trabajo.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conectores_y_fuentes_de_datos">4. Conectores y Fuentes de Datos</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_cargadores_de_documentos_en_langchain_pdf_txt_web_y_apis_externas">4.1. Cargadores de documentos en LangChain: PDF, TXT, web y APIs externas</h3>
<div class="paragraph">
<p>LangChain ofrece una amplia variedad de cargadores (Document Loaders) para importar información desde diferentes fuentes y formatos al formato estándar de Documentos de LangChain. Estos cargadores permiten trabajar con archivos PDF, TXT, páginas web y datos provenientes de APIs externas, facilitando la integración y procesamiento de información en aplicaciones de IA.</p>
</div>
<div class="sect3">
<h4 id="_txt_archivos_de_texto_plano">4.1.1. TXT: Archivos de texto plano</h4>
<div class="listingblock">
<div class="title">Utiliza <code>TextLoader</code> para cargar archivos <code>.txt</code> de forma sencilla.</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">TextLoader</span>

<span class="comment"># 1. Especifica la ruta al archivo .txt</span>
ruta_archivo = <span class="string"><span class="delimiter">&quot;</span><span class="content">data/sample.txt</span><span class="delimiter">&quot;</span></span>

<span class="comment"># 2. Carga el archivo en documentos LangChain</span>
loader = TextLoader(ruta_archivo, encoding=<span class="string"><span class="delimiter">&quot;</span><span class="content">utf-8</span><span class="delimiter">&quot;</span></span>)
documentos = loader.load()

<span class="comment"># 3. Acceso al contenido cargado</span>
<span class="keyword">for</span> doc <span class="keyword">in</span> documentos:
    print(doc.page_content)</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Admite autodetección de encoding y carga perezosa (lazy loading).</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_pdf_documentos_pdf">4.1.2. PDF: Documentos PDF</h4>
<div class="listingblock">
<div class="title">Para PDF, se recomienda <code>PyPDFLoader</code>, que permite extraer texto página por página e incluso imágenes si se requiere.</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">PyPDFLoader</span>

loader = PyPDFLoader(<span class="string"><span class="delimiter">&quot;</span><span class="content">ruta/al/archivo.pdf</span><span class="delimiter">&quot;</span></span>, extract_images=<span class="predefined-constant">False</span>)
docs = loader.load()
print(docs[<span class="integer">0</span>].page_content)</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Soporta carga asíncrona y extracción avanzada de contenido.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_web_páginas_y_sitios_web">4.1.3. Web: Páginas y sitios web</h4>
<div class="listingblock">
<div class="title">Ejemplo de uso de <code>WebBaseLoader</code> para cargar contenido de páginas web.</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">WebBaseLoader</span>

<span class="comment"># Cargar una o varias páginas web</span>
loader = WebBaseLoader([
    <span class="string"><span class="delimiter">&quot;</span><span class="content">https://www.example.com/</span><span class="delimiter">&quot;</span></span>,
    <span class="string"><span class="delimiter">&quot;</span><span class="content">https://www.python.org/</span><span class="delimiter">&quot;</span></span>
])
documentos = loader.load()
<span class="keyword">for</span> doc <span class="keyword">in</span> documentos:
    print(doc.page_content[:<span class="integer">200</span>])  <span class="comment"># Muestra los primeros 200 caracteres</span></code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Ejemplo de uso de <code>UnstructuredLoader</code> para cargar contenido web de forma estructurada.</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">asyncio</span>
<span class="keyword">from</span> <span class="include">langchain_unstructured</span> <span class="keyword">import</span> <span class="include">UnstructuredLoader</span>

async <span class="keyword">def</span> <span class="function">main</span>():
    loader = UnstructuredLoader(web_url=<span class="string"><span class="delimiter">&quot;</span><span class="content">https://python.langchain.com/docs/how_to/chatbots_memory/</span><span class="delimiter">&quot;</span></span>)
    docs = []
    async <span class="keyword">for</span> doc <span class="keyword">in</span> loader.alazy_load():
        docs.append(doc)
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Loaded {len(docs)} documents</span><span class="delimiter">&quot;</span></span>)

<span class="keyword">if</span> __name__ == <span class="string"><span class="delimiter">&quot;</span><span class="content">__main__</span><span class="delimiter">&quot;</span></span>:
    asyncio.run(main())</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Ejemplo con BeautifulSoup para cargar contenido web de forma sencilla.</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">AsyncChromiumLoader</span>
<span class="keyword">from</span> <span class="include">langchain_community.document_transformers</span> <span class="keyword">import</span> <span class="include">BeautifulSoupTransformer</span>

<span class="comment"># 1. Cargar HTML de una web (requiere Chromium instalado)</span>
loader = AsyncChromiumLoader([<span class="string"><span class="delimiter">&quot;</span><span class="content">https://python.org</span><span class="delimiter">&quot;</span></span>],
                            headless=<span class="predefined-constant">True</span>)
html_docs = loader.load()

<span class="comment"># 2. Transformar el HTML extrayendo solo las etiquetas relevantes</span>
bs_transformer = BeautifulSoupTransformer()
docs_limpios = bs_transformer.transform_documents(
    html_docs,
    tags_to_extract=[<span class="string"><span class="delimiter">&quot;</span><span class="content">p</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">li</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">div</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">a</span><span class="delimiter">&quot;</span></span>]  <span class="comment"># Extrae solo párrafos, listas, divisiones y enlaces</span>
)

<span class="comment"># 3. Mostrar parte del contenido limpio</span>
print(docs_limpios[<span class="integer">0</span>].page_content[:<span class="integer">500</span>])</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Recomendaciones:</div>
<ul>
<li>
<p>Para parsing simple y rápido, usa BeautifulSoup (<code>pip install langchain-community beautifulsoup4</code>).</p>
</li>
<li>
<p>Para parsing avanzado, usa Unstructured (<code>pip install langchain-unstructured</code>).</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Otros cargadores para web incluyen:</div>
<ul>
<li>
<p><code>RecursiveURL</code> (crawling recursivo de enlaces)</p>
</li>
<li>
<p><code>Sitemap</code> (carga desde sitemap XML)</p>
</li>
<li>
<p><code>Firecrawl</code>, <code>Docling</code>, <code>Hyperbrowser</code> y <code>AgentQL</code> para scraping y extracción estructurada vía API.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_apis_externas_y_cargadores_personalizados">4.1.4. APIs externas y cargadores personalizados</h4>
<div class="listingblock">
<div class="title">Puedes crear cargadores personalizados para consumir datos de APIs externas o formatos no soportados nativamente.</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">Document</span>

<span class="keyword">class</span> <span class="class">CustomAPILoader</span>:
    <span class="keyword">def</span> <span class="function">__init__</span>(<span class="predefined-constant">self</span>, endpoint):
        <span class="predefined-constant">self</span>.endpoint = endpoint

    <span class="keyword">def</span> <span class="function">load</span>(<span class="predefined-constant">self</span>):
        <span class="comment"># Lógica para llamar a la API y convertir la respuesta en Documentos</span>
        response = ...  <span class="comment"># Llama a la API</span>
        <span class="keyword">return</span> [Document(page_content=response[<span class="string"><span class="delimiter">&quot;</span><span class="content">texto</span><span class="delimiter">&quot;</span></span>], metadata={<span class="string"><span class="delimiter">&quot;</span><span class="content">source</span><span class="delimiter">&quot;</span></span>: <span class="predefined-constant">self</span>.endpoint})]</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Buenas prácticas y consideraciones</div>
<ul>
<li>
<p>Todos los cargadores devuelven una lista de objetos <code>Document</code>, que incluyen el contenido y metadatos útiles (fuente, página, etc.).</p>
</li>
<li>
<p>Usa carga perezosa (<code>lazy_load</code>) para grandes volúmenes de datos y evita problemas de memoria.</p>
</li>
<li>
<p>Para web, selecciona el cargador según la complejidad de la página y la necesidad de extracción estructurada.</p>
</li>
<li>
<p>Puedes combinar cargadores y dividir documentos en fragmentos para un procesamiento más eficiente.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_integraciones_con_plataformas_google_drive_wikipedia_etc_en_langchain">4.2. Integraciones con plataformas (Google Drive, Wikipedia, etc.) en LangChain</h3>
<div class="paragraph">
<p>LangChain permite conectar agentes, cadenas y modelos de lenguaje con plataformas populares como Google Drive, Wikipedia y otras fuentes externas mediante herramientas y cargadores específicos. Esto amplía las capacidades de tus aplicaciones, permitiendo consultar, analizar y utilizar datos actualizados y personalizados.</p>
</div>
<div class="sect3">
<h4 id="_google_drive">4.2.1. Google Drive</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Cargador de documentos:</strong> Permite importar Google Docs y otros archivos compatibles desde Google Drive a LangChain.</p>
</li>
<li>
<p><strong>Herramientas de búsqueda:</strong> Puedes buscar documentos por nombre, contenido o metadatos, y filtrar por carpetas o tipos de archivo.</p>
</li>
<li>
<p><strong>Configuración básica:</strong></p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Instalación de dependencias:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib langchain-googledrive</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Configuración de credenciales:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_googledrive.utilities.google_drive</span> <span class="keyword">import</span> <span class="include">GoogleDriveAPIWrapper</span>
<span class="keyword">from</span> <span class="include">langchain_googledrive.tools.google_drive.tool</span> <span class="keyword">import</span> <span class="include">GoogleDriveSearchTool</span>

wrapper = GoogleDriveAPIWrapper(folder_id=<span class="string"><span class="delimiter">&quot;</span><span class="content">root</span><span class="delimiter">&quot;</span></span>, num_results=<span class="integer">2</span>)
tool = GoogleDriveSearchTool(api_wrapper=wrapper)
docs = tool.run(<span class="string"><span class="delimiter">&quot;</span><span class="content">machine learning</span><span class="delimiter">&quot;</span></span>)
<span class="keyword">for</span> doc <span class="keyword">in</span> docs:
    print(doc.page_content[:<span class="integer">100</span>])</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_wikipedia">4.2.2. Wikipedia</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Herramienta WikipediaQueryRun:</strong> Permite consultar Wikipedia directamente desde agentes o cadenas.</p>
</li>
<li>
<p><strong>Uso básico en JavaScript:</strong></p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>import { WikipediaQueryRun } from "@langchain/community/tools/wikipedia_query_run";
const tool = new WikipediaQueryRun({ topKResults: 3 });
const res = await tool.invoke("LangChain");
console.log(res);</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Uso en Python:</strong> Puedes integrar Wikipedia usando agentes y herramientas de la comunidad, permitiendo respuestas contextuales y actualizadas.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_otras_integraciones_y_fuentes_externas">4.2.3. Otras integraciones y fuentes externas</h4>
<div class="ulist">
<ul>
<li>
<p><strong>APIs y bases de datos externas:</strong> Puedes conectar LangChain con cualquier API REST, base de datos SQL/NoSQL, o almacenamiento en la nube (AWS S3, Google Cloud Storage, etc.) mediante wrappers personalizados o cargadores específicos.</p>
</li>
<li>
<p><strong>Google Finance y Google Jobs:</strong> Herramientas para consultar datos financieros y ofertas de empleo usando la API de Google y SerpApi.</p>
</li>
<li>
<p><strong>Cargadores personalizados:</strong> Permiten consumir datos de cualquier fuente estructurada o no estructurada, adaptando el formato para su uso en LangChain.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Buenas prácticas</div>
<ul>
<li>
<p>Gestiona credenciales y permisos de forma segura (usa variables de entorno y archivos protegidos).</p>
</li>
<li>
<p>Aprovecha los filtros y plantillas de búsqueda para optimizar el acceso a grandes volúmenes de datos en Drive.</p>
</li>
<li>
<p>Combina varias integraciones para crear asistentes inteligentes que consulten múltiples fuentes en tiempo real.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_transformación_y_preprocesamiento_de_documentos_en_langchain">4.3. Transformación y preprocesamiento de documentos en LangChain</h3>
<div class="sect3">
<h4 id="_1_limpieza_y_normalización_de_texto">4.3.1. 1. Limpieza y normalización de texto</h4>
<div class="paragraph">
<p>La limpieza y normalización es el primer paso esencial para preparar documentos antes de usarlos en LangChain. Esto incluye:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Eliminación de ruido:</strong> Quita cabeceras, pies de página, URLs, emails y caracteres especiales que no aportan valor semántico.</p>
</li>
<li>
<p><strong>Normalización unicode:</strong> Convierte todos los caracteres a una forma estándar para evitar problemas con acentos o símbolos raros.</p>
</li>
<li>
<p><strong>Reducción de espacios y saltos de línea:</strong> Unifica espacios múltiples y elimina saltos innecesarios.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">re</span>
<span class="keyword">import</span> <span class="include">unicodedata</span>

<span class="keyword">def</span> <span class="function">clean_text</span>(text):
    <span class="comment"># Normaliza caracteres unicode</span>
    text = unicodedata.normalize(<span class="string"><span class="delimiter">'</span><span class="content">NFKD</span><span class="delimiter">'</span></span>, text)
    <span class="comment"># Elimina URLs</span>
    text = re.sub(<span class="string"><span class="modifier">r</span><span class="delimiter">'</span><span class="content">http</span><span class="content">\S</span><span class="content">+</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="delimiter">'</span></span>, text)
    <span class="comment"># Elimina emails</span>
    text = re.sub(<span class="string"><span class="modifier">r</span><span class="delimiter">'</span><span class="content">\S</span><span class="content">+@</span><span class="content">\S</span><span class="content">+</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="delimiter">'</span></span>, text)
    <span class="comment"># Elimina caracteres no alfanuméricos (excepto espacios)</span>
    text = re.sub(<span class="string"><span class="modifier">r</span><span class="delimiter">'</span><span class="content">[^a-zA-Z0-9</span><span class="content">\s</span><span class="content">]</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="delimiter">'</span></span>, text)
    <span class="comment"># Elimina espacios extra</span>
    text = re.sub(<span class="string"><span class="modifier">r</span><span class="delimiter">'</span><span class="content">\s</span><span class="content">+</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content"> </span><span class="delimiter">'</span></span>, text).strip()
    <span class="keyword">return</span> text

<span class="comment"># Aplicar limpieza a todos los documentos</span>
documents = [clean_text(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> raw_documents]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_2_división_de_texto_chunking_y_preservación_de_contexto">4.3.2. 2. División de texto (chunking) y preservación de contexto</h4>
<div class="paragraph">
<p>Dividir documentos extensos en fragmentos manejables es crucial para el procesamiento eficiente y la recuperación aumentada (RAG).</p>
</div>
<div class="ulist">
<div class="title">LangChain ofrece varios splitters:</div>
<ul>
<li>
<p><strong>Basados en longitud:</strong> Dividen por número de tokens o caracteres, útil para mantener los límites de contexto del modelo.</p>
</li>
<li>
<p><strong>Basados en estructura:</strong> Mantienen la coherencia semántica al intentar no romper párrafos o frases completas, usando <code>RecursiveCharacterTextSplitter</code>.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain.text_splitter</span> <span class="keyword">import</span> <span class="include">RecursiveCharacterTextSplitter</span>

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=<span class="integer">1000</span>,
    chunk_overlap=<span class="integer">200</span>,
    separators=[<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">.</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">!</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">?</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">,</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content"> </span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="delimiter">&quot;</span></span>]
)
split_documents = text_splitter.split_texts(documents)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_3_enriquecimiento_y_uso_de_metadatos">4.3.3. 3. Enriquecimiento y uso de metadatos</h4>
<div class="paragraph">
<p>Agregar metadatos como título, autor, fecha, fuente o etiquetas temáticas mejora la relevancia y filtrado durante la recuperación.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain.schema</span> <span class="keyword">import</span> <span class="include">Document</span>

doc = Document(
    page_content=<span class="string"><span class="delimiter">&quot;</span><span class="content">Texto limpio y segmentado</span><span class="delimiter">&quot;</span></span>,
    metadata={<span class="string"><span class="delimiter">&quot;</span><span class="content">title</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Ejemplo</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">source</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">manual.pdf</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">fecha</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">2024-06-01</span><span class="delimiter">&quot;</span></span>}
)</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Los metadatos pueden ser considerados por los sistemas de búsqueda y recuperación para mejorar la precisión de los resultados.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_4_tokenización_lematización_y_filtrado_de_stopwords">4.3.4. 4. Tokenización, lematización y filtrado de stopwords</h4>
<div class="paragraph">
<p>Para tareas avanzadas, se recomienda:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Tokenizar:</strong> Separar el texto en palabras o frases.</p>
</li>
<li>
<p><strong>Lematizar:</strong> Reducir palabras a su forma base.</p>
</li>
<li>
<p><strong>Eliminar stopwords:</strong> Quitar palabras comunes que no aportan significado (ej: "el", "de", "y").</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">spacy</span>
nlp = spacy.load(<span class="string"><span class="delimiter">'</span><span class="content">es_core_news_sm</span><span class="delimiter">'</span></span>)

<span class="keyword">def</span> <span class="function">preprocess_text</span>(text):
    doc = nlp(text)
    lemmatized = <span class="string"><span class="delimiter">'</span><span class="content"> </span><span class="delimiter">'</span></span>.join([token.lemma_ <span class="keyword">for</span> token <span class="keyword">in</span> doc <span class="keyword">if</span> <span class="keyword">not</span> token.is_stop])
    <span class="keyword">return</span> lemmatized

processed_documents = [preprocess_text(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> split_documents]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_5_preprocesamiento_específico_con_clases_y_transformadores_en_langchain">4.3.5. 5. Preprocesamiento específico con clases y transformadores en LangChain</h4>
<div class="paragraph">
<p>LangChain permite definir transformadores personalizados para aplicar cualquier lógica de preprocesamiento sobre objetos <code>Document</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain.schema.document</span> <span class="keyword">import</span> <span class="include">Document</span>, <span class="include">BaseDocumentTransformer</span>
<span class="keyword">from</span> <span class="include">typing</span> <span class="keyword">import</span> <span class="include">Any</span>, <span class="include">Sequence</span>

<span class="keyword">class</span> <span class="class">PreprocessTransformer</span>(BaseDocumentTransformer):
    <span class="keyword">def</span> <span class="function">transform_documents</span>(
        <span class="predefined-constant">self</span>, documents: Sequence[Document], **kwargs: Any
    ) -&gt; Sequence[Document]:
        <span class="keyword">for</span> document <span class="keyword">in</span> documents:
            <span class="comment"># Ejemplo: pasar todo a minúsculas</span>
            document.page_content = document.page_content.lower()
        <span class="keyword">return</span> documents</code></pre>
</div>
</div>
<div class="paragraph">
<p>Esto facilita la integración de cualquier pipeline de limpieza, normalización o enriquecimiento directamente en el flujo de trabajo de LangChain.</p>
</div>
</div>
<div class="sect3">
<h4 id="_6_preprocesamiento_de_preguntas_de_usuario">4.3.6. 6. Preprocesamiento de preguntas de usuario</h4>
<div class="paragraph">
<p>Para aplicaciones de preguntas y respuestas, es útil limpiar y normalizar las preguntas antes de pasarlas al motor de búsqueda o LLM:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Remover stopwords, corregir ortografía, eliminar caracteres especiales.</strong></p>
</li>
<li>
<p>Puede implementarse como un <code>Runnable</code> y añadirse a la cadena de procesamiento.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">nltk.corpus</span> <span class="keyword">import</span> <span class="include">stopwords</span>
<span class="keyword">from</span> <span class="include">nltk.tokenize</span> <span class="keyword">import</span> <span class="include">word_tokenize</span>
<span class="keyword">import</span> <span class="include">re</span>

<span class="keyword">class</span> <span class="class">PreprocessQuestion</span>:
    <span class="keyword">def</span> <span class="function">__init__</span>(<span class="predefined-constant">self</span>):
        <span class="predefined-constant">self</span>.stop_words = <span class="predefined">set</span>(stopwords.words(<span class="string"><span class="delimiter">'</span><span class="content">spanish</span><span class="delimiter">'</span></span>))

    <span class="keyword">def</span> <span class="function">run</span>(<span class="predefined-constant">self</span>, question: <span class="predefined">str</span>) -&gt; <span class="predefined">str</span>:
        question = re.sub(<span class="string"><span class="modifier">r</span><span class="delimiter">'</span><span class="content">\W</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content"> </span><span class="delimiter">'</span></span>, question)
        word_tokens = word_tokenize(question)
        filtered = <span class="string"><span class="delimiter">'</span><span class="content"> </span><span class="delimiter">'</span></span>.join(w <span class="keyword">for</span> w <span class="keyword">in</span> word_tokens <span class="keyword">if</span> w.lower() <span class="keyword">not</span> <span class="keyword">in</span> <span class="predefined-constant">self</span>.stop_words)
        <span class="keyword">return</span> filtered</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_ejemplo_completo_de_flujo_de_procesamiento_de_documentos_en_langchain">4.4. Ejemplo completo de flujo de procesamiento de documentos en LangChain</h3>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># 0. Instalar dependencias (ejecutar una vez)</span>
<span class="comment"># pip install langchain langchain_community python-dotenv spacy faiss-cpu ollama</span>
<span class="comment"># python -m spacy download es_core_news_sm</span>

<span class="keyword">import</span> <span class="include">os</span>
<span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">PyPDFLoader</span>, <span class="include">TextLoader</span>, <span class="include">WebBaseLoader</span>
<span class="keyword">from</span> <span class="include">langchain.text_splitter</span> <span class="keyword">import</span> <span class="include">RecursiveCharacterTextSplitter</span>
<span class="keyword">from</span> <span class="include">langchain_community.embeddings</span> <span class="keyword">import</span> <span class="include">OllamaEmbeddings</span>
<span class="keyword">from</span> <span class="include">langchain_community.vectorstores</span> <span class="keyword">import</span> <span class="include">FAISS</span>
<span class="keyword">import</span> <span class="include">spacy</span>

<span class="comment"># 1. Cargar documentos desde múltiples fuentes</span>
<span class="keyword">def</span> <span class="function">load_documents</span>():
    loaders = [
        PyPDFLoader(<span class="string"><span class="delimiter">&quot;</span><span class="content">documento.pdf</span><span class="delimiter">&quot;</span></span>),
        TextLoader(<span class="string"><span class="delimiter">&quot;</span><span class="content">texto.txt</span><span class="delimiter">&quot;</span></span>, encoding=<span class="string"><span class="delimiter">&quot;</span><span class="content">utf-8</span><span class="delimiter">&quot;</span></span>),
        WebBaseLoader([<span class="string"><span class="delimiter">&quot;</span><span class="content">https://ejemplo.com</span><span class="delimiter">&quot;</span></span>])
    ]

    documents = []
    <span class="keyword">for</span> loader <span class="keyword">in</span> loaders:
        documents.extend(loader.load())
    <span class="keyword">return</span> documents

<span class="comment"># 2. Limpiar y normalizar texto</span>
<span class="keyword">def</span> <span class="function">clean_text</span>(text):
    <span class="comment"># Eliminar caracteres especiales y normalizar espacios</span>
    text = re.sub(<span class="string"><span class="modifier">r</span><span class="delimiter">'</span><span class="content">[^</span><span class="content">\w</span><span class="content">\s</span><span class="content">áéíóúñÁÉÍÓÚÑ]</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="delimiter">'</span></span>, text)
    text = re.sub(<span class="string"><span class="modifier">r</span><span class="delimiter">'</span><span class="content">\s</span><span class="content">+</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content"> </span><span class="delimiter">'</span></span>, text).strip().lower()
    <span class="keyword">return</span> text

<span class="comment"># 3. Dividir en fragmentos con contexto</span>
<span class="keyword">def</span> <span class="function">split_documents</span>(docs):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=<span class="integer">1000</span>,
        chunk_overlap=<span class="integer">200</span>,
        separators=[<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">. </span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">! </span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">? </span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">, </span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content"> </span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="delimiter">&quot;</span></span>]
    )
    <span class="keyword">return</span> text_splitter.split_documents(docs)

<span class="comment"># 4. Enriquecer con metadatos</span>
<span class="keyword">def</span> <span class="function">add_metadata</span>(splits):
    <span class="keyword">for</span> split <span class="keyword">in</span> splits:
        split.metadata.update({
            <span class="string"><span class="delimiter">&quot;</span><span class="content">processed</span><span class="delimiter">&quot;</span></span>: <span class="predefined-constant">True</span>,
            <span class="string"><span class="delimiter">&quot;</span><span class="content">source_type</span><span class="delimiter">&quot;</span></span>: split.metadata[<span class="string"><span class="delimiter">&quot;</span><span class="content">source</span><span class="delimiter">&quot;</span></span>].split(<span class="string"><span class="delimiter">&quot;</span><span class="content">.</span><span class="delimiter">&quot;</span></span>)[-<span class="integer">1</span>].upper()
        })
    <span class="keyword">return</span> splits

<span class="comment"># 5. Procesamiento lingüístico con spaCy</span>
nlp = spacy.load(<span class="string"><span class="delimiter">&quot;</span><span class="content">es_core_news_sm</span><span class="delimiter">&quot;</span></span>)

<span class="keyword">def</span> <span class="function">lemmatize_text</span>(docs):
    <span class="keyword">for</span> doc <span class="keyword">in</span> docs:
        spacy_doc = nlp(doc.page_content)
        lemmas = [token.lemma_ <span class="keyword">for</span> token <span class="keyword">in</span> spacy_doc <span class="keyword">if</span> <span class="keyword">not</span> token.is_stop]
        doc.page_content = <span class="string"><span class="delimiter">&quot;</span><span class="content"> </span><span class="delimiter">&quot;</span></span>.join(lemmas)
    <span class="keyword">return</span> docs

<span class="comment"># 6. Crear embeddings y almacenar en vector store</span>
<span class="keyword">def</span> <span class="function">create_vector_store</span>(docs):
    embeddings = OllamaEmbeddings(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
    <span class="keyword">return</span> FAISS.from_documents(docs, embeddings)

<span class="comment"># 7. Procesar pregunta de usuario</span>
<span class="keyword">def</span> <span class="function">process_query</span>(query, vector_store):
    <span class="comment"># Preprocesar la pregunta</span>
    cleaned_query = clean_text(query)
    spacy_query = nlp(cleaned_query)
    lemmatized_query = <span class="string"><span class="delimiter">&quot;</span><span class="content"> </span><span class="delimiter">&quot;</span></span>.join([token.lemma_ <span class="keyword">for</span> token <span class="keyword">in</span> spacy_query <span class="keyword">if</span> <span class="keyword">not</span> token.is_stop])

    <span class="comment"># Buscar documentos relevantes</span>
    results = vector_store.similarity_search(lemmatized_query, k=<span class="integer">3</span>)
    <span class="keyword">return</span> results

<span class="comment"># --- Flujo principal ---</span>
<span class="keyword">if</span> __name__ == <span class="string"><span class="delimiter">&quot;</span><span class="content">__main__</span><span class="delimiter">&quot;</span></span>:
    <span class="comment"># Paso 1: Cargar documentos</span>
    raw_docs = load_documents()

    <span class="comment"># Paso 2-5: Procesamiento completo</span>
    cleaned_docs = [doc <span class="keyword">for</span> doc <span class="keyword">in</span> raw_docs <span class="keyword">if</span> clean_text(doc.page_content)]
    split_docs = split_documents(cleaned_docs)
    docs_with_metadata = add_metadata(split_docs)
    processed_docs = lemmatize_text(docs_with_metadata)

    <span class="comment"># Paso 6: Crear base de conocimientos</span>
    vector_store = create_vector_store(processed_docs)

    <span class="comment"># Paso 7: Ejemplo de consulta</span>
    query = <span class="string"><span class="delimiter">&quot;</span><span class="content">¿Qué ventajas ofrece LangChain?</span><span class="delimiter">&quot;</span></span>
    relevant_docs = process_query(query, vector_store)

    print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Documentos relevantes para la consulta: {query}</span><span class="delimiter">&quot;</span></span>)
    <span class="keyword">for</span> i, doc <span class="keyword">in</span> <span class="predefined">enumerate</span>(relevant_docs):
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Documento {i+1}:</span><span class="delimiter">&quot;</span></span>)
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Contenido: {doc.page_content[:200]}...</span><span class="delimiter">&quot;</span></span>)
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Metadatos: {doc.metadata}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Este flujo completo muestra cómo transformar documentos crudos en conocimiento estructurado listo para usar en aplicaciones RAG (Retrieval Augmented Generation) con LangChain.</p>
</div>
</div>
<div class="sect2">
<h3 id="_embeddings_incrustación_de_texto_y_creación_de_vectores_en_langchain">4.5. Embeddings: incrustación de texto y creación de vectores en LangChain</h3>
<div class="sect3">
<h4 id="_conceptos_clave">4.5.1. Conceptos clave</h4>
<div class="paragraph">
<p>Los embeddings son representaciones vectoriales de texto que capturan su significado semántico. En LangChain, se utilizan para:
- Búsqueda semántica: encontrar textos similares en el espacio vectorial
- Alimentar modelos de IA con información estructurada
- Construir sistemas RAG (Retrieval Augmented Generation)d</p>
</div>
</div>
<div class="sect3">
<h4 id="_componentes_principales">4.5.2. Componentes principales</h4>
<div class="paragraph">
<p><strong>1. Clase <code>Embeddings</code>:</strong>
Interfaz estándar para trabajar con diferentes proveedores (OpenAI, Hugging Face, custom).
Métodos esenciales:
- <code>embed_documents()</code>: Para textos largos (ej: documentos)
- <code>embed_query()</code>: Para consultas cortas</p>
</div>
<div class="paragraph">
<p><strong>2. Modelos soportados:</strong></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Proveedor</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Modelo ejemplo</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Dimensión</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenAI</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">text-embedding-3-small</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1536</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Hugging Face</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">all-MiniLM-L6-v2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">384</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cohere</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">embed-english-v3.0</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1024</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3">
<h4 id="_implementación_básica">4.5.3. Implementación básica</h4>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain.embeddings</span> <span class="keyword">import</span> <span class="include">OpenAIEmbeddings</span>, <span class="include">HuggingFaceEmbeddings</span>

<span class="comment"># Con OpenAI</span>
embeddings_openai = OpenAIEmbeddings(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">text-embedding-3-small</span><span class="delimiter">&quot;</span></span>)
vector_doc = embeddings_openai.embed_documents([<span class="string"><span class="delimiter">&quot;</span><span class="content">Texto de ejemplo</span><span class="delimiter">&quot;</span></span>])[<span class="integer">0</span>]

<span class="comment"># Con Hugging Face</span>
embeddings_hf = HuggingFaceEmbeddings(model_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">sentence-transformers/all-MiniLM-L6-v2</span><span class="delimiter">&quot;</span></span>)
vector_query = embeddings_hf.embed_query(<span class="string"><span class="delimiter">&quot;</span><span class="content">Consulta de ejemplo</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_proceso_completo_de_creación_de_vectores">4.5.4. Proceso completo de creación de vectores</h4>
<div class="paragraph">
<p><strong>Preprocesamiento:</strong>
   - Limpieza de texto (eliminar HTML, normalizar espacios)
   - División en chunks con `RecursiveCharacterTextSplitter`d</p>
</div>
<div class="paragraph">
<p><strong>Generación de embeddings:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">texts = [<span class="string"><span class="delimiter">&quot;</span><span class="content">Fragmento 1</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">Fragmento 2</span><span class="delimiter">&quot;</span></span>]
vectors = embeddings.embed_documents(texts)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Almacenamiento vectorial (FAISS ejemplo):</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.vectorstores</span> <span class="keyword">import</span> <span class="include">FAISS</span>

vector_store = FAISS.from_texts(texts, embeddings)
vector_store.save_local(<span class="string"><span class="delimiter">&quot;</span><span class="content">mi_store.faiss</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_buenas_prácticas">4.5.5. Buenas prácticas</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Gestión de modelos:</strong> Usar <code>model_kwargs</code> para configurar parámetros específicos</p>
</li>
<li>
<p><strong>Caché:</strong> Implementar <code>CacheBackedEmbeddings</code> para reutilizar embeddings</p>
</li>
<li>
<p><strong>Normalización:</strong> Aplicar L2-normalization para mejorar resultados de similitudd</p>
</li>
<li>
<p><strong>Versiones:</strong> Mantener versionado de embeddings al actualizar modelos</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_ejemplo_avanzado_con_embeddings_personalizados">4.5.6. Ejemplo avanzado con embeddings personalizados</h4>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain.embeddings.base</span> <span class="keyword">import</span> <span class="include">Embeddings</span>
<span class="keyword">import</span> <span class="include">numpy</span> <span class="keyword">as</span> np

<span class="keyword">class</span> <span class="class">CustomEmbeddings</span>(Embeddings):
    <span class="keyword">def</span> <span class="function">embed_documents</span>(<span class="predefined-constant">self</span>, texts: <span class="predefined">list</span>[<span class="predefined">str</span>]) -&gt; <span class="predefined">list</span>[<span class="predefined">list</span>[<span class="predefined">float</span>]]:
        <span class="keyword">return</span> [np.random.rand(<span class="integer">256</span>).tolist() <span class="keyword">for</span> _ <span class="keyword">in</span> texts]

    <span class="keyword">def</span> <span class="function">embed_query</span>(<span class="predefined-constant">self</span>, text: <span class="predefined">str</span>) -&gt; <span class="predefined">list</span>[<span class="predefined">float</span>]:
        <span class="keyword">return</span> np.random.rand(<span class="integer">256</span>).tolist()

<span class="comment"># Uso</span>
custom_emb = CustomEmbeddings()
vector = custom_emb.embed_query(<span class="string"><span class="delimiter">&quot;</span><span class="content">Texto personalizado</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Los embeddings son la base para construir aplicaciones de IA contextualizadas. LangChain simplifica su implementación mediante una API unificada que soporta múltiples proveedores y formatos.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_almacenamiento_y_búsqueda_en_bases_de_datos_vectoriales_con_langchain">4.6. Almacenamiento y búsqueda en bases de datos vectoriales con LangChain</h3>
<div class="sect3">
<h4 id="_principales_opciones_y_características_comparadas">4.6.1. Principales opciones y características comparadas</h4>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 16.667%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Base de Datos</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Tipo</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Escalabilidad</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Modos Búsqueda</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Hosting</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Integración LangChain</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">QDrant</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Motor especial</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Alta</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Densa/Híbrida</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cloud/Local</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">✅ Nativo</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">FAISS</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Biblioteca</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Media (RAM)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Densa</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Local</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">✅ Simple</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pinecone</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Managed</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Alta</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Densa</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cloud</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">✅ API</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Chroma</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Open-Source</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Media</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Densa</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Local/Cloud</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">✅ Directa</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">pgvector</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Extensión PG</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Altad</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Densad</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cualquier PG</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">✅ Vía SQLd</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3">
<h4 id="_configuración_básica_con_langchain">4.6.2. Configuración básica con LangChain</h4>
<div class="paragraph">
<p><strong>QDrant (local):</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_qdrant</span> <span class="keyword">import</span> <span class="include">QdrantVectorStore</span>
<span class="keyword">from</span> <span class="include">langchain.embeddings</span> <span class="keyword">import</span> <span class="include">OpenAIEmbeddings</span>

vector_store = QdrantVectorStore.from_documents(
    documents,
    OpenAIEmbeddings(),
    location=<span class="string"><span class="delimiter">&quot;</span><span class="content">:memory:</span><span class="delimiter">&quot;</span></span>,  <span class="comment"># Usar &quot;:memory:&quot; para modo RAM</span>
    collection_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">docs</span><span class="delimiter">&quot;</span></span>,
    retrieval_mode=<span class="string"><span class="delimiter">&quot;</span><span class="content">hybrid</span><span class="delimiter">&quot;</span></span>  <span class="comment"># dense/sparse/hybrid</span>
)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>FAISS (persistencia local):</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.vectorstores</span> <span class="keyword">import</span> <span class="include">FAISS</span>

faiss_index = FAISS.from_documents(docs, embeddings)
faiss_index.save_local(<span class="string"><span class="delimiter">&quot;</span><span class="content">faiss_index</span><span class="delimiter">&quot;</span></span>)  <span class="comment"># Guardar en disco</span></code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Pinecone (cloud):</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">pinecone</span>
<span class="keyword">from</span> <span class="include">langchain_pinecone</span> <span class="keyword">import</span> <span class="include">PineconeVectorStore</span>

pinecone.init(api_key=<span class="string"><span class="delimiter">&quot;</span><span class="content">TU_KEY</span><span class="delimiter">&quot;</span></span>)
index = PineconeVectorStore.from_documents(
    docs,
    embeddings,
    index_name=<span class="string"><span class="delimiter">&quot;</span><span class="content">langchain-demo</span><span class="delimiter">&quot;</span></span>
)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_búsquedas_avanzadas">4.6.3. Búsquedas avanzadas</h4>
<div class="paragraph">
<p><strong>Búsqueda híbrida en QDrant:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">results = vector_store.similarity_search(
    query=<span class="string"><span class="delimiter">&quot;</span><span class="content">Inteligencia Artificial</span><span class="delimiter">&quot;</span></span>,
    k=<span class="integer">5</span>,
    filter={<span class="string"><span class="delimiter">&quot;</span><span class="content">source</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">manual.pdf</span><span class="delimiter">&quot;</span></span>},  <span class="comment"># Filtro por metadatos</span>
    retrieval_mode=<span class="string"><span class="delimiter">&quot;</span><span class="content">hybrid</span><span class="delimiter">&quot;</span></span>  <span class="comment"># Combina dense + sparse vectors</span>
)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Búsqueda con filtros en Chroma:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python">retriever = chroma_db.as_retriever(
    search_kwargs={<span class="string"><span class="delimiter">&quot;</span><span class="content">filter</span><span class="delimiter">&quot;</span></span>: {<span class="string"><span class="delimiter">&quot;</span><span class="content">category</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">ciencia</span><span class="delimiter">&quot;</span></span>}, <span class="string"><span class="delimiter">&quot;</span><span class="content">k</span><span class="delimiter">&quot;</span></span>: <span class="integer">3</span>}
)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_rendimiento_y_mejores_prácticas">4.6.4. Rendimiento y mejores prácticas</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Elección de modelo de embeddings:</strong> Cohere (768D) vs OpenAI (1536D) - dimensiones afectan precisión/rendimiento</p>
</li>
<li>
<p><strong>Optimización de índices:</strong></p>
<div class="ulist">
<ul>
<li>
<p>QDrant: Ajustar HNSW (ef_construction=512, m=32)</p>
</li>
<li>
<p>FAISS: Usar IndexFlatL2 para precisión, IVFFlat para velocidad</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Estrategias de particionado:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Pinecone: Namespaces por tenant/categoría</p>
</li>
<li>
<p>QDrant: Colecciones separadas con sharding</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_casos_de_uso_recomendados">4.6.5. Casos de uso recomendados</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Desarrollo rápido:</strong> Chroma (local) + LangChain</p>
</li>
<li>
<p><strong>Alta escalabilidad:</strong> QDrant/Pinecone</p>
</li>
<li>
<p><strong>Privacidad total:</strong> FAISS/QDrant self-hosted</p>
</li>
<li>
<p><strong>Existente PostgreSQL:</strong> pgvectord</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>Elección óptima según necesidades: Chroma para prototipado, QDrant para balance rendimiento-control, Pinecone para escalabilidad sin gestión infra.</pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_agents_en_langchain">5. Agents en LangChain</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_introducción_a_los_agentes_definición_tipos_y_casos_de_uso">5.1. Introducción a los agentes: definición, tipos y casos de uso</h3>
<div class="sect3">
<h4 id="_qué_es_un_agente_en_langchain">5.1.1. ¿Qué es un agente en LangChain?</h4>
<div class="paragraph">
<p>Un agente en LangChain es un componente inteligente que utiliza modelos de lenguaje (LLMs) para tomar decisiones dinámicas sobre qué acciones ejecutar y en qué orden, con el objetivo de resolver tareas complejas. A diferencia de las cadenas (chains), que siguen una secuencia fija de pasos, los agentes pueden seleccionar y coordinar herramientas, interactuar con APIs, bases de datos y otros sistemas, y adaptar su comportamiento según el contexto y los resultados intermedios.</p>
</div>
<div class="paragraph">
<p>Los agentes funcionan como entidades virtuales capaces de razonar, actuar y aprender de la retroalimentación, permitiendo la automatización de flujos de trabajo avanzados y la integración con el mundo real.</p>
</div>
</div>
<div class="sect3">
<h4 id="_componentes_principales_de_un_agente">5.1.2. Componentes principales de un agente</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Modelo de lenguaje (LLM):</strong> Motor de razonamiento y generación de texto.</p>
</li>
<li>
<p><strong>Herramientas (Tools):</strong> Funciones o APIs externas que el agente puede invocar (búsqueda web, calculadora, bases de datos, etc.).</p>
</li>
<li>
<p><strong>Memoria:</strong> Permite mantener el contexto de la conversación o del flujo de trabajo.</p>
</li>
<li>
<p><strong>Prompt/Plantilla de sistema:</strong> Define el comportamiento general y las instrucciones para el agente.</p>
</li>
<li>
<p><strong>Ejecutor del agente:</strong> Orquesta el ciclo de acción-observación-reacción, gestionando la secuencia de decisiones y acciones.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_tipos_de_agentes_en_langchain">5.1.3. Tipos de agentes en LangChain</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Conversacionales:</strong> Mantienen el contexto y la memoria para interactuar de forma natural con el usuario (ej: asistentes virtuales, chatbots).</p>
</li>
<li>
<p><strong>ReAct (Reason + Act):</strong> Razonan paso a paso antes de ejecutar acciones, permitiendo decisiones informadas y explicables.</p>
</li>
<li>
<p><strong>ZeroShot y StructuredChat:</strong> Especializados en tareas específicas o en el uso de herramientas estructuradas sin ejemplos previos.</p>
</li>
<li>
<p><strong>Agentes de dominio:</strong> Adaptados a contextos concretos como bases de datos SQL, archivos CSV, marcos de datos, APIs abiertas, almacenamiento vectorial, etc..</p>
</li>
<li>
<p><strong>Agentes personalizados:</strong> Combinan herramientas y lógica específica para resolver flujos de trabajo complejos o integraciones empresariales avanzadas.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_casos_de_uso_destacados">5.1.4. Casos de uso destacados</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Automatización de tareas:</strong> Procesar y clasificar documentos, enviar datos a sistemas contables, ejecutar flujos multi-API.</p>
</li>
<li>
<p><strong>Asistentes virtuales y chatbots:</strong> Gestionar consultas frecuentes, mantener contexto conversacional, ofrecer respuestas personalizadas.</p>
</li>
<li>
<p><strong>Análisis y extracción de información:</strong> Buscar, analizar y resumir datos de fuentes externas (web, bases de datos, APIs).</p>
</li>
<li>
<p><strong>Toma de decisiones autónoma:</strong> Comparar productos, recomendar acciones, planificar tareas según criterios dinámicos.</p>
</li>
<li>
<p><strong>Traducción y generación de contenido:</strong> Traducir textos, redactar informes, crear resúmenes adaptados al usuario.</p>
</li>
<li>
<p><strong>Sistemas de recomendación y búsqueda avanzada:</strong> Personalizar sugerencias y mejorar la relevancia de resultados en plataformas digitales.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Ejemplo de flujo de trabajo de un agente</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_chroma</span> <span class="keyword">import</span> <span class="include">Chroma</span>
<span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbeddings</span>, <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain_core.output_parsers</span> <span class="keyword">import</span> <span class="include">StrOutputParser</span>
<span class="keyword">from</span> <span class="include">langchain_core.runnables</span> <span class="keyword">import</span> <span class="include">RunnablePassthrough</span>
<span class="keyword">from</span> <span class="include">langchain</span> <span class="keyword">import</span> <span class="include">hub</span>

<span class="comment"># 1. Cargar el almacén vectorial y embeddings locales</span>
vectorstore = Chroma(
    persist_directory=<span class="string"><span class="delimiter">&quot;</span><span class="content">./chroma_db</span><span class="delimiter">&quot;</span></span>,
    embedding_function=OllamaEmbeddings(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
)

<span class="comment"># 2. Configurar el modelo Llama3.2 local</span>
llm = OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)  <span class="comment"># o &quot;llama3&quot; según disponibilidad</span>

<span class="comment"># 3. Definir el recuperador (retriever)</span>
retriever = vectorstore.as_retriever()

<span class="comment"># 4. Función para formatear documentos recuperados</span>
<span class="keyword">def</span> <span class="function">format_docs</span>(docs):
    <span class="keyword">return</span> <span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>.join(doc.page_content <span class="keyword">for</span> doc <span class="keyword">in</span> docs)

<span class="comment"># 5. Cargar prompt RAG desde LangChain Hub</span>
rag_prompt = hub.pull(<span class="string"><span class="delimiter">&quot;</span><span class="content">rlm/rag-prompt</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># 6. Construir la cadena de preguntas y respuestas (QA chain)</span>
qa_chain = (
    {<span class="string"><span class="delimiter">&quot;</span><span class="content">context</span><span class="delimiter">&quot;</span></span>: retriever | format_docs, <span class="string"><span class="delimiter">&quot;</span><span class="content">question</span><span class="delimiter">&quot;</span></span>: RunnablePassthrough()}
    | rag_prompt
    | llm
    | StrOutputParser()
)

<span class="comment"># 7. Bucle interactivo del agente</span>
<span class="keyword">while</span> <span class="predefined-constant">True</span>:
    pregunta = <span class="predefined">input</span>(<span class="string"><span class="delimiter">&quot;</span><span class="content">Pregunta: </span><span class="delimiter">&quot;</span></span>)
    <span class="keyword">if</span> pregunta.lower() == <span class="string"><span class="delimiter">&quot;</span><span class="content">salir</span><span class="delimiter">&quot;</span></span>:
        <span class="keyword">break</span>
    respuesta = qa_chain.invoke(pregunta)
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Respuesta: {respuesta}</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_implementación_de_agentes_autónomos_en_langchain">5.2. Implementación de agentes autónomos en LangChain</h3>
<div class="sect3">
<h4 id="_componentes_clave">5.2.1. Componentes clave</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Herramientas (Tools):</strong> APIs, bases de datos, funciones externas</p>
</li>
<li>
<p><strong>Memoria:</strong> Historial de conversación y contexto</p>
</li>
<li>
<p><strong>LLM:</strong> Modelo de lenguaje para razonamiento</p>
</li>
<li>
<p><strong>Ejecutor:</strong> Orquesta el ciclo acción-decisión</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Implementación paso a paso de un agente autónomo con un tool, un modelo, memoria y ejecutor</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">langchain.agents</span> <span class="keyword">import</span> <span class="include">AgentExecutor</span>, <span class="include">Tool</span>, <span class="include">initialize_agent</span>, <span class="include">AgentType</span>
<span class="keyword">from</span> <span class="include">langchain.memory</span> <span class="keyword">import</span> <span class="include">ConversationBufferMemory</span>
<span class="keyword">from</span> <span class="include">langchain.tools</span> <span class="keyword">import</span> <span class="include">BaseTool</span>

<span class="comment"># 1. Definir herramienta personalizada</span>
<span class="keyword">class</span> <span class="class">CalculadoraTool</span>(BaseTool):
    name: str = <span class="string"><span class="delimiter">&quot;</span><span class="content">Calculadora</span><span class="delimiter">&quot;</span></span>
    description: str = <span class="string"><span class="delimiter">&quot;</span><span class="content">Útil para cálculos matemáticos. Entrada debe ser expresión numérica.</span><span class="delimiter">&quot;</span></span>

    <span class="keyword">def</span> <span class="function">_run</span>(<span class="predefined-constant">self</span>, expresion: <span class="predefined">str</span>) -&gt; <span class="predefined">str</span>:
        <span class="keyword">try</span>:
            <span class="keyword">return</span> <span class="predefined">str</span>(<span class="predefined">eval</span>(expresion))
        <span class="keyword">except</span> <span class="exception">Exception</span>:
            <span class="keyword">return</span> <span class="string"><span class="delimiter">&quot;</span><span class="content">Error en cálculo</span><span class="delimiter">&quot;</span></span>

<span class="comment"># 2. Configurar modelo local</span>
llm = Ollama(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>, temperature=<span class="float">0.3</span>)

<span class="comment"># 3. Inicializar memoria conversacional</span>
memory = ConversationBufferMemory(
    memory_key=<span class="string"><span class="delimiter">&quot;</span><span class="content">chat_history</span><span class="delimiter">&quot;</span></span>,
    return_messages=<span class="predefined-constant">True</span>
)

<span class="comment"># 4. Crear lista de herramientas</span>
tools = [
    CalculadoraTool(),
    Tool(
        name=<span class="string"><span class="delimiter">&quot;</span><span class="content">Búsqueda</span><span class="delimiter">&quot;</span></span>,
        func=<span class="keyword">lambda</span> q: <span class="string"><span class="delimiter">&quot;</span><span class="content">Implementar API búsqueda aquí</span><span class="delimiter">&quot;</span></span>,
        description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Útil para preguntas sobre actualidad</span><span class="delimiter">&quot;</span></span>
    )
]

<span class="comment"># 5. Inicializar agente REACT (sin prompt personalizado)</span>
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    memory=memory,
    verbose=<span class="predefined-constant">True</span>,
    max_iterations=<span class="integer">3</span>,
    early_stopping_method=<span class="string"><span class="delimiter">&quot;</span><span class="content">generate</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 6. Ejemplo de uso</span>
respuesta = agent.run(<span class="string"><span class="delimiter">&quot;</span><span class="content">Calcula (15^3 + 4^4) / 5</span><span class="delimiter">&quot;</span></span>)
print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Respuesta: {respuesta}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_aplicaciones_avanzadas">6. Aplicaciones Avanzadas</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_ejemplo_avanzado_de_workflow_en_llamaindex_con_integración_de_modelos_y_almacenamiento_vectorial">6.1. Ejemplo Avanzado de Workflow en LlamaIndex con Integración de Modelos y Almacenamiento Vectorial</h3>
<div class="sect3">
<h4 id="_1_requisitos_previos">6.1.1. 1. Requisitos Previos</h4>
<div class="paragraph">
<p>Instala las siguientes dependencias:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="bash">pip install llama-index-core llama-index-llms-ollama llama-index-embeddings-ollama ollama python-dotenv faiss-cpu</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_2_configuración_inicial">6.1.2. 2. Configuración Inicial</h4>
<div class="listingblock">
<div class="title">Crea un archivo <code>.env</code> para gestionar configuraciones:</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="ini">OLLAMA_HOST=http://localhost:11434
EMBEDDING_MODEL=all-minilm
LLM_MODEL=llama3.2
DATA_DIR=./datos</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_3_código_completo_del_workflow_de_langchain_usando_los_datos_de_env">6.1.3. 3. Código Completo del Workflow de Langchain usando los  datos de .env</h4>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">import</span> <span class="include">os</span>
<span class="keyword">from</span> <span class="include">dotenv</span> <span class="keyword">import</span> <span class="include">load_dotenv</span>
<span class="keyword">from</span> <span class="include">llama_index.core</span> <span class="keyword">import</span> <span class="include">SimpleDirectoryReader</span>, <span class="include">VectorStoreIndex</span>
<span class="keyword">from</span> <span class="include">llama_index.core.workflow</span> <span class="keyword">import</span> <span class="include">Workflow</span>, <span class="include">Step</span>, <span class="include">EventHandler</span>
<span class="keyword">from</span> <span class="include">llama_index.llms.ollama</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">llama_index.embeddings.ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbeddings</span>

<span class="comment"># Cargar variables de entorno</span>
load_dotenv()

<span class="comment"># Resto del código...</span></code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_chatbots_personalizados_y_asistentes_virtuales_con_langchain">6.2. Chatbots personalizados y asistentes virtuales con LangChain</h3>
<div class="sect3">
<h4 id="_por_qué_usar_langchain_para_chatbots_y_asistentes_virtuales">6.2.1. ¿Por qué usar LangChain para chatbots y asistentes virtuales?</h4>
<div class="paragraph">
<p>LangChain es un framework diseñado para facilitar la creación de chatbots y asistentes virtuales avanzados, permitiendo integrar modelos de lenguaje (como GPT-4, Llama 2, Claude, Gemini, etc.), flujos de conversación complejos, memoria contextual y conexiones a datos o APIs externas. Esto permite desarrollar bots que no solo responden, sino que entienden, recuerdan y se adaptan a las necesidades de los usuarios.</p>
</div>
</div>
<div class="sect3">
<h4 id="_características_principales">6.2.2. Características principales</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Memoria conversacional:</strong> Permite mantener el contexto y recordar preferencias o interacciones previas, mejorando la personalización y coherencia en las respuestas.</p>
</li>
<li>
<p><strong>Chains y agentes:</strong> Orquestan flujos de trabajo complejos, integrando lógica condicional, herramientas externas y toma de decisiones autónoma.</p>
</li>
<li>
<p><strong>Integración de datos:</strong> Los chatbots pueden consultar bases de datos, documentos, APIs o sistemas empresariales para ofrecer respuestas precisas y actualizadas.</p>
</li>
<li>
<p><strong>Personalización:</strong> Es posible definir la personalidad, tono y estilo del bot, así como entrenarlo con ejemplos reales y ajustar sus respuestas a cada caso de uso.</p>
</li>
<li>
<p><strong>Despliegue multiplataforma:</strong> Los asistentes pueden integrarse en webs, apps, WhatsApp, Slack, etc., y exponerse como API REST o mediante interfaces gráficas como Streamlit.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_ejemplo_básico_de_implementación_en_python">6.2.3. Ejemplo básico de implementación en Python</h4>
<div class="listingblock">
<div class="title">Este ejemplo muestra cómo crear un chatbot simple que responde a preguntas sobre tecnología usando un modelo de lenguaje y un prompt personalizado.</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">LLMChain</span>

<span class="comment"># 1. Configurar modelo local</span>
llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,
    temperature=<span class="float">0.5</span>  <span class="comment"># Balance entre precisión técnica y creatividad</span>
)

<span class="comment"># 2. Plantilla personalizada para respuestas técnicas</span>
tech_prompt = PromptTemplate(
    input_variables=[<span class="string"><span class="delimiter">&quot;</span><span class="content">pregunta</span><span class="delimiter">&quot;</span></span>],
    template=<span class="string"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Eres un experto en tecnología. Responde de forma clara y técnica.</span><span class="content">
</span><span class="content">
</span><span class="content">Contexto:</span><span class="content">
</span><span class="content">- Usa términos técnicos apropiados</span><span class="content">
</span><span class="content">- Incluye ejemplos prácticos cuando sea relevante</span><span class="content">
</span><span class="content">- Limita respuestas a 150 palabras máximo</span><span class="content">
</span><span class="content">
</span><span class="content">Pregunta: {pregunta}</span><span class="content">
</span><span class="content">Respuesta técnica:</span><span class="delimiter">&quot;&quot;&quot;</span></span>
)

<span class="comment"># 3. Crear cadena de conversación</span>
tech_chain = LLMChain(llm=llm, prompt=tech_prompt)

<span class="comment"># 4. Bucle interactivo</span>
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Chatbot Técnico (escribe 'salir' para terminar)</span><span class="delimiter">&quot;</span></span>)
<span class="keyword">while</span> <span class="predefined-constant">True</span>:
    user_input = <span class="predefined">input</span>(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Tú: </span><span class="delimiter">&quot;</span></span>)
    <span class="keyword">if</span> user_input.lower() == <span class="string"><span class="delimiter">'</span><span class="content">salir</span><span class="delimiter">'</span></span>:
        <span class="keyword">break</span>

    respuesta = tech_chain.invoke({<span class="string"><span class="delimiter">&quot;</span><span class="content">pregunta</span><span class="delimiter">&quot;</span></span>: user_input})
    print(f<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Asistente: {respuesta['text']}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_ejemplo_avanzado_chatbot_multimodal_y_personalizado">6.2.4. Ejemplo avanzado: chatbot multimodal y personalizado</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Memoria contextual:</strong> Añade ConversationBufferMemory para recordar la conversación.</p>
</li>
<li>
<p><strong>Herramientas externas:</strong> Integra APIs, bases de datos, búsqueda web, análisis de sentimientos, etc.</p>
</li>
<li>
<p><strong>Agentes:</strong> Usa agentes para decidir dinámicamente qué acción tomar (buscar, calcular, consultar documentos, etc.)</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Chatbot avanzado con herramientas y memoria</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.llms</span> <span class="keyword">import</span> <span class="include">Ollama</span>
<span class="keyword">from</span> <span class="include">langchain.agents</span> <span class="keyword">import</span> <span class="include">AgentExecutor</span>, <span class="include">Tool</span>, <span class="include">initialize_agent</span>
<span class="keyword">from</span> <span class="include">langchain.memory</span> <span class="keyword">import</span> <span class="include">ConversationBufferMemory</span>
<span class="keyword">from</span> <span class="include">langchain_community.chat_message_histories</span> <span class="keyword">import</span> <span class="include">ChatMessageHistory</span>
<span class="keyword">from</span> <span class="include">langchain.tools</span> <span class="keyword">import</span> <span class="include">BaseTool</span>
<span class="keyword">import</span> <span class="include">json</span>

<span class="comment"># 1. Herramienta de cálculo matemático</span>
<span class="keyword">class</span> <span class="class">CalculadoraTool</span>(BaseTool):
    name: str = <span class="string"><span class="delimiter">&quot;</span><span class="content">Calculadora</span><span class="delimiter">&quot;</span></span>
    description: str = <span class="string"><span class="delimiter">&quot;</span><span class="content">Útil para operaciones matemáticas. Entrada: expresión numérica.</span><span class="delimiter">&quot;</span></span>

    <span class="keyword">def</span> <span class="function">_run</span>(<span class="predefined-constant">self</span>, expresion: <span class="predefined">str</span>) -&gt; <span class="predefined">str</span>:
        <span class="keyword">try</span>:
            <span class="keyword">return</span> <span class="predefined">str</span>(<span class="predefined">eval</span>(expresion))
        <span class="keyword">except</span> <span class="exception">Exception</span>:
            <span class="keyword">return</span> <span class="string"><span class="delimiter">&quot;</span><span class="content">Error en el cálculo</span><span class="delimiter">&quot;</span></span>

<span class="comment"># 2. Configuración del modelo local</span>
llm = Ollama(
    model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>,
    temperature=<span class="float">0.4</span>,
    system=<span class="string"><span class="delimiter">&quot;</span><span class="content">Eres un asistente técnico especializado en matemáticas e IA.</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 3. Crear memoria con ChatMessageHistory</span>
chat_history = ChatMessageHistory()
memory = ConversationBufferMemory(
    memory_key=<span class="string"><span class="delimiter">&quot;</span><span class="content">chat_history</span><span class="delimiter">&quot;</span></span>,
    chat_memory=chat_history,
    return_messages=<span class="predefined-constant">True</span>
)

<span class="comment"># 4. Lista de herramientas</span>
tools = [
    CalculadoraTool(),
    Tool(
        name=<span class="string"><span class="delimiter">&quot;</span><span class="content">BúsquedaTécnica</span><span class="delimiter">&quot;</span></span>,
        func=<span class="keyword">lambda</span> q: <span class="string"><span class="delimiter">&quot;</span><span class="content">Resultado simulado para: </span><span class="delimiter">&quot;</span></span> + q,
        description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Búsqueda en documentación técnica</span><span class="delimiter">&quot;</span></span>
    )
]

<span class="comment"># 5. Configurar agente REACT</span>
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=<span class="string"><span class="delimiter">&quot;</span><span class="content">structured-chat-zero-shot-react-description</span><span class="delimiter">&quot;</span></span>,
    memory=memory,
    verbose=<span class="predefined-constant">True</span>,
    max_iterations=<span class="integer">3</span>,
    early_stopping_method=<span class="string"><span class="delimiter">&quot;</span><span class="content">generate</span><span class="delimiter">&quot;</span></span>
)

<span class="comment"># 6. Bucle interactivo</span>
print(<span class="string"><span class="delimiter">&quot;</span><span class="content">Chatbot Técnico (escribe 'salir' para terminar)</span><span class="delimiter">&quot;</span></span>)
<span class="keyword">while</span> <span class="predefined-constant">True</span>:
    user_input = <span class="predefined">input</span>(<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Usuario: </span><span class="delimiter">&quot;</span></span>)
    <span class="keyword">if</span> user_input.lower() == <span class="string"><span class="delimiter">'</span><span class="content">salir</span><span class="delimiter">'</span></span>:
        <span class="keyword">break</span>

    <span class="keyword">try</span>:
        respuesta = agent.run(user_input)
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="content">Asistente: {respuesta}</span><span class="delimiter">&quot;</span></span>)
    <span class="keyword">except</span> <span class="exception">Exception</span> <span class="keyword">as</span> e:
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Error: {str(e)}</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_casos_de_uso_reales">6.2.5. Casos de uso reales</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Atención al cliente:</strong> Bots que resuelven dudas frecuentes, gestionan reservas o escalan a agentes humanos si es necesario.</p>
</li>
<li>
<p><strong>Educación:</strong> Asistentes que explican conceptos, generan planes de estudio o corrigen ejercicios.</p>
</li>
<li>
<p><strong>Soporte interno:</strong> Bots que consultan bases de datos empresariales, documentos internos o sistemas de tickets.</p>
</li>
<li>
<p><strong>Marketing y ventas:</strong> Chatbots que recomiendan productos, analizan sentimientos o personalizan ofertas.</p>
</li>
<li>
<p><strong>Salud y bienestar:</strong> Asistentes que ayudan a gestionar citas, responder preguntas médicas básicas o guiar rutinas de autocuidado.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_mejores_prácticas">6.2.6. Mejores prácticas</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Define la personalidad y el dominio del bot</strong> para respuestas coherentes y alineadas con la marca.</p>
</li>
<li>
<p><strong>Optimiza el uso de memoria:</strong> Limita el historial guardado o usa resúmenes para mantener eficiencia y relevancia.</p>
</li>
<li>
<p><strong>Integra validación y filtrado de respuestas</strong> para evitar errores o información inapropiada.</p>
</li>
<li>
<p><strong>Despliega el bot en plataformas adecuadas</strong> (web, móvil, API, WhatsApp, Slack, etc.) según el público objetivo.</p>
</li>
<li>
<p><strong>Aprovecha la modularidad de LangChain</strong> para añadir nuevas funciones, herramientas o fuentes de datos fácilmente.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre>LangChain permite crear chatbots y asistentes virtuales inteligentes, personalizados y conectados a datos reales, transformando la experiencia de usuario en múltiples sectores y casos de uso.</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_resumen_y_análisis_de_grandes_volúmenes_de_texto">6.3. Resumen y análisis de grandes volúmenes de texto</h3>
<div class="sect3">
<h4 id="_introducción">6.3.1. Introducción</h4>
<div class="paragraph">
<p>El procesamiento y análisis de grandes volúmenes de texto es esencial en sectores como legal, salud, finanzas, educación y atención al cliente. LangChain y los LLMs modernos permiten automatizar la extracción de conocimiento, la síntesis de información y la obtención de insights clave a partir de millones de documentos, correos, informes o tickets.</p>
</div>
</div>
<div class="sect3">
<h4 id="_técnicas_clave_para_el_procesamiento_de_textos_extensos">6.3.2. Técnicas clave para el procesamiento de textos extensos</h4>
<div class="paragraph">
<p><strong>Resumen automático</strong>
   - <strong>Extractivo:</strong> Selecciona frases clave del texto original usando algoritmos como LexRank, TextRank o TF-IDF. Es rápido, transparente y útil para documentos técnicos, legales o científicos donde la fidelidad al texto original es prioritaria.
   - <strong>Abstractivo:</strong> Reescribe y sintetiza el contenido usando modelos generativos (BERT, GPT, Llama, Claude, Gemini, etc.). Produce resúmenes más legibles y adaptados al contexto, ideales para informes ejecutivos, resúmenes de prensa o síntesis educativas.</p>
</div>
<div class="paragraph">
<p><strong>Análisis semántico</strong>
   - <strong>Modelado de temas (Topic Modeling):</strong> Identifica patrones temáticos y agrupa documentos por tópicos usando LDA, LSA o embeddings. Permite detectar tendencias, áreas de interés y segmentar grandes corpus.
   - <strong>Análisis de sentimiento:</strong> Clasifica emociones (positivo, negativo, neutral) usando LSTM, transformers o APIs externas. Es clave en encuestas, reputación de marca y feedback de clientes.
   - <strong>NER y extracción de entidades:</strong> Identifica nombres, fechas, cantidades y conceptos clave para estructurar la información y facilitar búsquedas avanzadas.</p>
</div>
<div class="listingblock">
<div class="title">Procesamiento de Pipelines en LangChain</div>
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_community.document_loaders</span> <span class="keyword">import</span> <span class="include">WebBaseLoader</span>
<span class="keyword">from</span> <span class="include">langchain_text_splitters</span> <span class="keyword">import</span> <span class="include">RecursiveCharacterTextSplitter</span>
<span class="keyword">from</span> <span class="include">langchain_ollama</span> <span class="keyword">import</span> <span class="include">OllamaEmbeddings</span>, <span class="include">OllamaLLM</span>
<span class="keyword">from</span> <span class="include">langchain_community.vectorstores</span> <span class="keyword">import</span> <span class="include">Chroma</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">RetrievalQA</span>
<span class="keyword">from</span> <span class="include">langchain.output_parsers</span> <span class="keyword">import</span> <span class="include">PydanticOutputParser</span>
<span class="keyword">from</span> <span class="include">pydantic</span> <span class="keyword">import</span> <span class="include">BaseModel</span>

<span class="comment"># 1. Modelo de validación</span>
<span class="keyword">class</span> <span class="class">RespuestaTecnica</span>(BaseModel):
    concepto: <span class="predefined">str</span>
    aplicaciones: <span class="predefined">list</span>[<span class="predefined">str</span>]
    complejidad: <span class="predefined">int</span>

<span class="comment"># 2. Pipeline de procesamiento</span>
loader = WebBaseLoader([<span class="string"><span class="delimiter">&quot;</span><span class="content">https://docs.python.org/3/whatsnew/3.13.html</span><span class="delimiter">&quot;</span></span>])
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="integer">1000</span>)
chunks = text_splitter.split_documents(docs)

embeddings = OllamaEmbeddings(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">nomic-embed-text</span><span class="delimiter">&quot;</span></span>)
vectorstore = Chroma.from_documents(chunks, embeddings)

llm = OllamaLLM(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">llama3.2</span><span class="delimiter">&quot;</span></span>)
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectorstore.as_retriever()
)

<span class="comment"># 3. Ejecución sin validación estructurada</span>
resultado = qa_chain.invoke(<span class="string"><span class="delimiter">&quot;</span><span class="content">What’s New In Python 3.13?</span><span class="delimiter">&quot;</span></span>)
print(resultado[<span class="string"><span class="delimiter">&quot;</span><span class="content">result</span><span class="delimiter">&quot;</span></span>])</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Recuperación aumentada (RAG) y generación contextual</strong>
   - Permite responder preguntas específicas sobre grandes corpus, combinando búsqueda vectorial y generación de texto.
   - Mejora la precisión y relevancia de las respuestas, citando fuentes y fragmentos originales.</p>
</div>
<div class="paragraph">
<p><strong>Clasificación automática y etiquetado</strong>
   - Asigna categorías, etiquetas o prioridades a documentos usando modelos supervisados o prompts few-shot.
   - Útil para priorizar tickets, filtrar spam, clasificar noticias o segmentar clientes.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 11. Herramientas especializadas</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Herramienta</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Función principal</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ejemplo de uso</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Parafrasist</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Resumen multimodal (PDF, imágenes)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">resumir_texto(archivo.pdf, 30%)</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">MonkeyLearn</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Análisis de sentimiento sin código</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Clasificar reseñas de clientes</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">QuestionPro</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Extracción de insights en tiempo real</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Informes de satisfacción automáticos</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Nebuly</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Detección de tendencias con LLMs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Identificar problemas recurrentes</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">LangChain</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Orquestación de pipelines de IA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">RAG, resúmenes, análisis temático</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
</tbody>
</table>
<div class="ulist">
<div class="title">Casos de uso empresariales</div>
<ul>
<li>
<p><strong>Salud:</strong></p>
</li>
<li>
<p>Resumen automático de historiales clínicos para médicos.</p>
</li>
<li>
<p>Extracción de diagnósticos y tratamientos frecuentes.</p>
</li>
<li>
<p>Análisis de sentimiento en encuestas de satisfacción de pacientes.</p>
</li>
<li>
<p><strong>Legal:</strong></p>
</li>
<li>
<p>Extracción de cláusulas clave y fechas en contratos.</p>
</li>
<li>
<p>Resumen de jurisprudencia y dictámenes.</p>
</li>
<li>
<p>Búsqueda semántica en grandes repositorios jurídicos.</p>
</li>
<li>
<p><strong>Finanzas:</strong></p>
</li>
<li>
<p>Detección de patrones y anomalías en informes anuales.</p>
</li>
<li>
<p>Resúmenes ejecutivos para directivos.</p>
</li>
<li>
<p>Clasificación de riesgos y oportunidades en reportes de mercado.</p>
</li>
<li>
<p><strong>Educación:</strong></p>
</li>
<li>
<p>Generación de resúmenes de libros y artículos.</p>
</li>
<li>
<p>Análisis de tendencias en foros y plataformas educativas.</p>
</li>
<li>
<p>Evaluación automática de respuestas abiertas.</p>
</li>
<li>
<p><strong>Atención al cliente y RRHH:</strong></p>
</li>
<li>
<p>Análisis de sentimiento en tickets y encuestas.</p>
</li>
<li>
<p>Priorización automática de incidencias.</p>
</li>
<li>
<p>Extracción de insights para mejorar procesos internos.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Buenas prácticas</div>
<ol class="arabic">
<li>
<p><strong>Preprocesamiento robusto:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Lematización, eliminación de stopwords, normalización de caracteres y limpieza de HTML.</p>
</li>
<li>
<p>Fragmentar documentos largos en chunks solapados para preservar el contexto.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Validación humana:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Revisar una muestra de los resúmenes y análisis generados.</p>
</li>
<li>
<p>Ajustar prompts y modelos según el feedback recibido.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Escalabilidad:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Usar bases vectoriales como FAISS, Qdrant o Pinecone para búsquedas rápidas y clustering en grandes volúmenes.</p>
</li>
<li>
<p>Procesar documentos en batches y usar pipelines asíncronos.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Seguridad y privacidad:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Encriptar datos sensibles.</p>
</li>
<li>
<p>Controlar acceso a información confidencial y cumplir normativas (GDPR, HIPAA).</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Trazabilidad y explicabilidad:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Guardar logs de las operaciones y decisiones automáticas.</p>
</li>
<li>
<p>Citar fuentes y fragmentos originales en los resúmenes y respuestas generadas.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_integración_con_sistemas_ia_avanzados">6.3.3. Integración con sistemas IA avanzados</h4>
<div class="ulist">
<div class="title">Los sistemas RAG (Retrieval-Augmented Generation) combinan recuperación semántica y generación de texto para:</div>
<ul>
<li>
<p>Responder consultas específicas basadas en documentos internos, citando fragmentos relevantes.</p>
</li>
<li>
<p>Actualizar el conocimiento automáticamente al añadir nuevos documentos o fuentes.</p>
</li>
<li>
<p>Generar informes ejecutivos, dashboards y alertas contextuales para la toma de decisiones.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_integración_con_frontends_gradio_y_apis_rest">6.4. Integración con frontends: Gradio y APIs REST</h3>
<div class="sect3">
<h4 id="_integración_con_gradio_para_interfaces_de_chat">6.4.1. Integración con Gradio para interfaces de chat</h4>
<div class="paragraph">
<p><strong>Ejemplo básico de chatbot con historial conversacional:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain_openai</span> <span class="keyword">import</span> <span class="include">ChatOpenAI</span>
<span class="keyword">from</span> <span class="include">langchain.schema</span> <span class="keyword">import</span> <span class="include">AIMessage</span>, <span class="include">HumanMessage</span>
<span class="keyword">import</span> <span class="include">gradio</span> <span class="keyword">as</span> gr

model = ChatOpenAI(model=<span class="string"><span class="delimiter">&quot;</span><span class="content">gpt-4o-mini</span><span class="delimiter">&quot;</span></span>)

<span class="keyword">def</span> <span class="function">predict</span>(message, history):
    <span class="comment"># Convertir historial de Gradio a formato LangChain</span>
    langchain_history = []
    <span class="keyword">for</span> human, ai <span class="keyword">in</span> history:
        langchain_history.extend([
            HumanMessage(content=human),
            AIMessage(content=ai)
        ])

    <span class="comment"># Añadir nuevo mensaje y generar respuesta</span>
    langchain_history.append(HumanMessage(content=message))
    response = model.invoke(langchain_history)

    <span class="keyword">return</span> response.content

<span class="comment"># Crear interfaz Gradio con soporte para historial</span>
demo = gr.ChatInterface(
    predict,
    title=<span class="string"><span class="delimiter">&quot;</span><span class="content">Asistente Virtual con LangChain</span><span class="delimiter">&quot;</span></span>,
    description=<span class="string"><span class="delimiter">&quot;</span><span class="content">Escribe tu pregunta...</span><span class="delimiter">&quot;</span></span>
)
demo.launch()</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Características clave:</strong>
- Mantiene contexto conversacional
- Soporta streaming de respuestas
- Fácil despliegue en Hugging Face Spaces</p>
</div>
</div>
<div class="sect3">
<h4 id="_integración_con_apis_rest">6.4.2. Integración con APIs REST</h4>
<div class="paragraph">
<p><strong>1. Crear herramienta personalizada para llamadas API:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain.tools</span> <span class="keyword">import</span> <span class="include">tool</span>
<span class="keyword">import</span> <span class="include">requests</span>

<span class="decorator">@tool</span>
<span class="keyword">def</span> <span class="function">buscar_noticias</span>(tema: <span class="predefined">str</span>) -&gt; <span class="predefined">str</span>:
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">Busca noticias recientes usando NewsAPI</span><span class="delimiter">&quot;&quot;&quot;</span></span>
    url = <span class="string"><span class="delimiter">&quot;</span><span class="content">https://newsapi.org/v2/everything</span><span class="delimiter">&quot;</span></span>
    params = {
        <span class="string"><span class="delimiter">&quot;</span><span class="content">apiKey</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">TU_API_KEY</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">q</span><span class="delimiter">&quot;</span></span>: tema,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">pageSize</span><span class="delimiter">&quot;</span></span>: <span class="integer">3</span>
    }
    response = requests.get(url, params=params)
    <span class="keyword">return</span> <span class="string"><span class="delimiter">&quot;</span><span class="char">\n</span><span class="delimiter">&quot;</span></span>.join([art[<span class="string"><span class="delimiter">'</span><span class="content">title</span><span class="delimiter">'</span></span>] <span class="keyword">for</span> art <span class="keyword">in</span> response.json()[<span class="string"><span class="delimiter">'</span><span class="content">articles</span><span class="delimiter">'</span></span>]])</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>2. Usar la herramienta en un agente:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain.agents</span> <span class="keyword">import</span> <span class="include">initialize_agent</span>

tools = [buscar_noticias]
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=<span class="predefined-constant">True</span>
)

<span class="comment"># Ejemplo de uso</span>
respuesta = agent.run(<span class="string"><span class="delimiter">&quot;</span><span class="content">¿Qué hay de nuevo sobre inteligencia artificial?</span><span class="delimiter">&quot;</span></span>)</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>3. Exponer como API REST con FastAPI:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">fastapi</span> <span class="keyword">import</span> <span class="include">FastAPI</span>
<span class="keyword">from</span> <span class="include">pydantic</span> <span class="keyword">import</span> <span class="include">BaseModel</span>

app = FastAPI()

<span class="keyword">class</span> <span class="class">Query</span>(BaseModel):
    text: <span class="predefined">str</span>

<span class="decorator">@app.post</span>(<span class="string"><span class="delimiter">&quot;</span><span class="content">/chat</span><span class="delimiter">&quot;</span></span>)
<span class="keyword">def</span> <span class="function">chat_endpoint</span>(query: Query):
    <span class="keyword">return</span> {<span class="string"><span class="delimiter">&quot;</span><span class="content">response</span><span class="delimiter">&quot;</span></span>: agent.run(query.text)}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_caso_de_uso_avanzado_sistema_multimodal">6.4.3. Caso de uso avanzado: Sistema multimodal</h4>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="comment"># Combinar Gradio + APIs + Herramientas personalizadas</span>
<span class="keyword">with</span> gr.Blocks() <span class="keyword">as</span> demo:
    chatbot = gr.Chatbot()
    msg = gr.Textbox()

    <span class="keyword">def</span> <span class="function">respond</span>(message, chat_history):
        response = agent.run({
            <span class="string"><span class="delimiter">&quot;</span><span class="content">input</span><span class="delimiter">&quot;</span></span>: message,
            <span class="string"><span class="delimiter">&quot;</span><span class="content">chat_history</span><span class="delimiter">&quot;</span></span>: chat_history
        })
        chat_history.append((message, response))
        <span class="keyword">return</span> <span class="string"><span class="delimiter">&quot;</span><span class="delimiter">&quot;</span></span>, chat_history

    msg.submit(respond, [msg, chatbot], [msg, chatbot])</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_recursos_y_mejores_prácticas">6.4.4. Recursos y mejores prácticas</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Repositorios útiles:</strong></p>
</li>
<li>
<p>[langchain-gradio-template](<a href="https://github.com/hwchase17/langchain-gradio-template" class="bare">https://github.com/hwchase17/langchain-gradio-template</a>)</p>
</li>
<li>
<p>[langchain-gradio](<a href="https://github.com/AK391/langchain-gradio" class="bare">https://github.com/AK391/langchain-gradio</a>)</p>
</li>
<li>
<p><strong>Consideraciones importantes:</strong></p>
</li>
<li>
<p>Gestionar secretos API con variables de entornod</p>
</li>
<li>
<p>Limitar tasa de solicitudes para APIs externasd</p>
</li>
<li>
<p>Implementar caché para respuestas frecuentes</p>
</li>
<li>
<p>Validar y sanitizar entradas de usuario</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>Esta integración permite crear sistemas completos donde los modelos de LangChain interactúan con usuarios finales a través de interfaces amigables y se conectan con sistemas externos mediante APIs.</pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_buenas_prácticas_y_despliegue">7. Buenas Prácticas y Despliegue</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_seguridad_y_gestión_de_claves_api">7.1. Seguridad y gestión de claves API</h3>

</div>
<div class="sect2">
<h3 id="_optimización_de_costes_y_rendimiento">7.2. Optimización de costes y rendimiento</h3>

</div>
<div class="sect2">
<h3 id="_control_de_versiones_y_pruebas">7.3. Control de versiones y pruebas</h3>

</div>
<div class="sect2">
<h3 id="_despliegue_de_aplicaciones_langchain_en_producción">7.4. Despliegue de aplicaciones LangChain en producción</h3>

</div>
</div>
</div>
<div class="sect1">
<h2 id="_recursos_y_comunidad">8. Recursos y Comunidad</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_documentación_oficial_y_recursos_de_aprendizaje_de_langchain">8.1. Documentación oficial y recursos de aprendizaje de LangChain</h3>
<div class="sect3">
<h4 id="_documentación_oficial">8.1.1. Documentación oficial</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Sitio principal</strong>: <a href="https://python.langchain.com/docs/" class="bare">https://python.langchain.com/docs/</a> d</p>
</li>
<li>
<p>Guías paso a paso para todos los componentes (models, chains, agents, memory)</p>
</li>
<li>
<p>Tutoriales prácticos con código ejecutable</p>
</li>
<li>
<p>Referencia completa de la API</p>
</li>
<li>
<p><strong>Arquitectura del framework</strong>: Explicación detallada de <code>langchain-core</code>, <code>langchain-community</code> y <code>langgraph</code></p>
</li>
<li>
<p><strong>LangSmith</strong>: Plataforma para monitoreo, debugging y evaluación de aplicaciones LLM</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_recursos_clave_en_español">8.1.2. Recursos clave en español</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Guía de inicio rápido</strong>: <a href="https://samusarmiento.hashnode.dev/langchain-101-guia-de-inicio-rapido" class="bare">https://samusarmiento.hashnode.dev/langchain-101-guia-de-inicio-rapido</a> d</p>
</li>
<li>
<p>Instalación, configuración y primeros pasos con ejemplos prácticos</p>
</li>
<li>
<p>Explicación de <code>PromptTemplate</code> y <code>LLMChain</code></p>
</li>
<li>
<p><strong>Mentores Tech</strong>: <a href="https://www.mentorestech.com/resource-blog-content/langchain-y-recursos-de-estudio-para-aprenderlo" class="bare">https://www.mentorestech.com/resource-blog-content/langchain-y-recursos-de-estudio-para-aprenderlo</a></p>
</li>
<li>
<p>Listado actualizado de recursos oficiales y comunitarios</p>
</li>
<li>
<p>Consejos para aprender desde cero hasta nivel avanzado</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_cursos_recomendados">8.1.3. Cursos recomendados</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>LangChain Chat Models &amp; Agents</strong> (DeepLearning.ai)</p>
<div class="ulist">
<ul>
<li>
<p>Gratuito, 1h 38min de duración</p>
</li>
<li>
<p>Impartido por Harrison Chase (creador de LangChain) y Andrew Ng</p>
</li>
<li>
<p>Contenido: Models, Memory, Chains, Agents, RAG</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Learn LangChain in 7 Easy Steps</strong> (YouTube)</p>
<div class="ulist">
<ul>
<li>
<p>Tutorial interactivo con mapas conceptuales y código</p>
</li>
<li>
<p>Enfoque en componentes clave: prompts, chains, agents, tools</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_comunidad_y_recursos_adicionales">8.1.4. Comunidad y recursos adicionales</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Repositorio GitHub</strong>: <a href="https://github.com/langchain-ai/langchain" class="bare">https://github.com/langchain-ai/langchain</a></p>
</li>
<li>
<p>Código fuente, issues y contribuciones</p>
</li>
<li>
<p>+100 notebooks de ejemplos prácticos</p>
</li>
<li>
<p><strong>LangChain Hub</strong>: <a href="https://smith.langchain.com/hub" class="bare">https://smith.langchain.com/hub</a></p>
</li>
<li>
<p>Plantillas reutilizables de prompts y chains</p>
</li>
<li>
<p>Ejemplos de RAG, chatbots y flujos complejos</p>
</li>
<li>
<p><strong>Canal de YouTube oficial</strong>: <a href="https://www.youtube.com/@LangChainAI" class="bare">https://www.youtube.com/@LangChainAI</a></p>
</li>
<li>
<p>Tutoriales en video y actualizaciones del framework</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_herramientas_para_desarrollo_avanzado">8.1.5. Herramientas para desarrollo avanzado</h4>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Herramienta</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Uso principal</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Enlace</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">LangGraph</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Creación de agents con estado</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="https://langchain.com/docs" class="bare">https://langchain.com/docs</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">LangChain Express</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Lenguaje para componer flujos complejos</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Incluido en <code>langchain-core</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">FAISS/Qdrant</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Bases vectoriales para RAG</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Documentación oficial</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3">
<h4 id="_consejos_para_el_aprendizaje">8.1.6. Consejos para el aprendizaje</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Comienza con el Quickstart oficial  para entender la arquitectura básica</p>
</li>
<li>
<p>Experimenta con el curso de DeepLearning.ai  para aplicaciones reales</p>
</li>
<li>
<p>Únete a la comunidad en GitHub para resolver dudas específicas</p>
</li>
<li>
<p>Para chatbots: Explora las plantillas de <code>ConversationalRetrievalChain</code></p>
</li>
<li>
<p>Usa LangSmith  desde el principio para depurar y optimizar tus cadenas</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre>Estos recursos proporcionan un camino estructurado para dominar LangChain, desde conceptos básicos hasta implementaciones empresariales complejas.</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_repositorios_y_ejemplos_prácticos_de_langchain">8.2. Repositorios y ejemplos prácticos de LangChain</h3>
<div class="sect3">
<h4 id="_repositorios_destacados">8.2.1. Repositorios destacados</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Repositorio oficial de LangChain (Python):</strong></p>
</li>
<li>
<p><a href="https://github.com/langchain-ai/langchain" class="bare">https://github.com/langchain-ai/langchain</a></p>
</li>
<li>
<p>Incluye código fuente, notebooks ejecutables, ejemplos de chains, agentes, herramientas y memoria. Es el punto de partida para explorar todas las capacidades del framework, así como para contribuir o consultar dudas técnicas.</p>
</li>
<li>
<p><strong>Ejemplos prácticos en Python:</strong></p>
</li>
<li>
<p><a href="https://github.com/djsquircle/LangChain_Examples" class="bare">https://github.com/djsquircle/LangChain_Examples</a></p>
</li>
<li>
<p>Colección de scripts y notebooks con implementaciones de tareas reales: plantillas de prompt, chains secuenciales, resumen de PDFs, extracción web, análisis de sentimiento, procesamiento de CSV y más.</p>
</li>
<li>
<p>Ejemplo de estructura:</p>
</li>
<li>
<p><code>01.01_simple_prompt_template.py</code>: Uso básico de prompt templates</p>
</li>
<li>
<p><code>01.04_simple_summarizer.py</code>: Resumidor automático</p>
</li>
<li>
<p><code>10.01_pdf_summarizing.py</code>: Resumen de documentos PDF</p>
</li>
<li>
<p><code>11.01_web_article_summarizer.py</code>: Extracción y resumen de artículos web</p>
</li>
<li>
<p><code>03.01_life_coach_with_few_shot_example.py</code>: Ejemplo de few-shot learning</p>
</li>
<li>
<p><code>07.01_output_parser_csv.py</code>: Parseo estructurado de salidas</p>
</li>
<li>
<p>Instrucciones claras para instalación y ejecución paso a paso</p>
</li>
<li>
<p><strong>Repositorio de chatbot personalizado:</strong></p>
</li>
<li>
<p><a href="https://github.com/marcosd59/langchain-chatbot" class="bare">https://github.com/marcosd59/langchain-chatbot</a></p>
</li>
<li>
<p>Ejemplo de chatbot con memoria, chains secuenciales y personalización de prompts, ideal para quienes buscan crear asistentes virtuales adaptados a un dominio concreto.</p>
</li>
<li>
<p><strong>LangChain en JavaScript/TypeScript:</strong></p>
</li>
<li>
<p><a href="https://github.com/langchain-ai/langchainjs" class="bare">https://github.com/langchain-ai/langchainjs</a></p>
</li>
<li>
<p>Permite crear aplicaciones context-aware y agentes en Node.js, navegadores y plataformas serverless. Incluye ejemplos de chatbots, RAG y despliegue en la nube.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_ejemplos_prácticos_y_tutoriales">8.2.2. Ejemplos prácticos y tutoriales</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Tutoriales paso a paso y casos de uso:</strong></p>
</li>
<li>
<p><a href="https://nanonets.com/blog/langchain/" class="bare">https://nanonets.com/blog/langchain/</a></p>
</li>
<li>
<p>Guía completa con ejemplos de instalación, creación de chains, integración de modelos y despliegue como API REST.</p>
</li>
<li>
<p><a href="https://www.unite.ai/es/Ingenier%C3%ADa-r%C3%A1pida-de-cero-a-avanzada-con-langchain-en-python/" class="bare">https://www.unite.ai/es/Ingenier%C3%ADa-r%C3%A1pida-de-cero-a-avanzada-con-langchain-en-python/</a></p>
</li>
<li>
<p>Explicaciones claras, ejemplos de procesamiento de arXiv, QA sobre documentos y uso de chains encadenadas.</p>
</li>
<li>
<p><strong>Proyectos end-to-end en video:</strong></p>
</li>
<li>
<p><a href="https://www.youtube.com/watch?v=x0AnCE9SE4A" class="bare">https://www.youtube.com/watch?v=x0AnCE9SE4A</a></p>
</li>
<li>
<p>Curso práctico con seis proyectos completos usando OpenAI, Gemini Pro, Llama 2 y despliegue en Hugging Face Spaces. Incluye integración frontend-backend y ejemplos de chaining avanzado.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_ejemplo_básico_en_python">8.2.3. Ejemplo básico en Python:</h4>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">from</span> <span class="include">langchain.llms</span> <span class="keyword">import</span> <span class="include">OpenAI</span>
<span class="keyword">from</span> <span class="include">langchain.chains</span> <span class="keyword">import</span> <span class="include">LLMChain</span>
<span class="keyword">from</span> <span class="include">langchain.prompts</span> <span class="keyword">import</span> <span class="include">PromptTemplate</span>

template = <span class="string"><span class="delimiter">&quot;</span><span class="content">¿Cuál es la capital de {pais}?</span><span class="delimiter">&quot;</span></span>
prompt = PromptTemplate.from_template(template)
llm = OpenAI(temperature=<span class="float">0.7</span>)
chain = LLMChain(prompt=prompt, llm=llm)
respuesta = chain.run(<span class="string"><span class="delimiter">&quot;</span><span class="content">Chile</span><span class="delimiter">&quot;</span></span>)
print(respuesta)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_casos_de_uso_frecuentes">8.2.4. Casos de uso frecuentes</h4>
<div class="ulist">
<ul>
<li>
<p>Chatbots personalizados y asistentes virtuales con memoria y lógica avanzada</p>
</li>
<li>
<p>Sistemas de preguntas y respuestas sobre documentos propios (RAG)</p>
</li>
<li>
<p>Automatización de procesos y agentes que usan herramientas externas</p>
</li>
<li>
<p>Extracción y resumen de información de PDFs, webs y APIs</p>
</li>
<li>
<p>Análisis legal, financiero o médico sobre grandes volúmenes de datos</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_cómo_empezar_con_los_ejemplos">8.2.5. Cómo empezar con los ejemplos</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Clona el repositorio o descarga los scripts/notebooks deseados.</p>
</li>
<li>
<p>Crea un entorno virtual y activa las dependencias necesarias.</p>
</li>
<li>
<p>Configura tus claves API en un archivo <code>.env</code> si es necesario.</p>
</li>
<li>
<p>Ejecuta los ejemplos y modifica los parámetros para adaptarlos a tu caso de uso.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre>Estos repositorios y ejemplos prácticos son el mejor punto de partida para aprender LangChain, experimentar con sus componentes y crear aplicaciones inteligentes sobre tus propios datos y flujos de trabajo.</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_comunidad_y_foros_de_soporte_de_langchain">8.3. Comunidad y foros de soporte de LangChain</h3>
<div class="sect3">
<h4 id="_espacios_oficiales_de_la_comunidad">8.3.1. Espacios oficiales de la comunidad</h4>
<div class="ulist">
<ul>
<li>
<p><strong>LangChain Community Hub:</strong>
Página central para conectarse con otros desarrolladores, compartir conocimientos, descubrir eventos y contribuir al futuro del framework.</p>
</li>
<li>
<p><a href="https://www.langchain.com/community" class="bare">https://www.langchain.com/community</a></p>
</li>
<li>
<p><strong>Slack oficial:</strong>
Espacio de chat donde puedes interactuar con miles de desarrolladores, resolver dudas en tiempo real, participar en canales temáticos y recibir anuncios de novedades.</p>
</li>
<li>
<p>Acceso desde la web de la comunidad</p>
</li>
<li>
<p><strong>GitHub Discussions:</strong>
Foro abierto para plantear preguntas, compartir ideas, colaborar en proyectos y buscar ayuda sobre problemas técnicos o integraciones específicas.</p>
</li>
<li>
<p><a href="https://github.com/tryAGI/LangChain/discussions" class="bare">https://github.com/tryAGI/LangChain/discussions</a></p>
</li>
<li>
<p><strong>LangChain.js Community Navigator:</strong>
Página para usuarios de LangChain en JavaScript/TypeScript, con recursos, eventos, meetups y enlaces a foros de soporte y contribución.</p>
</li>
<li>
<p><a href="https://js.langchain.com/docs/community/" class="bare">https://js.langchain.com/docs/community/</a></p>
</li>
<li>
<p><strong>Discord:</strong>
Existen herramientas y toolkits para integrar bots de LangChain en Discord, facilitando la colaboración y el soporte en comunidades técnicas.</p>
</li>
<li>
<p>Paquete: <code>langchain-discord-shikenso</code> en PyPI</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_modalidades_de_participación">8.3.2. Modalidades de participación</h4>
<div class="ulist">
<ul>
<li>
<p><strong>Contribuir con código:</strong>
Más de 3.500 personas han contribuido al desarrollo del ecosistema LangChain. Puedes proponer mejoras, corregir errores, añadir nuevas funciones o mejorar la documentación.</p>
</li>
<li>
<p><strong>LangChain Community Champions y Ambassadors:</strong>
Programas para reconocer y apoyar a los miembros más activos, con acceso directo al equipo, influencia en la hoja de ruta, eventos exclusivos y acceso anticipado a productos y recursos.</p>
</li>
<li>
<p><strong>Meetups, eventos y hackathons:</strong>
La comunidad organiza encuentros presenciales y virtuales, talleres, hackathons y webinars para aprender, colaborar y compartir experiencias sobre LangChain y aplicaciones de IA.</p>
</li>
<li>
<p><strong>Blog y difusión:</strong>
Los miembros comparten artículos, tutoriales y casos de uso en blogs y redes sociales, amplificando el conocimiento y las mejores prácticas en el desarrollo con LangChain.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_buenas_prácticas_para_aprovechar_la_comunidad">8.3.3. Buenas prácticas para aprovechar la comunidad</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Explora los canales oficiales</strong> para dudas técnicas, networking y anuncios.</p>
</li>
<li>
<p><strong>Participa en eventos y meetups</strong> para aprender de otros usuarios y expertos.</p>
</li>
<li>
<p><strong>Contribuye con ejemplos prácticos o tutoriales</strong>, especialmente si prefieres el aprendizaje basado en código y repositorios.</p>
</li>
<li>
<p><strong>Utiliza foros y GitHub Discussions</strong> para problemas complejos o integraciones avanzadas.</p>
</li>
<li>
<p><strong>Aprovecha los toolkits de Discord</strong> para integrar bots de soporte y automatización en tus propias comunidades técnicas.</p>
</li>
</ol>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2025-06-10 02:40:12 +0200
</div>
</div>
</body>
</html>